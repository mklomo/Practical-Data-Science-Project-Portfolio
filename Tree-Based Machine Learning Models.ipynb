{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5d510bb",
   "metadata": {},
   "source": [
    "# Tree-Based Machine Learning Models\n",
    "\n",
    "This notebook covers the ff:\n",
    "- How decision trees work in machine learning\n",
    "- Random forests in sklearn and H20\n",
    "- Feature Importances from tree-based methods\n",
    "- Boosted algorithms i.e. AdaBoost, XGBoost, LightGBM and CatBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7375b1",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "Decision trees are simple algorithms, which split data based on specific values of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "dc99131f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>NAME_EDUCATION_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "      <td>1</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>1129500.0</td>\n",
       "      <td>Higher education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
       "0      100002       1         Cash loans           M            N   \n",
       "1      100003       0         Cash loans           F            N   \n",
       "2      100004       0    Revolving loans           M            Y   \n",
       "3      100006       0         Cash loans           F            N   \n",
       "4      100007       0         Cash loans           M            N   \n",
       "\n",
       "  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0               Y             0          202500.0    406597.5      24700.5   \n",
       "1               N             0          270000.0   1293502.5      35698.5   \n",
       "2               Y             0           67500.0    135000.0       6750.0   \n",
       "3               Y             0          135000.0    312682.5      29686.5   \n",
       "4               Y             0          121500.0    513000.0      21865.5   \n",
       "\n",
       "   AMT_GOODS_PRICE            NAME_EDUCATION_TYPE  \n",
       "0         351000.0  Secondary / secondary special  \n",
       "1        1129500.0               Higher education  \n",
       "2         135000.0  Secondary / secondary special  \n",
       "3         297000.0  Secondary / secondary special  \n",
       "4         513000.0  Secondary / secondary special  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_path = \"https://raw.githubusercontent.com/PacktPublishing/Practical-Data-Science-with-Python/main/15-Chapter-15/data/loan_data_sample.csv\"\n",
    "\n",
    "df = pd.read_csv(df_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f720415c",
   "metadata": {},
   "source": [
    "Since sklearn can only handle numeric data. Lets perform changes to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "576f4004",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df.drop(\"SK_ID_CURR\", axis = 1).copy()\n",
    "\n",
    "final_df['NAME_CONTRACT_TYPE'] = final_df['NAME_CONTRACT_TYPE'].map(\n",
    "                                            {'Cash loans': 0, 'Revolving loans': 1})\n",
    "final_df['CODE_GENDER'] = final_df['CODE_GENDER'].map({'M': 0, 'F': 1})\n",
    "\n",
    "final_df['FLAG_OWN_CAR'] = final_df['FLAG_OWN_CAR'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "final_df['FLAG_OWN_REALTY'] = final_df['FLAG_OWN_REALTY'].map(\n",
    "                                        {'N': 0, 'Y': 1})\n",
    "\n",
    "final_df['NAME_EDUCATION_TYPE'] = final_df['NAME_EDUCATION_TYPE'].map({'Lower secondary': 0,\n",
    "                                                    'Secondary / secondary special': 0,\n",
    "                                                    'Incomplete higher': 1,\n",
    "                                                    'Higher education': 2,\n",
    "                                                    'Academic degree': 2})\n",
    "final_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "36a4c26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 307217 entries, 0 to 307510\n",
      "Data columns (total 11 columns):\n",
      " #   Column               Non-Null Count   Dtype  \n",
      "---  ------               --------------   -----  \n",
      " 0   TARGET               307217 non-null  int64  \n",
      " 1   NAME_CONTRACT_TYPE   307217 non-null  int64  \n",
      " 2   CODE_GENDER          307217 non-null  float64\n",
      " 3   FLAG_OWN_CAR         307217 non-null  int64  \n",
      " 4   FLAG_OWN_REALTY      307217 non-null  int64  \n",
      " 5   CNT_CHILDREN         307217 non-null  int64  \n",
      " 6   AMT_INCOME_TOTAL     307217 non-null  float64\n",
      " 7   AMT_CREDIT           307217 non-null  float64\n",
      " 8   AMT_ANNUITY          307217 non-null  float64\n",
      " 9   AMT_GOODS_PRICE      307217 non-null  float64\n",
      " 10  NAME_EDUCATION_TYPE  307217 non-null  int64  \n",
      "dtypes: float64(5), int64(6)\n",
      "memory usage: 28.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Verify the new df's features\n",
    "final_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72af6e6",
   "metadata": {},
   "source": [
    "We make a copy of the DataFrame so we still have the original version, and convert our string columns to numeric. Most of the columns are binary, so we can convert them to 0s and 1s. The education column has several values, and we turn them into an ordinal variable, which is a categorical category with ordering. \n",
    "\n",
    "We also need to drop missing values since sklearn cannot handle them (we could also impute missing values, of course). After this conversion, we can double-check the datatypes are correct with final_df.info()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24efd15a",
   "metadata": {},
   "source": [
    "> Decision trees work by splitting the data based on specific values from feature columns. For example, we might split our data into two groups based on AMT_INCOME_TOTAL, since above a value of around 200,000, the chances of payment\n",
    "difficulty are slightly lower (in other words, the TARGET variable is more likely to be 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b73f63",
   "metadata": {},
   "source": [
    "The machine learning aspect comes into play with decision trees from automating\n",
    "splitting decisions. There are several different algorithms for creating decision trees, and some of the top algorithms are `C4.5, C5.0, and CART (classification and regression trees)`. \n",
    "\n",
    "The sklearn package uses a version of CART. With CART, we split the data into binary splits (as shown in Figure 15.1) and do so in a greedy manner.\n",
    "For example, we try splitting the data on all features and all values of the features, and then find which one splits the data best. \n",
    "\n",
    "For classification, we measure the best split by the \"purity\" of the nodes, meaning the split that breaks up the data into unique classes best is used. We can measure this using the Gini criteria or entropy (entropy is also called \"information gain\"). Both these criteria have a minimum of 0 when the classes in the leaf nodes are pure, and have a maximum value when classes are evenly distributed. For regression, we can use measures such as **meansquare error (MSE), mean-absolute error (MAE), or others**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fde1d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before fitting models to the data, lets break it up into test and training sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "features = final_df.drop(\"TARGET\", axis = 1)\n",
    "\n",
    "targets = final_df['TARGET']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features,\n",
    "                                                    targets,\n",
    "                                                    stratify = targets,\n",
    "                                                    random_state = 42)\n",
    "                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4f414d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9991095280498664\n",
      "Test Accuracy: 0.832\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Now lets fit a decision tree to our data\n",
    "\n",
    "dt_class = DecisionTreeClassifier()\n",
    "\n",
    "dt_class.fit(x_train, y_train)\n",
    "\n",
    "print(f\"Train Accuracy: {dt_class.score(x_train, y_train)}\")\n",
    "print(f\"Test Accuracy: {dt_class.score(x_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04ddbe8",
   "metadata": {},
   "source": [
    "That looks like severe overfitting – the train score is almost 100% accuracy while\n",
    "the test score is much lower. The reason for this is our decision trees are allowed to\n",
    "grow to unlimited depths (or heights, if you want to think of it that way). The tree\n",
    "continues to split the data until each leaf is very pure. \n",
    "\n",
    "While this works great for the training data and we can almost always get near 100% accuracy on the training set this way, it will almost always perform poorly on the test set. We can see how\n",
    "many splits our decision tree has with dt.get_depth(), which turns out to be 50. This\n",
    "means we have 50 splits and 50 layers of nodes after the root node, which is quite a\n",
    "lot. The depth of the tree can be restricted with the max_depth hyperparameter, which\n",
    "we set to 2 here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3edaf5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9154051647373108\n",
      "Test Accuracy: 0.9146666666666666\n"
     ]
    }
   ],
   "source": [
    "small_dt_class = DecisionTreeClassifier(max_depth = 2, max_features = None)\n",
    "\n",
    "small_dt_class.fit(x_train, y_train)\n",
    "\n",
    "print(f\"Train Accuracy: {small_dt_class.score(x_train, y_train)}\")\n",
    "print(f\"Test Accuracy: {small_dt_class.score(x_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fa120c",
   "metadata": {},
   "source": [
    "We also set the max_features hyperparameter, which controls how many features\n",
    "are tried at each split. A random subset of features is used at each split, which is the\n",
    "square root of the number of features by default. `Setting max_features=None` uses all\n",
    "features at every split. There are several other hyperparameters we can set as well:\n",
    "`criterion ('gini' or 'entropy')` and many others that control how many samples\n",
    "can be in nodes or are required to split a node (such as min_samples_leaf). With\n",
    "classification, we can also set class_weight to control the weights for each class in the\n",
    "Gini criterion or entropy calculations.\n",
    "\n",
    "\n",
    "Our accuracy on the train and test sets now are both around 93.6%, which is about\n",
    "the same as the no information rate, meaning our model isn't doing better than\n",
    "random chance. This doesn't mean it's completely useless – we might be able to\n",
    "predict certain samples better than random chance and can understand relationships\n",
    "of features to the target from the model. To understand this better, we can plot the\n",
    "tree with an sklearn function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bf272c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "\n",
    "f = plt.figure(figsize = (10, 10))\n",
    "_ = plot_tree(small_dt_class, feature_names = features.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7410afb0",
   "metadata": {},
   "source": [
    "Decision trees are easy to explain because we can plot them and see the rules for\n",
    "making splits. However, they are limited in performance (accuracy for classification)\n",
    "due to their simplicity. One method that improves performance is random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f16fdc6",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "Random forests build on decision trees. Instead of using a single tree, a forest\n",
    "uses a collection of them (as you may have guessed from the name). Each tree is\n",
    "built with a sample of the data which we get from bootstrapping, or sampling\n",
    "with replacement. We also randomly subset the features at each split, as we saw is\n",
    "possible with max_features for the sklearn decision trees. Random forests can also\n",
    "be parallelized across processors or computers since we can build each decision\n",
    "tree separately because they are independent of one another. This ML algorithm is\n",
    "called an ensemble method since it uses a collection of models.\n",
    "\n",
    "\n",
    "\n",
    "The combination of bootstrapping and combining several models in an ensemble is called bagging (a\n",
    "portmanteau of bootstrapping and aggregating). There are several implementations\n",
    "of random forests in Python, R, other programming languages, and data science\n",
    "GUIs. Two easy ways to use them in Python are with sklearn and the H2O packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ee9e7d",
   "metadata": {},
   "source": [
    "### Random Forests with SKLEARN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3f362ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9341050756901158\n",
      "Test Accuracy: 0.9146666666666666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rfc = RandomForestClassifier(max_depth = 10, n_jobs = -1, random_state = 42)\n",
    "\n",
    "rfc.fit(x_train, y_train)\n",
    "\n",
    "print(f\"Train Accuracy: {rfc.score(x_train, y_train)}\")\n",
    "print(f\"Test Accuracy: {rfc.score(x_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48037012",
   "metadata": {},
   "source": [
    "Here, we set a few other parameters for the classifier class: n_jobs=-1, which uses all\n",
    "available CPU cores in parallel, and the random_state argument so that results are\n",
    "reproducible. The scores here are about the same as the no information rate – 93.1%\n",
    "accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0ed2f6",
   "metadata": {},
   "source": [
    "We can tune the hyperparameters to improve our model. We can tune:\n",
    "- The number of trees\n",
    "- The depth of trees\n",
    "- Max_Features for each split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd6dcb3",
   "metadata": {},
   "source": [
    "We are searching some of the typical ranges for the three hyperparameters\n",
    "mentioned above, and also fixing the random_state and n_jobs arguments. The\n",
    "training data was sampled down to only 1,000 datapoints because this takes a while\n",
    "to run, since we are fitting 135 models with the grid search (using the default of 5\n",
    "CV splits with 3 unique values of 3 hyperparameters, or 5*3*3*3=135). After the grid\n",
    "search completes, our best hyperparameters and score are the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ee106b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=10, max_features=3,\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=-1, oob_score=False, random_state=42, verbose=0,\n",
      "                       warm_start=False)\n",
      "0.9110000000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "x_tr_sample = x_train.sample(1000)\n",
    "\n",
    "y_tr_sample = y_train.loc[x_tr_sample.index]\n",
    "\n",
    "\n",
    "params = {'n_estimators': [100, 300, 500],\n",
    "            'max_depth': [10, 15, 20],\n",
    "            'max_features': [3, 6, 9],\n",
    "            'random_state': [42],\n",
    "            'n_jobs': [-1]}\n",
    "\n",
    "\n",
    "gs = GridSearchCV(estimator = rfc, param_grid = params, n_jobs = -1)\n",
    "\n",
    "gs.fit(x_tr_sample, y_tr_sample)\n",
    "\n",
    "print(gs.best_estimator_)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e94848",
   "metadata": {},
   "source": [
    "We can see we're still not outperforming the no information rate, although we did\n",
    "settle on a different value for the maximum number of features.\n",
    "\n",
    "The training and hyperparameter search can take a long time with the full dataset or\n",
    "another even bigger dataset, and we have a few options for dealing with it:\n",
    "\n",
    "- Scale down the data with sampling\n",
    "- Wait for the job to complete (although this could take hours or even days)\n",
    "- Scale up to a more powerful computer (for example, using cloud resources)\n",
    "- Scale up to a cluster using packages such as Dask\n",
    "- Use another package or software that can scale using computing clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcca23",
   "metadata": {},
   "source": [
    "### Random Forest with H20\n",
    "\n",
    "\n",
    "H2O is another ML package in Python besides sklearn. It has a few ML algorithms\n",
    "and some nice advantages over sklearn. For random forests (and other tree-based\n",
    "methods), it allows us to use missing values and categorical features, unlike sklearn.\n",
    "H2O can also scale up on a cluster for big data. The H2O.ai company also offers\n",
    "other products, such as their driverless AI, which is a data science and machine\n",
    "learning GUI that offers lots of automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d499a5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; Java HotSpot(TM) 64-Bit Server VM (build 18.0.1.1+2-6, mixed mode, sharing)\n",
      "  Starting server from c:\\Users\\INNO\\Documents\\Python Development\\Practical Data Science\\final_env\\final_env\\lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: C:\\Users\\INNO\\AppData\\Local\\Temp\\tmpkjnzxjv3\n",
      "  JVM stdout: C:\\Users\\INNO\\AppData\\Local\\Temp\\tmpkjnzxjv3\\h2o_INNO_started_from_python.out\n",
      "  JVM stderr: C:\\Users\\INNO\\AppData\\Local\\Temp\\tmpkjnzxjv3\\h2o_INNO_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Etc/UTC</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.36.1.1</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>1 month and 7 days </td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_INNO_aryamo</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>3.953 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.7.9 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  -----------------------------\n",
       "H2O_cluster_uptime:         02 secs\n",
       "H2O_cluster_timezone:       Etc/UTC\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.36.1.1\n",
       "H2O_cluster_version_age:    1 month and 7 days\n",
       "H2O_cluster_name:           H2O_from_python_INNO_aryamo\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    3.953 Gb\n",
       "H2O_cluster_total_cores:    8\n",
       "H2O_cluster_allowed_cores:  8\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.7.9 final\n",
       "--------------------------  -----------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h2o\n",
    "\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6578f27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n"
     ]
    }
   ],
   "source": [
    "hf = h2o.H2OFrame(df)\n",
    "\n",
    "hf[\"TARGET\"] = hf[\"TARGET\"].asfactor()\n",
    "\n",
    "train, valid = hf.split_frame(ratios=[.8], seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d2f343",
   "metadata": {},
   "source": [
    "First, we simply use the h2o.H2OFrame function to convert our pandas DataFrame\n",
    "to an H2OFrame. Then we set the target column as a \"factor\" datatype, meaning it\n",
    "is a categorical variable. This is necessary so that the random forest will perform\n",
    "categorization and not regression. We could also convert other non-string columns\n",
    "that should be categorical to factors to be more thorough. Lastly, we break up our\n",
    "data into training and validation sets, with 80% of the data going to training. Now\n",
    "we can fit our model to the data and evaluate performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6fd1b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf Model Build progress: |██████████████████████████████████████████████████████| (done) 100%\n",
      "Model Details\n",
      "=============\n",
      "H2ORandomForestEstimator :  Distributed Random Forest\n",
      "Model Key:  DRF_model_python_1653093399467_1\n",
      "\n",
      "\n",
      "Model Summary: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>number_of_trees</th>\n",
       "      <th>number_of_internal_trees</th>\n",
       "      <th>model_size_in_bytes</th>\n",
       "      <th>min_depth</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>mean_depth</th>\n",
       "      <th>min_leaves</th>\n",
       "      <th>max_leaves</th>\n",
       "      <th>mean_leaves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>98460.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>73.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     number_of_trees  number_of_internal_trees  model_size_in_bytes  \\\n",
       "0              100.0                     100.0              98460.0   \n",
       "\n",
       "   min_depth  max_depth  mean_depth  min_leaves  max_leaves  mean_leaves  \n",
       "0       10.0       10.0        10.0        48.0       100.0        73.38  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: drf\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.06572158665417781\n",
      "RMSE: 0.2563622176807218\n",
      "LogLoss: 0.2623794357429694\n",
      "Mean Per-Class Error: 0.3885067845138802\n",
      "AUC: 0.628973193908461\n",
      "AUCPR: 0.18402234344105498\n",
      "Gini: 0.2579463878169219\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.1487879599294355: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Error</th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>973.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0.1218</td>\n",
       "      <td>(135.0/1108.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>57.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.6552</td>\n",
       "      <td>(57.0/87.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Total</td>\n",
       "      <td>1030.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>0.1607</td>\n",
       "      <td>(192.0/1195.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0      1   Error             Rate\n",
       "0      0   973.0  135.0  0.1218   (135.0/1108.0)\n",
       "1      1    57.0   30.0  0.6552      (57.0/87.0)\n",
       "2  Total  1030.0  165.0  0.1607   (192.0/1195.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>threshold</th>\n",
       "      <th>value</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>max f1</td>\n",
       "      <td>0.148788</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max f2</td>\n",
       "      <td>0.091521</td>\n",
       "      <td>0.319293</td>\n",
       "      <td>202.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>max f0point5</td>\n",
       "      <td>0.305326</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>max accuracy</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.929707</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max precision</td>\n",
       "      <td>0.760347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max recall</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>397.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>max specificity</td>\n",
       "      <td>0.760347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>max absolute_mcc</td>\n",
       "      <td>0.305326</td>\n",
       "      <td>0.206592</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>max min_per_class_accuracy</td>\n",
       "      <td>0.073032</td>\n",
       "      <td>0.595668</td>\n",
       "      <td>235.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max mean_per_class_accuracy</td>\n",
       "      <td>0.136173</td>\n",
       "      <td>0.618356</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>max tns</td>\n",
       "      <td>0.760347</td>\n",
       "      <td>1108.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>max fns</td>\n",
       "      <td>0.760347</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>max fps</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1108.000000</td>\n",
       "      <td>399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max tps</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>397.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>max tnr</td>\n",
       "      <td>0.760347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>max fnr</td>\n",
       "      <td>0.760347</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>max fpr</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max tpr</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>397.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         metric  threshold        value    idx\n",
       "0                        max f1   0.148788     0.238095  117.0\n",
       "1                        max f2   0.091521     0.319293  202.0\n",
       "2                  max f0point5   0.305326     0.294118   23.0\n",
       "3                  max accuracy   0.546875     0.929707    2.0\n",
       "4                 max precision   0.760347     1.000000    0.0\n",
       "5                    max recall   0.001210     1.000000  397.0\n",
       "6               max specificity   0.760347     1.000000    0.0\n",
       "7              max absolute_mcc   0.305326     0.206592   23.0\n",
       "8    max min_per_class_accuracy   0.073032     0.595668  235.0\n",
       "9   max mean_per_class_accuracy   0.136173     0.618356  130.0\n",
       "10                      max tns   0.760347  1108.000000    0.0\n",
       "11                      max fns   0.760347    86.000000    0.0\n",
       "12                      max fps   0.000000  1108.000000  399.0\n",
       "13                      max tps   0.001210    87.000000  397.0\n",
       "14                      max tnr   0.760347     1.000000    0.0\n",
       "15                      max fnr   0.760347     0.988506    0.0\n",
       "16                      max fpr   0.000000     1.000000  399.0\n",
       "17                      max tpr   0.001210     1.000000  397.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate:  7.28 %, avg score:  7.98 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>cumulative_data_fraction</th>\n",
       "      <th>lower_threshold</th>\n",
       "      <th>lift</th>\n",
       "      <th>cumulative_lift</th>\n",
       "      <th>response_rate</th>\n",
       "      <th>score</th>\n",
       "      <th>cumulative_response_rate</th>\n",
       "      <th>cumulative_score</th>\n",
       "      <th>capture_rate</th>\n",
       "      <th>cumulative_capture_rate</th>\n",
       "      <th>gain</th>\n",
       "      <th>cumulative_gain</th>\n",
       "      <th>kolmogorov_smirnov</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.010042</td>\n",
       "      <td>0.380633</td>\n",
       "      <td>5.723180</td>\n",
       "      <td>5.723180</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.468729</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.468729</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>472.318008</td>\n",
       "      <td>472.318008</td>\n",
       "      <td>0.051154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.020084</td>\n",
       "      <td>0.305250</td>\n",
       "      <td>5.723180</td>\n",
       "      <td>5.723180</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.332958</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.400844</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.114943</td>\n",
       "      <td>472.318008</td>\n",
       "      <td>472.318008</td>\n",
       "      <td>0.102307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.030126</td>\n",
       "      <td>0.268633</td>\n",
       "      <td>1.144636</td>\n",
       "      <td>4.196999</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.287123</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.362937</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.126437</td>\n",
       "      <td>14.463602</td>\n",
       "      <td>319.699872</td>\n",
       "      <td>0.103874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.040167</td>\n",
       "      <td>0.243092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.147749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252241</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.335263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126437</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>214.774904</td>\n",
       "      <td>0.093043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.050209</td>\n",
       "      <td>0.222838</td>\n",
       "      <td>3.433908</td>\n",
       "      <td>3.204981</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.235239</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.315258</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.160920</td>\n",
       "      <td>243.390805</td>\n",
       "      <td>220.498084</td>\n",
       "      <td>0.119403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.100418</td>\n",
       "      <td>0.171724</td>\n",
       "      <td>1.144636</td>\n",
       "      <td>2.174808</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.193983</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>0.254620</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.218391</td>\n",
       "      <td>14.463602</td>\n",
       "      <td>117.480843</td>\n",
       "      <td>0.127236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.150628</td>\n",
       "      <td>0.140549</td>\n",
       "      <td>2.518199</td>\n",
       "      <td>2.289272</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.156625</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.221955</td>\n",
       "      <td>0.126437</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>151.819923</td>\n",
       "      <td>128.927203</td>\n",
       "      <td>0.209449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.122899</td>\n",
       "      <td>0.931229</td>\n",
       "      <td>1.954023</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.131879</td>\n",
       "      <td>0.142259</td>\n",
       "      <td>0.199719</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.390805</td>\n",
       "      <td>-6.877070</td>\n",
       "      <td>95.402299</td>\n",
       "      <td>0.205787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.300418</td>\n",
       "      <td>0.096420</td>\n",
       "      <td>1.144636</td>\n",
       "      <td>1.683476</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.109065</td>\n",
       "      <td>0.122563</td>\n",
       "      <td>0.169417</td>\n",
       "      <td>0.114943</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>14.463602</td>\n",
       "      <td>68.347581</td>\n",
       "      <td>0.221451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.074970</td>\n",
       "      <td>0.807978</td>\n",
       "      <td>1.465517</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.085198</td>\n",
       "      <td>0.106695</td>\n",
       "      <td>0.148450</td>\n",
       "      <td>0.080460</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>-19.202164</td>\n",
       "      <td>46.551724</td>\n",
       "      <td>0.200828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.500418</td>\n",
       "      <td>0.058564</td>\n",
       "      <td>0.572318</td>\n",
       "      <td>1.286280</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.066419</td>\n",
       "      <td>0.093645</td>\n",
       "      <td>0.131989</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.643678</td>\n",
       "      <td>-42.768199</td>\n",
       "      <td>28.627994</td>\n",
       "      <td>0.154508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.045009</td>\n",
       "      <td>0.807978</td>\n",
       "      <td>1.206897</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.051912</td>\n",
       "      <td>0.087866</td>\n",
       "      <td>0.118699</td>\n",
       "      <td>0.080460</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>-19.202164</td>\n",
       "      <td>20.689655</td>\n",
       "      <td>0.133885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.699582</td>\n",
       "      <td>0.033639</td>\n",
       "      <td>0.461702</td>\n",
       "      <td>1.100822</td>\n",
       "      <td>0.033613</td>\n",
       "      <td>0.038610</td>\n",
       "      <td>0.080144</td>\n",
       "      <td>0.107299</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.770115</td>\n",
       "      <td>-53.829808</td>\n",
       "      <td>10.082220</td>\n",
       "      <td>0.076072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.022548</td>\n",
       "      <td>0.915709</td>\n",
       "      <td>1.077586</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.028672</td>\n",
       "      <td>0.078452</td>\n",
       "      <td>0.097429</td>\n",
       "      <td>0.091954</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>-8.429119</td>\n",
       "      <td>7.758621</td>\n",
       "      <td>0.066943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.899582</td>\n",
       "      <td>0.008791</td>\n",
       "      <td>0.807978</td>\n",
       "      <td>1.047741</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.015360</td>\n",
       "      <td>0.076279</td>\n",
       "      <td>0.088344</td>\n",
       "      <td>0.080460</td>\n",
       "      <td>0.942529</td>\n",
       "      <td>-19.202164</td>\n",
       "      <td>4.774125</td>\n",
       "      <td>0.046319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.572318</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.003498</td>\n",
       "      <td>0.072803</td>\n",
       "      <td>0.079824</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-42.768199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    group  cumulative_data_fraction  lower_threshold      lift  \\\n",
       "0       1                  0.010042         0.380633  5.723180   \n",
       "1       2                  0.020084         0.305250  5.723180   \n",
       "2       3                  0.030126         0.268633  1.144636   \n",
       "3       4                  0.040167         0.243092  0.000000   \n",
       "4       5                  0.050209         0.222838  3.433908   \n",
       "5       6                  0.100418         0.171724  1.144636   \n",
       "6       7                  0.150628         0.140549  2.518199   \n",
       "7       8                  0.200000         0.122899  0.931229   \n",
       "8       9                  0.300418         0.096420  1.144636   \n",
       "9      10                  0.400000         0.074970  0.807978   \n",
       "10     11                  0.500418         0.058564  0.572318   \n",
       "11     12                  0.600000         0.045009  0.807978   \n",
       "12     13                  0.699582         0.033639  0.461702   \n",
       "13     14                  0.800000         0.022548  0.915709   \n",
       "14     15                  0.899582         0.008791  0.807978   \n",
       "15     16                  1.000000         0.000000  0.572318   \n",
       "\n",
       "    cumulative_lift  response_rate     score  cumulative_response_rate  \\\n",
       "0          5.723180       0.416667  0.468729                  0.416667   \n",
       "1          5.723180       0.416667  0.332958                  0.416667   \n",
       "2          4.196999       0.083333  0.287123                  0.305556   \n",
       "3          3.147749       0.000000  0.252241                  0.229167   \n",
       "4          3.204981       0.250000  0.235239                  0.233333   \n",
       "5          2.174808       0.083333  0.193983                  0.158333   \n",
       "6          2.289272       0.183333  0.156625                  0.166667   \n",
       "7          1.954023       0.067797  0.131879                  0.142259   \n",
       "8          1.683476       0.083333  0.109065                  0.122563   \n",
       "9          1.465517       0.058824  0.085198                  0.106695   \n",
       "10         1.286280       0.041667  0.066419                  0.093645   \n",
       "11         1.206897       0.058824  0.051912                  0.087866   \n",
       "12         1.100822       0.033613  0.038610                  0.080144   \n",
       "13         1.077586       0.066667  0.028672                  0.078452   \n",
       "14         1.047741       0.058824  0.015360                  0.076279   \n",
       "15         1.000000       0.041667  0.003498                  0.072803   \n",
       "\n",
       "    cumulative_score  capture_rate  cumulative_capture_rate        gain  \\\n",
       "0           0.468729      0.057471                 0.057471  472.318008   \n",
       "1           0.400844      0.057471                 0.114943  472.318008   \n",
       "2           0.362937      0.011494                 0.126437   14.463602   \n",
       "3           0.335263      0.000000                 0.126437 -100.000000   \n",
       "4           0.315258      0.034483                 0.160920  243.390805   \n",
       "5           0.254620      0.057471                 0.218391   14.463602   \n",
       "6           0.221955      0.126437                 0.344828  151.819923   \n",
       "7           0.199719      0.045977                 0.390805   -6.877070   \n",
       "8           0.169417      0.114943                 0.505747   14.463602   \n",
       "9           0.148450      0.080460                 0.586207  -19.202164   \n",
       "10          0.131989      0.057471                 0.643678  -42.768199   \n",
       "11          0.118699      0.080460                 0.724138  -19.202164   \n",
       "12          0.107299      0.045977                 0.770115  -53.829808   \n",
       "13          0.097429      0.091954                 0.862069   -8.429119   \n",
       "14          0.088344      0.080460                 0.942529  -19.202164   \n",
       "15          0.079824      0.057471                 1.000000  -42.768199   \n",
       "\n",
       "    cumulative_gain  kolmogorov_smirnov  \n",
       "0        472.318008            0.051154  \n",
       "1        472.318008            0.102307  \n",
       "2        319.699872            0.103874  \n",
       "3        214.774904            0.093043  \n",
       "4        220.498084            0.119403  \n",
       "5        117.480843            0.127236  \n",
       "6        128.927203            0.209449  \n",
       "7         95.402299            0.205787  \n",
       "8         68.347581            0.221451  \n",
       "9         46.551724            0.200828  \n",
       "10        28.627994            0.154508  \n",
       "11        20.689655            0.133885  \n",
       "12        10.082220            0.076072  \n",
       "13         7.758621            0.066943  \n",
       "14         4.774125            0.046319  \n",
       "15         0.000000            0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: drf\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.08937720731268622\n",
      "RMSE: 0.29896021024993646\n",
      "LogLoss: 0.3247372532149751\n",
      "Mean Per-Class Error: 0.41333333333333333\n",
      "AUC: 0.6216969696969697\n",
      "AUCPR: 0.13274419818173797\n",
      "Gini: 0.2433939393939395\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.11625296175479893: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Error</th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>(44.0/275.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>(20.0/30.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Total</td>\n",
       "      <td>251.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>(64.0/305.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0     1   Error           Rate\n",
       "0      0  231.0  44.0    0.16   (44.0/275.0)\n",
       "1      1   20.0  10.0  0.6667    (20.0/30.0)\n",
       "2  Total  251.0  54.0  0.2098   (64.0/305.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>threshold</th>\n",
       "      <th>value</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>max f1</td>\n",
       "      <td>0.116253</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max f2</td>\n",
       "      <td>0.031641</td>\n",
       "      <td>0.397727</td>\n",
       "      <td>231.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>max f0point5</td>\n",
       "      <td>0.116253</td>\n",
       "      <td>0.203252</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>max accuracy</td>\n",
       "      <td>0.348601</td>\n",
       "      <td>0.898361</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max precision</td>\n",
       "      <td>0.116253</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max recall</td>\n",
       "      <td>0.011668</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>max specificity</td>\n",
       "      <td>0.348601</td>\n",
       "      <td>0.996364</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>max absolute_mcc</td>\n",
       "      <td>0.025834</td>\n",
       "      <td>0.135755</td>\n",
       "      <td>244.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>max min_per_class_accuracy</td>\n",
       "      <td>0.068463</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>122.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max mean_per_class_accuracy</td>\n",
       "      <td>0.048440</td>\n",
       "      <td>0.599697</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>max tns</td>\n",
       "      <td>0.348601</td>\n",
       "      <td>274.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>max fns</td>\n",
       "      <td>0.348601</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>max fps</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>303.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max tps</td>\n",
       "      <td>0.011668</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>max tnr</td>\n",
       "      <td>0.348601</td>\n",
       "      <td>0.996364</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>max fnr</td>\n",
       "      <td>0.348601</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>max fpr</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>303.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max tpr</td>\n",
       "      <td>0.011668</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>280.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         metric  threshold       value    idx\n",
       "0                        max f1   0.116253    0.238095   53.0\n",
       "1                        max f2   0.031641    0.397727  231.0\n",
       "2                  max f0point5   0.116253    0.203252   53.0\n",
       "3                  max accuracy   0.348601    0.898361    0.0\n",
       "4                 max precision   0.116253    0.185185   53.0\n",
       "5                    max recall   0.011668    1.000000  280.0\n",
       "6               max specificity   0.348601    0.996364    0.0\n",
       "7              max absolute_mcc   0.025834    0.135755  244.0\n",
       "8    max min_per_class_accuracy   0.068463    0.566667  122.0\n",
       "9   max mean_per_class_accuracy   0.048440    0.599697  178.0\n",
       "10                      max tns   0.348601  274.000000    0.0\n",
       "11                      max fns   0.348601   30.000000    0.0\n",
       "12                      max fps   0.000000  275.000000  303.0\n",
       "13                      max tps   0.011668   30.000000  280.0\n",
       "14                      max tnr   0.348601    0.996364    0.0\n",
       "15                      max fnr   0.348601    1.000000    0.0\n",
       "16                      max fpr   0.000000    1.000000  303.0\n",
       "17                      max tpr   0.011668    1.000000  280.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate:  9.84 %, avg score:  7.31 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>cumulative_data_fraction</th>\n",
       "      <th>lower_threshold</th>\n",
       "      <th>lift</th>\n",
       "      <th>cumulative_lift</th>\n",
       "      <th>response_rate</th>\n",
       "      <th>score</th>\n",
       "      <th>cumulative_response_rate</th>\n",
       "      <th>cumulative_score</th>\n",
       "      <th>capture_rate</th>\n",
       "      <th>cumulative_capture_rate</th>\n",
       "      <th>gain</th>\n",
       "      <th>cumulative_gain</th>\n",
       "      <th>kolmogorov_smirnov</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.013115</td>\n",
       "      <td>0.276121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-0.014545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.022951</td>\n",
       "      <td>0.246785</td>\n",
       "      <td>3.388889</td>\n",
       "      <td>1.452381</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.252915</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285289</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>238.888889</td>\n",
       "      <td>45.238095</td>\n",
       "      <td>0.011515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>0.239543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.016667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241670</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.272203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.000606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.042623</td>\n",
       "      <td>0.219391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.782051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225826</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.261501</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-21.794872</td>\n",
       "      <td>-0.010303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.052459</td>\n",
       "      <td>0.201147</td>\n",
       "      <td>3.388889</td>\n",
       "      <td>1.270833</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.211545</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.252134</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>238.888889</td>\n",
       "      <td>27.083333</td>\n",
       "      <td>0.015758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.101639</td>\n",
       "      <td>0.151158</td>\n",
       "      <td>2.033333</td>\n",
       "      <td>1.639785</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.170210</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.212494</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>103.333333</td>\n",
       "      <td>63.978495</td>\n",
       "      <td>0.072121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.150820</td>\n",
       "      <td>0.129422</td>\n",
       "      <td>1.355556</td>\n",
       "      <td>1.547101</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.140410</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>0.188988</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>35.555556</td>\n",
       "      <td>54.710145</td>\n",
       "      <td>0.091515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.105421</td>\n",
       "      <td>2.033333</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.116746</td>\n",
       "      <td>0.163934</td>\n",
       "      <td>0.171224</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>103.333333</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>0.147879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.301639</td>\n",
       "      <td>0.086710</td>\n",
       "      <td>0.655914</td>\n",
       "      <td>1.326087</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.095426</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.145683</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-34.408602</td>\n",
       "      <td>32.608696</td>\n",
       "      <td>0.109091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.069406</td>\n",
       "      <td>1.355556</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.077323</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>0.128873</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>35.555556</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.147879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.501639</td>\n",
       "      <td>0.057363</td>\n",
       "      <td>0.983871</td>\n",
       "      <td>1.262527</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.062532</td>\n",
       "      <td>0.124183</td>\n",
       "      <td>0.115432</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>-1.612903</td>\n",
       "      <td>26.252723</td>\n",
       "      <td>0.146061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.046175</td>\n",
       "      <td>1.355556</td>\n",
       "      <td>1.277778</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.052265</td>\n",
       "      <td>0.125683</td>\n",
       "      <td>0.105076</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>35.555556</td>\n",
       "      <td>27.777778</td>\n",
       "      <td>0.184848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.698361</td>\n",
       "      <td>0.036911</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>1.193271</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.041713</td>\n",
       "      <td>0.117371</td>\n",
       "      <td>0.096152</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>-32.222222</td>\n",
       "      <td>19.327074</td>\n",
       "      <td>0.149697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.025938</td>\n",
       "      <td>0.983871</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.031916</td>\n",
       "      <td>0.114754</td>\n",
       "      <td>0.087991</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>-1.612903</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>0.147879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.898361</td>\n",
       "      <td>0.013266</td>\n",
       "      <td>0.338889</td>\n",
       "      <td>1.076034</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019308</td>\n",
       "      <td>0.105839</td>\n",
       "      <td>0.080471</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>-66.111111</td>\n",
       "      <td>7.603406</td>\n",
       "      <td>0.075758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.327957</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.007465</td>\n",
       "      <td>0.098361</td>\n",
       "      <td>0.073051</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-67.204301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    group  cumulative_data_fraction  lower_threshold      lift  \\\n",
       "0       1                  0.013115         0.276121  0.000000   \n",
       "1       2                  0.022951         0.246785  3.388889   \n",
       "2       3                  0.032787         0.239543  0.000000   \n",
       "3       4                  0.042623         0.219391  0.000000   \n",
       "4       5                  0.052459         0.201147  3.388889   \n",
       "5       6                  0.101639         0.151158  2.033333   \n",
       "6       7                  0.150820         0.129422  1.355556   \n",
       "7       8                  0.200000         0.105421  2.033333   \n",
       "8       9                  0.301639         0.086710  0.655914   \n",
       "9      10                  0.400000         0.069406  1.355556   \n",
       "10     11                  0.501639         0.057363  0.983871   \n",
       "11     12                  0.600000         0.046175  1.355556   \n",
       "12     13                  0.698361         0.036911  0.677778   \n",
       "13     14                  0.800000         0.025938  0.983871   \n",
       "14     15                  0.898361         0.013266  0.338889   \n",
       "15     16                  1.000000         0.000000  0.327957   \n",
       "\n",
       "    cumulative_lift  response_rate     score  cumulative_response_rate  \\\n",
       "0          0.000000       0.000000  0.309569                  0.000000   \n",
       "1          1.452381       0.333333  0.252915                  0.142857   \n",
       "2          1.016667       0.000000  0.241670                  0.100000   \n",
       "3          0.782051       0.000000  0.225826                  0.076923   \n",
       "4          1.270833       0.333333  0.211545                  0.125000   \n",
       "5          1.639785       0.200000  0.170210                  0.161290   \n",
       "6          1.547101       0.133333  0.140410                  0.152174   \n",
       "7          1.666667       0.200000  0.116746                  0.163934   \n",
       "8          1.326087       0.064516  0.095426                  0.130435   \n",
       "9          1.333333       0.133333  0.077323                  0.131148   \n",
       "10         1.262527       0.096774  0.062532                  0.124183   \n",
       "11         1.277778       0.133333  0.052265                  0.125683   \n",
       "12         1.193271       0.066667  0.041713                  0.117371   \n",
       "13         1.166667       0.096774  0.031916                  0.114754   \n",
       "14         1.076034       0.033333  0.019308                  0.105839   \n",
       "15         1.000000       0.032258  0.007465                  0.098361   \n",
       "\n",
       "    cumulative_score  capture_rate  cumulative_capture_rate        gain  \\\n",
       "0           0.309569      0.000000                 0.000000 -100.000000   \n",
       "1           0.285289      0.033333                 0.033333  238.888889   \n",
       "2           0.272203      0.000000                 0.033333 -100.000000   \n",
       "3           0.261501      0.000000                 0.033333 -100.000000   \n",
       "4           0.252134      0.033333                 0.066667  238.888889   \n",
       "5           0.212494      0.100000                 0.166667  103.333333   \n",
       "6           0.188988      0.066667                 0.233333   35.555556   \n",
       "7           0.171224      0.100000                 0.333333  103.333333   \n",
       "8           0.145683      0.066667                 0.400000  -34.408602   \n",
       "9           0.128873      0.133333                 0.533333   35.555556   \n",
       "10          0.115432      0.100000                 0.633333   -1.612903   \n",
       "11          0.105076      0.133333                 0.766667   35.555556   \n",
       "12          0.096152      0.066667                 0.833333  -32.222222   \n",
       "13          0.087991      0.100000                 0.933333   -1.612903   \n",
       "14          0.080471      0.033333                 0.966667  -66.111111   \n",
       "15          0.073051      0.033333                 1.000000  -67.204301   \n",
       "\n",
       "    cumulative_gain  kolmogorov_smirnov  \n",
       "0       -100.000000           -0.014545  \n",
       "1         45.238095            0.011515  \n",
       "2          1.666667            0.000606  \n",
       "3        -21.794872           -0.010303  \n",
       "4         27.083333            0.015758  \n",
       "5         63.978495            0.072121  \n",
       "6         54.710145            0.091515  \n",
       "7         66.666667            0.147879  \n",
       "8         32.608696            0.109091  \n",
       "9         33.333333            0.147879  \n",
       "10        26.252723            0.146061  \n",
       "11        27.777778            0.184848  \n",
       "12        19.327074            0.149697  \n",
       "13        16.666667            0.147879  \n",
       "14         7.603406            0.075758  \n",
       "15         0.000000            0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>duration</th>\n",
       "      <th>number_of_trees</th>\n",
       "      <th>training_rmse</th>\n",
       "      <th>training_logloss</th>\n",
       "      <th>training_auc</th>\n",
       "      <th>training_pr_auc</th>\n",
       "      <th>training_lift</th>\n",
       "      <th>training_classification_error</th>\n",
       "      <th>validation_rmse</th>\n",
       "      <th>validation_logloss</th>\n",
       "      <th>validation_auc</th>\n",
       "      <th>validation_pr_auc</th>\n",
       "      <th>validation_lift</th>\n",
       "      <th>validation_classification_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>2022-05-21 00:36:44</td>\n",
       "      <td>0.100 sec</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>2022-05-21 00:36:45</td>\n",
       "      <td>0.490 sec</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.330199</td>\n",
       "      <td>2.677670</td>\n",
       "      <td>0.606980</td>\n",
       "      <td>0.105789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188372</td>\n",
       "      <td>0.371217</td>\n",
       "      <td>4.038783</td>\n",
       "      <td>0.507091</td>\n",
       "      <td>0.100345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.901639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>2022-05-21 00:36:45</td>\n",
       "      <td>0.566 sec</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.349798</td>\n",
       "      <td>3.316352</td>\n",
       "      <td>0.572616</td>\n",
       "      <td>0.094259</td>\n",
       "      <td>0.807978</td>\n",
       "      <td>0.263848</td>\n",
       "      <td>0.338889</td>\n",
       "      <td>2.420950</td>\n",
       "      <td>0.564970</td>\n",
       "      <td>0.121360</td>\n",
       "      <td>1.452381</td>\n",
       "      <td>0.288525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>2022-05-21 00:36:45</td>\n",
       "      <td>0.651 sec</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.332029</td>\n",
       "      <td>2.504306</td>\n",
       "      <td>0.622149</td>\n",
       "      <td>0.104908</td>\n",
       "      <td>0.832463</td>\n",
       "      <td>0.295174</td>\n",
       "      <td>0.329195</td>\n",
       "      <td>1.698207</td>\n",
       "      <td>0.586606</td>\n",
       "      <td>0.120920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>2022-05-21 00:36:45</td>\n",
       "      <td>0.691 sec</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.312433</td>\n",
       "      <td>1.958585</td>\n",
       "      <td>0.612495</td>\n",
       "      <td>0.108770</td>\n",
       "      <td>2.197701</td>\n",
       "      <td>0.321608</td>\n",
       "      <td>0.319012</td>\n",
       "      <td>1.286316</td>\n",
       "      <td>0.574364</td>\n",
       "      <td>0.119849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>2022-05-21 00:36:45</td>\n",
       "      <td>0.742 sec</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.306573</td>\n",
       "      <td>1.833051</td>\n",
       "      <td>0.601577</td>\n",
       "      <td>0.112069</td>\n",
       "      <td>2.616311</td>\n",
       "      <td>0.358543</td>\n",
       "      <td>0.315375</td>\n",
       "      <td>1.075125</td>\n",
       "      <td>0.592606</td>\n",
       "      <td>0.121076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.504918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>2022-05-21 00:36:45</td>\n",
       "      <td>0.794 sec</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.294238</td>\n",
       "      <td>1.478228</td>\n",
       "      <td>0.592657</td>\n",
       "      <td>0.123309</td>\n",
       "      <td>4.578544</td>\n",
       "      <td>0.299369</td>\n",
       "      <td>0.312037</td>\n",
       "      <td>0.963506</td>\n",
       "      <td>0.598667</td>\n",
       "      <td>0.121907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.360656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>2022-05-21 00:36:45</td>\n",
       "      <td>0.832 sec</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.285802</td>\n",
       "      <td>1.298071</td>\n",
       "      <td>0.605329</td>\n",
       "      <td>0.133336</td>\n",
       "      <td>6.867816</td>\n",
       "      <td>0.285589</td>\n",
       "      <td>0.307491</td>\n",
       "      <td>0.852483</td>\n",
       "      <td>0.624182</td>\n",
       "      <td>0.136933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>2022-05-21 00:36:45</td>\n",
       "      <td>0.866 sec</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.283880</td>\n",
       "      <td>1.257848</td>\n",
       "      <td>0.597720</td>\n",
       "      <td>0.122439</td>\n",
       "      <td>5.723180</td>\n",
       "      <td>0.176930</td>\n",
       "      <td>0.306793</td>\n",
       "      <td>0.846533</td>\n",
       "      <td>0.622848</td>\n",
       "      <td>0.132700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.409836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>2022-05-21 00:36:45</td>\n",
       "      <td>0.902 sec</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.284506</td>\n",
       "      <td>1.197090</td>\n",
       "      <td>0.599531</td>\n",
       "      <td>0.115294</td>\n",
       "      <td>3.433908</td>\n",
       "      <td>0.169666</td>\n",
       "      <td>0.306215</td>\n",
       "      <td>0.746624</td>\n",
       "      <td>0.603576</td>\n",
       "      <td>0.123103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.462295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>2022-05-21 00:36:45</td>\n",
       "      <td>0.973 sec</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.280773</td>\n",
       "      <td>1.025798</td>\n",
       "      <td>0.614116</td>\n",
       "      <td>0.115083</td>\n",
       "      <td>3.433908</td>\n",
       "      <td>0.153390</td>\n",
       "      <td>0.306391</td>\n",
       "      <td>0.647360</td>\n",
       "      <td>0.606727</td>\n",
       "      <td>0.125435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.481967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td>2022-05-21 00:36:45</td>\n",
       "      <td>1.009 sec</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.281218</td>\n",
       "      <td>1.000805</td>\n",
       "      <td>0.600780</td>\n",
       "      <td>0.114329</td>\n",
       "      <td>3.433908</td>\n",
       "      <td>0.143581</td>\n",
       "      <td>0.304462</td>\n",
       "      <td>0.647523</td>\n",
       "      <td>0.599939</td>\n",
       "      <td>0.128839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.504918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>2022-05-21 00:36:45</td>\n",
       "      <td>1.056 sec</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.275503</td>\n",
       "      <td>0.831085</td>\n",
       "      <td>0.613726</td>\n",
       "      <td>0.123141</td>\n",
       "      <td>3.433908</td>\n",
       "      <td>0.134680</td>\n",
       "      <td>0.306693</td>\n",
       "      <td>0.653683</td>\n",
       "      <td>0.581091</td>\n",
       "      <td>0.123476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.537705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td></td>\n",
       "      <td>2022-05-21 00:36:45</td>\n",
       "      <td>1.113 sec</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.271910</td>\n",
       "      <td>0.770469</td>\n",
       "      <td>0.616648</td>\n",
       "      <td>0.141179</td>\n",
       "      <td>5.723180</td>\n",
       "      <td>0.157718</td>\n",
       "      <td>0.306135</td>\n",
       "      <td>0.652445</td>\n",
       "      <td>0.580788</td>\n",
       "      <td>0.126214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>2022-05-21 00:36:45</td>\n",
       "      <td>1.180 sec</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.270424</td>\n",
       "      <td>0.716337</td>\n",
       "      <td>0.623839</td>\n",
       "      <td>0.132079</td>\n",
       "      <td>3.433908</td>\n",
       "      <td>0.158424</td>\n",
       "      <td>0.306745</td>\n",
       "      <td>0.649758</td>\n",
       "      <td>0.575212</td>\n",
       "      <td>0.120100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td></td>\n",
       "      <td>2022-05-21 00:36:45</td>\n",
       "      <td>1.228 sec</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.268824</td>\n",
       "      <td>0.716227</td>\n",
       "      <td>0.615109</td>\n",
       "      <td>0.136273</td>\n",
       "      <td>4.578544</td>\n",
       "      <td>0.151591</td>\n",
       "      <td>0.306220</td>\n",
       "      <td>0.647635</td>\n",
       "      <td>0.572000</td>\n",
       "      <td>0.119599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.567213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td></td>\n",
       "      <td>2022-05-21 00:36:46</td>\n",
       "      <td>1.328 sec</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.268785</td>\n",
       "      <td>0.689415</td>\n",
       "      <td>0.619564</td>\n",
       "      <td>0.135300</td>\n",
       "      <td>4.226348</td>\n",
       "      <td>0.115481</td>\n",
       "      <td>0.305235</td>\n",
       "      <td>0.646044</td>\n",
       "      <td>0.577394</td>\n",
       "      <td>0.122038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.593443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td></td>\n",
       "      <td>2022-05-21 00:36:46</td>\n",
       "      <td>1.367 sec</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.266756</td>\n",
       "      <td>0.657821</td>\n",
       "      <td>0.622479</td>\n",
       "      <td>0.134384</td>\n",
       "      <td>3.169761</td>\n",
       "      <td>0.193305</td>\n",
       "      <td>0.304883</td>\n",
       "      <td>0.547277</td>\n",
       "      <td>0.584303</td>\n",
       "      <td>0.123583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.596721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td>2022-05-21 00:36:46</td>\n",
       "      <td>1.426 sec</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.265442</td>\n",
       "      <td>0.630321</td>\n",
       "      <td>0.628377</td>\n",
       "      <td>0.142229</td>\n",
       "      <td>3.433908</td>\n",
       "      <td>0.110460</td>\n",
       "      <td>0.304066</td>\n",
       "      <td>0.543653</td>\n",
       "      <td>0.594606</td>\n",
       "      <td>0.124921</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td></td>\n",
       "      <td>2022-05-21 00:36:46</td>\n",
       "      <td>1.459 sec</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.265342</td>\n",
       "      <td>0.578868</td>\n",
       "      <td>0.629336</td>\n",
       "      <td>0.137548</td>\n",
       "      <td>3.433908</td>\n",
       "      <td>0.181590</td>\n",
       "      <td>0.303699</td>\n",
       "      <td>0.544196</td>\n",
       "      <td>0.588848</td>\n",
       "      <td>0.126083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.186885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp    duration  number_of_trees  training_rmse  \\\n",
       "0     2022-05-21 00:36:44   0.100 sec              0.0            NaN   \n",
       "1     2022-05-21 00:36:45   0.490 sec              1.0       0.330199   \n",
       "2     2022-05-21 00:36:45   0.566 sec              2.0       0.349798   \n",
       "3     2022-05-21 00:36:45   0.651 sec              3.0       0.332029   \n",
       "4     2022-05-21 00:36:45   0.691 sec              4.0       0.312433   \n",
       "5     2022-05-21 00:36:45   0.742 sec              5.0       0.306573   \n",
       "6     2022-05-21 00:36:45   0.794 sec              6.0       0.294238   \n",
       "7     2022-05-21 00:36:45   0.832 sec              7.0       0.285802   \n",
       "8     2022-05-21 00:36:45   0.866 sec              8.0       0.283880   \n",
       "9     2022-05-21 00:36:45   0.902 sec              9.0       0.284506   \n",
       "10    2022-05-21 00:36:45   0.973 sec             10.0       0.280773   \n",
       "11    2022-05-21 00:36:45   1.009 sec             11.0       0.281218   \n",
       "12    2022-05-21 00:36:45   1.056 sec             12.0       0.275503   \n",
       "13    2022-05-21 00:36:45   1.113 sec             13.0       0.271910   \n",
       "14    2022-05-21 00:36:45   1.180 sec             14.0       0.270424   \n",
       "15    2022-05-21 00:36:45   1.228 sec             15.0       0.268824   \n",
       "16    2022-05-21 00:36:46   1.328 sec             16.0       0.268785   \n",
       "17    2022-05-21 00:36:46   1.367 sec             17.0       0.266756   \n",
       "18    2022-05-21 00:36:46   1.426 sec             18.0       0.265442   \n",
       "19    2022-05-21 00:36:46   1.459 sec             19.0       0.265342   \n",
       "\n",
       "    training_logloss  training_auc  training_pr_auc  training_lift  \\\n",
       "0                NaN           NaN              NaN            NaN   \n",
       "1           2.677670      0.606980         0.105789       0.000000   \n",
       "2           3.316352      0.572616         0.094259       0.807978   \n",
       "3           2.504306      0.622149         0.104908       0.832463   \n",
       "4           1.958585      0.612495         0.108770       2.197701   \n",
       "5           1.833051      0.601577         0.112069       2.616311   \n",
       "6           1.478228      0.592657         0.123309       4.578544   \n",
       "7           1.298071      0.605329         0.133336       6.867816   \n",
       "8           1.257848      0.597720         0.122439       5.723180   \n",
       "9           1.197090      0.599531         0.115294       3.433908   \n",
       "10          1.025798      0.614116         0.115083       3.433908   \n",
       "11          1.000805      0.600780         0.114329       3.433908   \n",
       "12          0.831085      0.613726         0.123141       3.433908   \n",
       "13          0.770469      0.616648         0.141179       5.723180   \n",
       "14          0.716337      0.623839         0.132079       3.433908   \n",
       "15          0.716227      0.615109         0.136273       4.578544   \n",
       "16          0.689415      0.619564         0.135300       4.226348   \n",
       "17          0.657821      0.622479         0.134384       3.169761   \n",
       "18          0.630321      0.628377         0.142229       3.433908   \n",
       "19          0.578868      0.629336         0.137548       3.433908   \n",
       "\n",
       "    training_classification_error  validation_rmse  validation_logloss  \\\n",
       "0                             NaN              NaN                 NaN   \n",
       "1                        0.188372         0.371217            4.038783   \n",
       "2                        0.263848         0.338889            2.420950   \n",
       "3                        0.295174         0.329195            1.698207   \n",
       "4                        0.321608         0.319012            1.286316   \n",
       "5                        0.358543         0.315375            1.075125   \n",
       "6                        0.299369         0.312037            0.963506   \n",
       "7                        0.285589         0.307491            0.852483   \n",
       "8                        0.176930         0.306793            0.846533   \n",
       "9                        0.169666         0.306215            0.746624   \n",
       "10                       0.153390         0.306391            0.647360   \n",
       "11                       0.143581         0.304462            0.647523   \n",
       "12                       0.134680         0.306693            0.653683   \n",
       "13                       0.157718         0.306135            0.652445   \n",
       "14                       0.158424         0.306745            0.649758   \n",
       "15                       0.151591         0.306220            0.647635   \n",
       "16                       0.115481         0.305235            0.646044   \n",
       "17                       0.193305         0.304883            0.547277   \n",
       "18                       0.110460         0.304066            0.543653   \n",
       "19                       0.181590         0.303699            0.544196   \n",
       "\n",
       "    validation_auc  validation_pr_auc  validation_lift  \\\n",
       "0              NaN                NaN              NaN   \n",
       "1         0.507091           0.100345         0.000000   \n",
       "2         0.564970           0.121360         1.452381   \n",
       "3         0.586606           0.120920         0.000000   \n",
       "4         0.574364           0.119849         0.000000   \n",
       "5         0.592606           0.121076         0.000000   \n",
       "6         0.598667           0.121907         0.000000   \n",
       "7         0.624182           0.136933         0.000000   \n",
       "8         0.622848           0.132700         0.000000   \n",
       "9         0.603576           0.123103         0.000000   \n",
       "10        0.606727           0.125435         0.000000   \n",
       "11        0.599939           0.128839         0.000000   \n",
       "12        0.581091           0.123476         0.000000   \n",
       "13        0.580788           0.126214         0.000000   \n",
       "14        0.575212           0.120100         0.000000   \n",
       "15        0.572000           0.119599         0.000000   \n",
       "16        0.577394           0.122038         0.000000   \n",
       "17        0.584303           0.123583         0.000000   \n",
       "18        0.594606           0.124921         0.000000   \n",
       "19        0.588848           0.126083         0.000000   \n",
       "\n",
       "    validation_classification_error  \n",
       "0                               NaN  \n",
       "1                          0.901639  \n",
       "2                          0.288525  \n",
       "3                          0.377049  \n",
       "4                          0.475410  \n",
       "5                          0.504918  \n",
       "6                          0.360656  \n",
       "7                          0.209836  \n",
       "8                          0.409836  \n",
       "9                          0.462295  \n",
       "10                         0.481967  \n",
       "11                         0.504918  \n",
       "12                         0.537705  \n",
       "13                         0.144262  \n",
       "14                         0.547541  \n",
       "15                         0.567213  \n",
       "16                         0.593443  \n",
       "17                         0.596721  \n",
       "18                         0.177049  \n",
       "19                         0.186885  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n",
      "\n",
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>relative_importance</th>\n",
       "      <th>scaled_importance</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SK_ID_CURR</td>\n",
       "      <td>568.539612</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.182759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMT_CREDIT</td>\n",
       "      <td>502.312469</td>\n",
       "      <td>0.883514</td>\n",
       "      <td>0.161470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMT_ANNUITY</td>\n",
       "      <td>500.242035</td>\n",
       "      <td>0.879872</td>\n",
       "      <td>0.160804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMT_INCOME_TOTAL</td>\n",
       "      <td>450.123322</td>\n",
       "      <td>0.791718</td>\n",
       "      <td>0.144693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMT_GOODS_PRICE</td>\n",
       "      <td>448.799347</td>\n",
       "      <td>0.789390</td>\n",
       "      <td>0.144268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CNT_CHILDREN</td>\n",
       "      <td>225.642838</td>\n",
       "      <td>0.396881</td>\n",
       "      <td>0.072533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NAME_EDUCATION_TYPE</td>\n",
       "      <td>119.711693</td>\n",
       "      <td>0.210560</td>\n",
       "      <td>0.038482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FLAG_OWN_CAR</td>\n",
       "      <td>118.369934</td>\n",
       "      <td>0.208200</td>\n",
       "      <td>0.038050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CODE_GENDER</td>\n",
       "      <td>78.648125</td>\n",
       "      <td>0.138334</td>\n",
       "      <td>0.025282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FLAG_OWN_REALTY</td>\n",
       "      <td>75.926506</td>\n",
       "      <td>0.133547</td>\n",
       "      <td>0.024407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NAME_CONTRACT_TYPE</td>\n",
       "      <td>22.561703</td>\n",
       "      <td>0.039684</td>\n",
       "      <td>0.007253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               variable  relative_importance  scaled_importance  percentage\n",
       "0            SK_ID_CURR           568.539612           1.000000    0.182759\n",
       "1            AMT_CREDIT           502.312469           0.883514    0.161470\n",
       "2           AMT_ANNUITY           500.242035           0.879872    0.160804\n",
       "3      AMT_INCOME_TOTAL           450.123322           0.791718    0.144693\n",
       "4       AMT_GOODS_PRICE           448.799347           0.789390    0.144268\n",
       "5          CNT_CHILDREN           225.642838           0.396881    0.072533\n",
       "6   NAME_EDUCATION_TYPE           119.711693           0.210560    0.038482\n",
       "7          FLAG_OWN_CAR           118.369934           0.208200    0.038050\n",
       "8           CODE_GENDER            78.648125           0.138334    0.025282\n",
       "9       FLAG_OWN_REALTY            75.926506           0.133547    0.024407\n",
       "10   NAME_CONTRACT_TYPE            22.561703           0.039684    0.007253"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from h2o.estimators import H2ORandomForestEstimator\n",
    "\n",
    "\n",
    "drf = H2ORandomForestEstimator(ntrees = 100, max_depth = 10, mtries = 3)\n",
    "\n",
    "feature_columns = hf.columns\n",
    "\n",
    "feature_columns.remove(\"TARGET\")\n",
    "\n",
    "target_column = \"TARGET\"\n",
    "\n",
    "drf.train(x=feature_columns,\n",
    "            y=target_column,\n",
    "            training_frame=train,\n",
    "            validation_frame=valid)\n",
    "drf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5996510f",
   "metadata": {},
   "source": [
    "We first import the class and initialize it, saving it in the variable drf, which stands\n",
    "for \"distributed random forest.\" Since H2O is ready to scale, it will use all CPU cores\n",
    "and resources available, so our random forest is distributed across resources. We set\n",
    "the same three hyperparameters we set for sklearn: the number of trees (ntrees),\n",
    "the depth of trees (max_depth), and the number of features to use at each node split\n",
    "(mtries). We could use GridSearch here...\n",
    "\n",
    "Next, we create a list of our columns that are the features from the columns of the\n",
    "H20Frame and remove the TARGET item from the list. Then we create a variable to\n",
    "store the target value column name (target_column). Finally, we train the model\n",
    "with drf.train, giving it our feature names, target name, and training and validation\n",
    "H2OFrames. This will display a progress bar to show how the fit is progressing\n",
    "(similar to what the h2o.H2OFrame function outputs). After it's fit, we can print out\n",
    "the drf variable to see the results by simply running it in a separate Jupyter notebook\n",
    "cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e91b6446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf prediction progress: |███████████████████████████████████████████████████████| (done) 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = drf.predict(train)\n",
    "\n",
    "(predictions['p1'] > 0.097).as_data_frame()['p1'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b009e0",
   "metadata": {},
   "source": [
    "The first line uses the model to make predictions on the provided H2OFrame,\n",
    "and returns an H2OFrame with three columns: predict, p0, and p1. These are the\n",
    "predictions (using a threshold of 0.5), the probability of class 0, and the probability of\n",
    "class 1. The second line takes our F1-optimized threshold and rounds the predictions\n",
    "for p1 up to 1 if they exceed this threshold. We also convert this to a pandas\n",
    "DataFrame with as_data_frame(), then select the p1 column, and convert it to a\n",
    "NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdbf3f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = h2o.save_model(model=drf, path=\"data\\models\", force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed47ab72",
   "metadata": {},
   "source": [
    "The save_model function simply takes our trained model and a path (which creates\n",
    "a folder with that name in our current directory). The force=True argument will\n",
    "overwrite the model files if they already exist. The save_path variable holds the full\n",
    "path to the model, which we can use to load it into memory with h2o.load_model.\n",
    "Lastly, H2O has a few nice convenience functions for tree models. It has a learning\n",
    "curve plot, which shows a metric on the y axis and the number of trees on the x axis.\n",
    "\n",
    "\n",
    "This can be used to optimize the number of trees without using grid search. We can\n",
    "plot this with drf.learning_curve_plot(). By default, it shows log loss, and in our\n",
    "case, the log loss for the validation set flattens out around 10 trees (so we shouldn't\n",
    "need more than that for optimal performance). Another convenient function it has is\n",
    "plotting variable importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3196e882",
   "metadata": {},
   "source": [
    "## Feature Importance from tree-based methods\n",
    "\n",
    "Feature importance, also called variable importance, can be calculated from treebased methods by summing the reduction in Gini or entropy over all the trees for each variable.\n",
    "\n",
    "So, if a particular variable is used to split the data and reduces the Gini or entropy value by a large amount, that feature is important for making predictions. This is a nice contrast to using coefficient-based feature importance from logistic or\n",
    "linear regression, because tree-based feature importances are non-linear. \n",
    "\n",
    "There are other ways of calculating feature importance as well, such as permutation feature importance and SHAP (SHapley Additive exPlanations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc650b76",
   "metadata": {},
   "source": [
    "### Using H2O for feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "beb08250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h2o.plot._plot_result._MObject at 0x1e9436f5bc8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "drf.varimp_plot(server=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00407c44",
   "metadata": {},
   "source": [
    "From this, we can see there is no single feature that vastly stands above the rest.\n",
    "However, it looks like the top five features have relatively more importance than the\n",
    "rest , and there is a somewhat large drop in importance after the fifth feature (CODE_\n",
    "GENDER)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bd826d",
   "metadata": {},
   "source": [
    "> One interesting thing about tree-based feature importances is they\n",
    "can change if we remove other features. Sometimes the changes\n",
    "can be dramatic and features will move drastically in their ranking.\n",
    "For example, try removing the topmost important feature and\n",
    "examining the feature importances again.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8f8b3c",
   "metadata": {},
   "source": [
    "## Using SKLEARN Random Forest Feature Importances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c395893b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\INNO\\Documents\\Python Development\\Practical Data Science\\final_env\\final_env\\lib\\site-packages\\yellowbrick\\base.py:246: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from yellowbrick.model_selection import feature_importances\n",
    "\n",
    "\n",
    "_ = feature_importances(gs.best_estimator_,\n",
    "                        x_train, \n",
    "                        y_train,\n",
    "                        colors = ['darkblue'] * features.shape[0]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5cdb50",
   "metadata": {},
   "source": [
    "We can see there are some differences in the feature importances between `H2O` and\n",
    "`sklearn`. Remember that a big difference was that we included the missing values\n",
    "and categorical columns (such as NAME_EDUCATION_TYPE) as-is with the H2O random forest, while we pre-processed our data for sklearn to convert everything to numeric values and removed all missing values.\n",
    "\n",
    "\n",
    "Random forests are a good model to try and often work well. However, another\n",
    "class of tree-based models was created that improves upon random forests and often outperforms them: boosted models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3eeaf2",
   "metadata": {},
   "source": [
    "## Boosted Trees: AdaBoost, xgbOOST, LightGBM, and CatBoost\n",
    "\n",
    "Boosted machine learning models were first introduced around 1989 and have been shown to perform well. Some of the more common boosting algorithms you will see are AdaBoost, gradient boosting, XGBoost, LightGBM, and CatBoost.\n",
    "\n",
    "\n",
    "XGBoost has been used to win several machine learning competitions (for example, on Kaggle), and was initially released in 2014. LightGBM was developed shortly after by Microsoft and released in 2016, while CatBoost was released in 2017. (For a more detailed history of boosting, see this paper: https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf.) \n",
    "\n",
    "These boosting algorithms have slightly different algorithms and implementations, and when trying models on a dataset, it doesn't hurt to try as many of them as you can. An easy way to do this is with the PyCaret\n",
    "package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b6de21",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "\n",
    "With AdaBoost, we can choose what our weak learner is, but usually it is a 1-split\n",
    "decision tree (called a \"stump\"). The `sklearn` package has classification and\n",
    "regression versions of AdaBoost, and we can use the classifier with our loan payment\n",
    "dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1d83e6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9180765805877115\n",
      "0.9146666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "adaboost = AdaBoostClassifier(n_estimators = 100, random_state = 42, learning_rate = 0.5)\n",
    "\n",
    "\n",
    "adaboost.fit(x_train, y_train)\n",
    "\n",
    "print(adaboost.score(x_train, y_train))\n",
    "print(adaboost.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b4910",
   "metadata": {},
   "source": [
    "Again, this works the same as all other sklearn ML algorithms – create the classifier,\n",
    "fit to the data, then score and make predictions. The two hyperparameters for\n",
    "AdaBoost in sklearn are the number of weak learners and the learning rate.\n",
    "\n",
    "\n",
    "These two interact – the lower the learning rate, the more estimators we will need\n",
    "to arrive at comparable performance. We can also change the base learner from a\n",
    "decision tree stump to another ML algorithm with the base_estimator argument.\n",
    "Our train and test scores here are about the same as the no information rate at 0.919.\n",
    "\n",
    "\n",
    "We can optimize the `hyperparameters` with the grid search or Bayesian search\n",
    "methods. But we can also use the pycaret package to easily search some hyperparameters with cross-validation.\n",
    "\n",
    "We will need to first set up our data (the datatypes should be checked,\n",
    "and confirmed by pressing Enter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0d58d3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_6e570\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6e570_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_6e570_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_6e570_row0_col0\" class=\"data row0 col0\" >session_id</td>\n",
       "      <td id=\"T_6e570_row0_col1\" class=\"data row0 col1\" >1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_6e570_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_6e570_row1_col1\" class=\"data row1 col1\" >TARGET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_6e570_row2_col0\" class=\"data row2 col0\" >Target Type</td>\n",
       "      <td id=\"T_6e570_row2_col1\" class=\"data row2 col1\" >Binary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_6e570_row3_col0\" class=\"data row3 col0\" >Label Encoded</td>\n",
       "      <td id=\"T_6e570_row3_col1\" class=\"data row3 col1\" >0: 0, 1: 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_6e570_row4_col0\" class=\"data row4 col0\" >Original Data</td>\n",
       "      <td id=\"T_6e570_row4_col1\" class=\"data row4 col1\" >(307217, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_6e570_row5_col0\" class=\"data row5 col0\" >Missing Values</td>\n",
       "      <td id=\"T_6e570_row5_col1\" class=\"data row5 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_6e570_row6_col0\" class=\"data row6 col0\" >Numeric Features</td>\n",
       "      <td id=\"T_6e570_row6_col1\" class=\"data row6 col1\" >4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_6e570_row7_col0\" class=\"data row7 col0\" >Categorical Features</td>\n",
       "      <td id=\"T_6e570_row7_col1\" class=\"data row7 col1\" >6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_6e570_row8_col0\" class=\"data row8 col0\" >Ordinal Features</td>\n",
       "      <td id=\"T_6e570_row8_col1\" class=\"data row8 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_6e570_row9_col0\" class=\"data row9 col0\" >High Cardinality Features</td>\n",
       "      <td id=\"T_6e570_row9_col1\" class=\"data row9 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_6e570_row10_col0\" class=\"data row10 col0\" >High Cardinality Method</td>\n",
       "      <td id=\"T_6e570_row10_col1\" class=\"data row10 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_6e570_row11_col0\" class=\"data row11 col0\" >Transformed Train Set</td>\n",
       "      <td id=\"T_6e570_row11_col1\" class=\"data row11 col1\" >(215051, 25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_6e570_row12_col0\" class=\"data row12 col0\" >Transformed Test Set</td>\n",
       "      <td id=\"T_6e570_row12_col1\" class=\"data row12 col1\" >(92166, 25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_6e570_row13_col0\" class=\"data row13 col0\" >Shuffle Train-Test</td>\n",
       "      <td id=\"T_6e570_row13_col1\" class=\"data row13 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_6e570_row14_col0\" class=\"data row14 col0\" >Stratify Train-Test</td>\n",
       "      <td id=\"T_6e570_row14_col1\" class=\"data row14 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_6e570_row15_col0\" class=\"data row15 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_6e570_row15_col1\" class=\"data row15 col1\" >StratifiedKFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_6e570_row16_col0\" class=\"data row16 col0\" >Fold Number</td>\n",
       "      <td id=\"T_6e570_row16_col1\" class=\"data row16 col1\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_6e570_row17_col0\" class=\"data row17 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_6e570_row17_col1\" class=\"data row17 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_6e570_row18_col0\" class=\"data row18 col0\" >Use GPU</td>\n",
       "      <td id=\"T_6e570_row18_col1\" class=\"data row18 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_6e570_row19_col0\" class=\"data row19 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_6e570_row19_col1\" class=\"data row19 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_6e570_row20_col0\" class=\"data row20 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_6e570_row20_col1\" class=\"data row20 col1\" >clf-default-name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_6e570_row21_col0\" class=\"data row21 col0\" >USI</td>\n",
       "      <td id=\"T_6e570_row21_col1\" class=\"data row21 col1\" >2243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_6e570_row22_col0\" class=\"data row22 col0\" >Imputation Type</td>\n",
       "      <td id=\"T_6e570_row22_col1\" class=\"data row22 col1\" >simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_6e570_row23_col0\" class=\"data row23 col0\" >Iterative Imputation Iteration</td>\n",
       "      <td id=\"T_6e570_row23_col1\" class=\"data row23 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_6e570_row24_col0\" class=\"data row24 col0\" >Numeric Imputer</td>\n",
       "      <td id=\"T_6e570_row24_col1\" class=\"data row24 col1\" >mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_6e570_row25_col0\" class=\"data row25 col0\" >Iterative Imputation Numeric Model</td>\n",
       "      <td id=\"T_6e570_row25_col1\" class=\"data row25 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_6e570_row26_col0\" class=\"data row26 col0\" >Categorical Imputer</td>\n",
       "      <td id=\"T_6e570_row26_col1\" class=\"data row26 col1\" >constant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_6e570_row27_col0\" class=\"data row27 col0\" >Iterative Imputation Categorical Model</td>\n",
       "      <td id=\"T_6e570_row27_col1\" class=\"data row27 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_6e570_row28_col0\" class=\"data row28 col0\" >Unknown Categoricals Handling</td>\n",
       "      <td id=\"T_6e570_row28_col1\" class=\"data row28 col1\" >least_frequent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "      <td id=\"T_6e570_row29_col0\" class=\"data row29 col0\" >Normalize</td>\n",
       "      <td id=\"T_6e570_row29_col1\" class=\"data row29 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "      <td id=\"T_6e570_row30_col0\" class=\"data row30 col0\" >Normalize Method</td>\n",
       "      <td id=\"T_6e570_row30_col1\" class=\"data row30 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "      <td id=\"T_6e570_row31_col0\" class=\"data row31 col0\" >Transformation</td>\n",
       "      <td id=\"T_6e570_row31_col1\" class=\"data row31 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
       "      <td id=\"T_6e570_row32_col0\" class=\"data row32 col0\" >Transformation Method</td>\n",
       "      <td id=\"T_6e570_row32_col1\" class=\"data row32 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
       "      <td id=\"T_6e570_row33_col0\" class=\"data row33 col0\" >PCA</td>\n",
       "      <td id=\"T_6e570_row33_col1\" class=\"data row33 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
       "      <td id=\"T_6e570_row34_col0\" class=\"data row34 col0\" >PCA Method</td>\n",
       "      <td id=\"T_6e570_row34_col1\" class=\"data row34 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
       "      <td id=\"T_6e570_row35_col0\" class=\"data row35 col0\" >PCA Components</td>\n",
       "      <td id=\"T_6e570_row35_col1\" class=\"data row35 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
       "      <td id=\"T_6e570_row36_col0\" class=\"data row36 col0\" >Ignore Low Variance</td>\n",
       "      <td id=\"T_6e570_row36_col1\" class=\"data row36 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
       "      <td id=\"T_6e570_row37_col0\" class=\"data row37 col0\" >Combine Rare Levels</td>\n",
       "      <td id=\"T_6e570_row37_col1\" class=\"data row37 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
       "      <td id=\"T_6e570_row38_col0\" class=\"data row38 col0\" >Rare Level Threshold</td>\n",
       "      <td id=\"T_6e570_row38_col1\" class=\"data row38 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row39\" class=\"row_heading level0 row39\" >39</th>\n",
       "      <td id=\"T_6e570_row39_col0\" class=\"data row39 col0\" >Numeric Binning</td>\n",
       "      <td id=\"T_6e570_row39_col1\" class=\"data row39 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row40\" class=\"row_heading level0 row40\" >40</th>\n",
       "      <td id=\"T_6e570_row40_col0\" class=\"data row40 col0\" >Remove Outliers</td>\n",
       "      <td id=\"T_6e570_row40_col1\" class=\"data row40 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row41\" class=\"row_heading level0 row41\" >41</th>\n",
       "      <td id=\"T_6e570_row41_col0\" class=\"data row41 col0\" >Outliers Threshold</td>\n",
       "      <td id=\"T_6e570_row41_col1\" class=\"data row41 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row42\" class=\"row_heading level0 row42\" >42</th>\n",
       "      <td id=\"T_6e570_row42_col0\" class=\"data row42 col0\" >Remove Multicollinearity</td>\n",
       "      <td id=\"T_6e570_row42_col1\" class=\"data row42 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row43\" class=\"row_heading level0 row43\" >43</th>\n",
       "      <td id=\"T_6e570_row43_col0\" class=\"data row43 col0\" >Multicollinearity Threshold</td>\n",
       "      <td id=\"T_6e570_row43_col1\" class=\"data row43 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row44\" class=\"row_heading level0 row44\" >44</th>\n",
       "      <td id=\"T_6e570_row44_col0\" class=\"data row44 col0\" >Clustering</td>\n",
       "      <td id=\"T_6e570_row44_col1\" class=\"data row44 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row45\" class=\"row_heading level0 row45\" >45</th>\n",
       "      <td id=\"T_6e570_row45_col0\" class=\"data row45 col0\" >Clustering Iteration</td>\n",
       "      <td id=\"T_6e570_row45_col1\" class=\"data row45 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row46\" class=\"row_heading level0 row46\" >46</th>\n",
       "      <td id=\"T_6e570_row46_col0\" class=\"data row46 col0\" >Polynomial Features</td>\n",
       "      <td id=\"T_6e570_row46_col1\" class=\"data row46 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row47\" class=\"row_heading level0 row47\" >47</th>\n",
       "      <td id=\"T_6e570_row47_col0\" class=\"data row47 col0\" >Polynomial Degree</td>\n",
       "      <td id=\"T_6e570_row47_col1\" class=\"data row47 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row48\" class=\"row_heading level0 row48\" >48</th>\n",
       "      <td id=\"T_6e570_row48_col0\" class=\"data row48 col0\" >Trignometry Features</td>\n",
       "      <td id=\"T_6e570_row48_col1\" class=\"data row48 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row49\" class=\"row_heading level0 row49\" >49</th>\n",
       "      <td id=\"T_6e570_row49_col0\" class=\"data row49 col0\" >Polynomial Threshold</td>\n",
       "      <td id=\"T_6e570_row49_col1\" class=\"data row49 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row50\" class=\"row_heading level0 row50\" >50</th>\n",
       "      <td id=\"T_6e570_row50_col0\" class=\"data row50 col0\" >Group Features</td>\n",
       "      <td id=\"T_6e570_row50_col1\" class=\"data row50 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row51\" class=\"row_heading level0 row51\" >51</th>\n",
       "      <td id=\"T_6e570_row51_col0\" class=\"data row51 col0\" >Feature Selection</td>\n",
       "      <td id=\"T_6e570_row51_col1\" class=\"data row51 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row52\" class=\"row_heading level0 row52\" >52</th>\n",
       "      <td id=\"T_6e570_row52_col0\" class=\"data row52 col0\" >Features Selection Threshold</td>\n",
       "      <td id=\"T_6e570_row52_col1\" class=\"data row52 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row53\" class=\"row_heading level0 row53\" >53</th>\n",
       "      <td id=\"T_6e570_row53_col0\" class=\"data row53 col0\" >Feature Interaction</td>\n",
       "      <td id=\"T_6e570_row53_col1\" class=\"data row53 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row54\" class=\"row_heading level0 row54\" >54</th>\n",
       "      <td id=\"T_6e570_row54_col0\" class=\"data row54 col0\" >Feature Ratio</td>\n",
       "      <td id=\"T_6e570_row54_col1\" class=\"data row54 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row55\" class=\"row_heading level0 row55\" >55</th>\n",
       "      <td id=\"T_6e570_row55_col0\" class=\"data row55 col0\" >Interaction Threshold</td>\n",
       "      <td id=\"T_6e570_row55_col1\" class=\"data row55 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row56\" class=\"row_heading level0 row56\" >56</th>\n",
       "      <td id=\"T_6e570_row56_col0\" class=\"data row56 col0\" >Fix Imbalance</td>\n",
       "      <td id=\"T_6e570_row56_col1\" class=\"data row56 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e570_level0_row57\" class=\"row_heading level0 row57\" >57</th>\n",
       "      <td id=\"T_6e570_row57_col0\" class=\"data row57 col0\" >Fix Imbalance Method</td>\n",
       "      <td id=\"T_6e570_row57_col1\" class=\"data row57 col1\" >SMOTE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x286e35d7910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pycaret.classification import setup, create_model, tune_model\n",
    "\n",
    "classification = setup(data=final_df, target = \"TARGET\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "10ca0d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7b3c2_row3_col0, #T_7b3c2_row3_col1, #T_7b3c2_row3_col2, #T_7b3c2_row3_col3, #T_7b3c2_row3_col4, #T_7b3c2_row3_col5, #T_7b3c2_row3_col6 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7b3c2\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7b3c2_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_7b3c2_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_7b3c2_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_7b3c2_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_7b3c2_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_7b3c2_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_7b3c2_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7b3c2_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7b3c2_row0_col0\" class=\"data row0 col0\" >0.9194</td>\n",
       "      <td id=\"T_7b3c2_row0_col1\" class=\"data row0 col1\" >0.6272</td>\n",
       "      <td id=\"T_7b3c2_row0_col2\" class=\"data row0 col2\" >0.0000</td>\n",
       "      <td id=\"T_7b3c2_row0_col3\" class=\"data row0 col3\" >0.0000</td>\n",
       "      <td id=\"T_7b3c2_row0_col4\" class=\"data row0 col4\" >0.0000</td>\n",
       "      <td id=\"T_7b3c2_row0_col5\" class=\"data row0 col5\" >0.0000</td>\n",
       "      <td id=\"T_7b3c2_row0_col6\" class=\"data row0 col6\" >0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b3c2_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7b3c2_row1_col0\" class=\"data row1 col0\" >0.9194</td>\n",
       "      <td id=\"T_7b3c2_row1_col1\" class=\"data row1 col1\" >0.6165</td>\n",
       "      <td id=\"T_7b3c2_row1_col2\" class=\"data row1 col2\" >0.0000</td>\n",
       "      <td id=\"T_7b3c2_row1_col3\" class=\"data row1 col3\" >0.0000</td>\n",
       "      <td id=\"T_7b3c2_row1_col4\" class=\"data row1 col4\" >0.0000</td>\n",
       "      <td id=\"T_7b3c2_row1_col5\" class=\"data row1 col5\" >0.0000</td>\n",
       "      <td id=\"T_7b3c2_row1_col6\" class=\"data row1 col6\" >0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b3c2_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_7b3c2_row2_col0\" class=\"data row2 col0\" >0.9194</td>\n",
       "      <td id=\"T_7b3c2_row2_col1\" class=\"data row2 col1\" >0.6156</td>\n",
       "      <td id=\"T_7b3c2_row2_col2\" class=\"data row2 col2\" >0.0000</td>\n",
       "      <td id=\"T_7b3c2_row2_col3\" class=\"data row2 col3\" >0.0000</td>\n",
       "      <td id=\"T_7b3c2_row2_col4\" class=\"data row2 col4\" >0.0000</td>\n",
       "      <td id=\"T_7b3c2_row2_col5\" class=\"data row2 col5\" >0.0000</td>\n",
       "      <td id=\"T_7b3c2_row2_col6\" class=\"data row2 col6\" >0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b3c2_level0_row3\" class=\"row_heading level0 row3\" >Mean</th>\n",
       "      <td id=\"T_7b3c2_row3_col0\" class=\"data row3 col0\" >0.9194</td>\n",
       "      <td id=\"T_7b3c2_row3_col1\" class=\"data row3 col1\" >0.6198</td>\n",
       "      <td id=\"T_7b3c2_row3_col2\" class=\"data row3 col2\" >0.0000</td>\n",
       "      <td id=\"T_7b3c2_row3_col3\" class=\"data row3 col3\" >0.0000</td>\n",
       "      <td id=\"T_7b3c2_row3_col4\" class=\"data row3 col4\" >0.0000</td>\n",
       "      <td id=\"T_7b3c2_row3_col5\" class=\"data row3 col5\" >0.0000</td>\n",
       "      <td id=\"T_7b3c2_row3_col6\" class=\"data row3 col6\" >0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b3c2_level0_row4\" class=\"row_heading level0 row4\" >SD</th>\n",
       "      <td id=\"T_7b3c2_row4_col0\" class=\"data row4 col0\" >0.0000</td>\n",
       "      <td id=\"T_7b3c2_row4_col1\" class=\"data row4 col1\" >0.0053</td>\n",
       "      <td id=\"T_7b3c2_row4_col2\" class=\"data row4 col2\" >0.0000</td>\n",
       "      <td id=\"T_7b3c2_row4_col3\" class=\"data row4 col3\" >0.0000</td>\n",
       "      <td id=\"T_7b3c2_row4_col4\" class=\"data row4 col4\" >0.0000</td>\n",
       "      <td id=\"T_7b3c2_row4_col5\" class=\"data row4 col5\" >0.0000</td>\n",
       "      <td id=\"T_7b3c2_row4_col6\" class=\"data row4 col6\" >0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x286d6824460>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ada_boost = create_model(\"ada\", fold = 3)\n",
    "\n",
    "tuned_ada_boost, grid_search = tune_model(ada_boost, fold = 3, return_tuner = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa624a86",
   "metadata": {},
   "source": [
    "We set the fold argument to 3 to use 3-fold cross-validation – the default is 10-\n",
    "fold, and this algorithm already takes a long time to fit. In fact, you may want to\n",
    "sample down the data to test this if it takes too long. This searches a default spread\n",
    "of hyperparameters and we can access the hyperparameters and scores from the\n",
    "gridsearch variable that was returned because we set return_tuner=True.\n",
    "\n",
    "\n",
    "The results from the hyperparameter search (if we use the default sklearn grid\n",
    "search) can be found from gridsearch.cv_results_['params'] and gridsearch.\n",
    "cv_results_['mean_test_score'] (for example, you might use zip() to combine and\n",
    "print these out).\n",
    "\n",
    "\n",
    "Another way to use AdaBoost is directly from sklearn with the AdaBoostClassifier\n",
    "and AdaBoostRegressor classes, along with using a tuner like the sklearn grid search\n",
    "or another package for searching hyperparameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a7a338",
   "metadata": {},
   "source": [
    "\n",
    "AdaBoost can work well for some problems but has been shown to sometimes\n",
    "not work well when the data has too much noise or a specific type of noise.\n",
    "Other boosting algorithms, such as the ones we will cover next, often outperform\n",
    "AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42804e3",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "XGBoost stands for \"extreme gradient boosting.\" It makes several improvements\n",
    "upon plain gradient boosting, such as using Newton boosting. Instead of finding\n",
    "the ideal multiplier to scale each weak learner by (which is like a step length in our\n",
    "gradient descent), XGBoost solves the direction and step length in one equation. By\n",
    "contrast, gradient boosting uses something called a line search to find the optimum\n",
    "multiplier (step length) for each weak learner. This means XGBoost can be faster than\n",
    "plain gradient boosting. It also is implemented in several coding languages, meaning\n",
    "it can be deployed in a number of situations. Additionally, it can be used with big\n",
    "data in a few different ways, such as Dask, H2O, Spark, and AWS SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6972e471",
   "metadata": {},
   "source": [
    "### XGBoost with PyCaret\n",
    "\n",
    "Again, we can use xgboost easily through pycaret, which, by default, searches the\n",
    "following hyperparameter space:\n",
    "\n",
    "\n",
    "- learning_rate: 0.0000001 to 0.5\n",
    "- n_estmators: 10-300 in steps of 10\n",
    "- subsample: 0.2 to 1\n",
    "- max_depth: 1 to 11 in steps of 1\n",
    "- colsample_bytree: 0.5 to 1\n",
    "- min_child_weight: 1 to 4 in steps of 1\n",
    "- reg_alpha: 0.0000001 to 10\n",
    "- reg_lambda: 0.0000001 to 10\n",
    "- scale_pos_weight: 0 to 50 in steps of 0.1\n",
    "\n",
    "We can see XGBoost has many more hyperparameters than AdaBoost. This is done\n",
    "using an sklearn grid search by default, so it can take a long time. To speed it\n",
    "up, we can use another tuner, like Bayesian search, which searches the following\n",
    "hyperparameter spaces by default in pycaret:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19026545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune_distributions = {\n",
    "#                        \"learning_rate\": UniformDistribution(0.000001, 0.5, log=True),\n",
    "#                        \"n_estimators\": IntUniformDistribution(10, 300),\n",
    "#                        \"subsample\": UniformDistribution(0.2, 1),\n",
    "#                        \"max_depth\": IntUniformDistribution(1, 11),\n",
    "#                        \"colsample_bytree\": UniformDistribution(0.5, 1),\n",
    "#                        \"min_child_weight\": IntUniformDistribution(1, 4),\n",
    "#                        \"reg_alpha\": UniformDistribution(0.0000000001, 10, log=True),\n",
    "#                        \"reg_lambda\": UniformDistribution(0.0000000001, 10, log=True),\n",
    "#                        \"scale_pos_weight\": UniformDistribution(1, 50),\n",
    "#}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88383058",
   "metadata": {},
   "source": [
    "We can see it's the same areas as the grid search. These hyperparameters represent\n",
    "the following:\n",
    "\n",
    "- learning_rate: the scaling factor that multiplies incremental trees in the\n",
    "algorithm\n",
    "- n_estmators: the number of trees in the algorithm\n",
    "- subsample: the fraction of data sampled for each tree\n",
    "- max_depth: the depth of each tree (number of splits)\n",
    "- colsample_bytree: the fraction of features sampled for each tree\n",
    "- min_child_weight: determines if a node should split or not based on the\n",
    "purity of samples in the node\n",
    "- reg_alpha: L1 regularization of weights for each leaf (each leaf of each tree\n",
    "has a weight associated with it in the XGBoost implementation)\n",
    "- reg_lambda: L2 regularization of weights for each leaf\n",
    "- scale_pos_weight: This controls the balance of positive and negative values\n",
    "for a binary classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9630cf86",
   "metadata": {},
   "source": [
    "The pycaret search spaces are generally good, although some people use different\n",
    "strategies for tuning XGBoost hyperparameters. Some strategies will fix the number\n",
    "of trees and tune the learning rate, while others fix the learning rate and tune the\n",
    "number of trees. Using xgboost with pycaret is the same as with AdaBoost, but we\n",
    "will use the scikit-optimize Bayesian search here with 10 iterations (so it completes\n",
    "faster than the grid search from sklearn):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fdea22d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3199a_row3_col0, #T_3199a_row3_col1, #T_3199a_row3_col2, #T_3199a_row3_col3, #T_3199a_row3_col4, #T_3199a_row3_col5, #T_3199a_row3_col6 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3199a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3199a_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_3199a_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_3199a_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_3199a_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_3199a_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_3199a_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_3199a_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3199a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_3199a_row0_col0\" class=\"data row0 col0\" >0.9194</td>\n",
       "      <td id=\"T_3199a_row0_col1\" class=\"data row0 col1\" >0.6137</td>\n",
       "      <td id=\"T_3199a_row0_col2\" class=\"data row0 col2\" >0.0000</td>\n",
       "      <td id=\"T_3199a_row0_col3\" class=\"data row0 col3\" >0.0000</td>\n",
       "      <td id=\"T_3199a_row0_col4\" class=\"data row0 col4\" >0.0000</td>\n",
       "      <td id=\"T_3199a_row0_col5\" class=\"data row0 col5\" >0.0000</td>\n",
       "      <td id=\"T_3199a_row0_col6\" class=\"data row0 col6\" >0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3199a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_3199a_row1_col0\" class=\"data row1 col0\" >0.9194</td>\n",
       "      <td id=\"T_3199a_row1_col1\" class=\"data row1 col1\" >0.5977</td>\n",
       "      <td id=\"T_3199a_row1_col2\" class=\"data row1 col2\" >0.0000</td>\n",
       "      <td id=\"T_3199a_row1_col3\" class=\"data row1 col3\" >0.0000</td>\n",
       "      <td id=\"T_3199a_row1_col4\" class=\"data row1 col4\" >0.0000</td>\n",
       "      <td id=\"T_3199a_row1_col5\" class=\"data row1 col5\" >0.0000</td>\n",
       "      <td id=\"T_3199a_row1_col6\" class=\"data row1 col6\" >0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3199a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_3199a_row2_col0\" class=\"data row2 col0\" >0.9194</td>\n",
       "      <td id=\"T_3199a_row2_col1\" class=\"data row2 col1\" >0.6074</td>\n",
       "      <td id=\"T_3199a_row2_col2\" class=\"data row2 col2\" >0.0000</td>\n",
       "      <td id=\"T_3199a_row2_col3\" class=\"data row2 col3\" >0.0000</td>\n",
       "      <td id=\"T_3199a_row2_col4\" class=\"data row2 col4\" >0.0000</td>\n",
       "      <td id=\"T_3199a_row2_col5\" class=\"data row2 col5\" >0.0000</td>\n",
       "      <td id=\"T_3199a_row2_col6\" class=\"data row2 col6\" >0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3199a_level0_row3\" class=\"row_heading level0 row3\" >Mean</th>\n",
       "      <td id=\"T_3199a_row3_col0\" class=\"data row3 col0\" >0.9194</td>\n",
       "      <td id=\"T_3199a_row3_col1\" class=\"data row3 col1\" >0.6063</td>\n",
       "      <td id=\"T_3199a_row3_col2\" class=\"data row3 col2\" >0.0000</td>\n",
       "      <td id=\"T_3199a_row3_col3\" class=\"data row3 col3\" >0.0000</td>\n",
       "      <td id=\"T_3199a_row3_col4\" class=\"data row3 col4\" >0.0000</td>\n",
       "      <td id=\"T_3199a_row3_col5\" class=\"data row3 col5\" >0.0000</td>\n",
       "      <td id=\"T_3199a_row3_col6\" class=\"data row3 col6\" >0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3199a_level0_row4\" class=\"row_heading level0 row4\" >SD</th>\n",
       "      <td id=\"T_3199a_row4_col0\" class=\"data row4 col0\" >0.0000</td>\n",
       "      <td id=\"T_3199a_row4_col1\" class=\"data row4 col1\" >0.0066</td>\n",
       "      <td id=\"T_3199a_row4_col2\" class=\"data row4 col2\" >0.0000</td>\n",
       "      <td id=\"T_3199a_row4_col3\" class=\"data row4 col3\" >0.0000</td>\n",
       "      <td id=\"T_3199a_row4_col4\" class=\"data row4 col4\" >0.0000</td>\n",
       "      <td id=\"T_3199a_row4_col5\" class=\"data row4 col5\" >0.0000</td>\n",
       "      <td id=\"T_3199a_row4_col6\" class=\"data row4 col6\" >0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x286e3396100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb = create_model(\"xgboost\", fold = 3)\n",
    "\n",
    "best_xgb, tuner = tune_model(xgb,\n",
    "                             fold = 3,  \n",
    "                             search_library = \"scikit-optimize\",\n",
    "                             return_tuner = True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ddce7b",
   "metadata": {},
   "source": [
    "> If you see an error similar to ValueError: Estimator xgboost not\n",
    "available, you will need to install xgboost with conda or pip. A pip\n",
    "install will include GPU support (for Window and Linux) as long\n",
    "as you have installed CUDA first. An easy way to install CUDA is\n",
    "with conda install -c anaconda cudatoolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d8e76c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:logistic',\n",
       " 'use_label_encoder': True,\n",
       " 'base_score': 0.5,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bynode': 1,\n",
       " 'colsample_bytree': 0.6489901613145924,\n",
       " 'enable_categorical': False,\n",
       " 'gamma': 0,\n",
       " 'gpu_id': -1,\n",
       " 'importance_type': None,\n",
       " 'interaction_constraints': '',\n",
       " 'learning_rate': 0.0012348552837614137,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 2,\n",
       " 'min_child_weight': 3,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': '()',\n",
       " 'n_estimators': 18,\n",
       " 'n_jobs': -1,\n",
       " 'num_parallel_tree': 1,\n",
       " 'predictor': 'auto',\n",
       " 'random_state': 1980,\n",
       " 'reg_alpha': 6.172650040638662,\n",
       " 'reg_lambda': 5.8265833278527836e-09,\n",
       " 'scale_pos_weight': 2.103821536756728,\n",
       " 'subsample': 0.8142597613411604,\n",
       " 'tree_method': 'auto',\n",
       " 'validate_parameters': 1,\n",
       " 'verbosity': 0}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best results\n",
    "\n",
    "best_xgb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f9c89c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('actual_estimator__colsample_bytree', 0.970136877041888),\n",
       "              ('actual_estimator__learning_rate', 0.00039381361449212674),\n",
       "              ('actual_estimator__max_depth', 3),\n",
       "              ('actual_estimator__min_child_weight', 3),\n",
       "              ('actual_estimator__n_estimators', 143),\n",
       "              ('actual_estimator__reg_alpha', 1.5288412592503967e-08),\n",
       "              ('actual_estimator__reg_lambda', 2.2709590343692762e-05),\n",
       "              ('actual_estimator__scale_pos_weight', 49.91753331173587),\n",
       "              ('actual_estimator__subsample', 0.4803734904548599)]),\n",
       " OrderedDict([('actual_estimator__colsample_bytree', 0.6240378601903266),\n",
       "              ('actual_estimator__learning_rate', 3.386679586776302e-06),\n",
       "              ('actual_estimator__max_depth', 5),\n",
       "              ('actual_estimator__min_child_weight', 3),\n",
       "              ('actual_estimator__n_estimators', 78),\n",
       "              ('actual_estimator__reg_alpha', 4.72391609165053e-08),\n",
       "              ('actual_estimator__reg_lambda', 0.003821095807217221),\n",
       "              ('actual_estimator__scale_pos_weight', 40.38165343480145),\n",
       "              ('actual_estimator__subsample', 0.25235030277526205)]),\n",
       " OrderedDict([('actual_estimator__colsample_bytree', 0.6489901613145924),\n",
       "              ('actual_estimator__learning_rate', 0.0012348552837614137),\n",
       "              ('actual_estimator__max_depth', 2),\n",
       "              ('actual_estimator__min_child_weight', 3),\n",
       "              ('actual_estimator__n_estimators', 18),\n",
       "              ('actual_estimator__reg_alpha', 6.172650040638662),\n",
       "              ('actual_estimator__reg_lambda', 5.8265833278527836e-09),\n",
       "              ('actual_estimator__scale_pos_weight', 2.103821536756728),\n",
       "              ('actual_estimator__subsample', 0.8142597613411604)]),\n",
       " OrderedDict([('actual_estimator__colsample_bytree', 0.711822766455414),\n",
       "              ('actual_estimator__learning_rate', 1.382703756510453e-05),\n",
       "              ('actual_estimator__max_depth', 5),\n",
       "              ('actual_estimator__min_child_weight', 1),\n",
       "              ('actual_estimator__n_estimators', 25),\n",
       "              ('actual_estimator__reg_alpha', 8.313714818654254e-10),\n",
       "              ('actual_estimator__reg_lambda', 0.001052544933118272),\n",
       "              ('actual_estimator__scale_pos_weight', 48.12460838672476),\n",
       "              ('actual_estimator__subsample', 0.770519810566932)]),\n",
       " OrderedDict([('actual_estimator__colsample_bytree', 0.5185545595906709),\n",
       "              ('actual_estimator__learning_rate', 0.0006885034911362117),\n",
       "              ('actual_estimator__max_depth', 5),\n",
       "              ('actual_estimator__min_child_weight', 2),\n",
       "              ('actual_estimator__n_estimators', 261),\n",
       "              ('actual_estimator__reg_alpha', 6.854991590896971e-06),\n",
       "              ('actual_estimator__reg_lambda', 0.007723879372260388),\n",
       "              ('actual_estimator__scale_pos_weight', 2.9354070507289665),\n",
       "              ('actual_estimator__subsample', 0.4445136152825414)]),\n",
       " OrderedDict([('actual_estimator__colsample_bytree', 0.8359053962445442),\n",
       "              ('actual_estimator__learning_rate', 1.9958440181478457e-05),\n",
       "              ('actual_estimator__max_depth', 10),\n",
       "              ('actual_estimator__min_child_weight', 3),\n",
       "              ('actual_estimator__n_estimators', 170),\n",
       "              ('actual_estimator__reg_alpha', 1.0601940686314006e-10),\n",
       "              ('actual_estimator__reg_lambda', 1.6925849078641103e-08),\n",
       "              ('actual_estimator__scale_pos_weight', 6.30485893340235),\n",
       "              ('actual_estimator__subsample', 0.745333400216019)]),\n",
       " OrderedDict([('actual_estimator__colsample_bytree', 0.6946356600239667),\n",
       "              ('actual_estimator__learning_rate', 8.9890229973914e-06),\n",
       "              ('actual_estimator__max_depth', 10),\n",
       "              ('actual_estimator__min_child_weight', 1),\n",
       "              ('actual_estimator__n_estimators', 25),\n",
       "              ('actual_estimator__reg_alpha', 0.13938243096361916),\n",
       "              ('actual_estimator__reg_lambda', 2.1331727161329477),\n",
       "              ('actual_estimator__scale_pos_weight', 10.946761171273767),\n",
       "              ('actual_estimator__subsample', 0.5178353415637531)]),\n",
       " OrderedDict([('actual_estimator__colsample_bytree', 0.6579434631237593),\n",
       "              ('actual_estimator__learning_rate', 0.001231145157963542),\n",
       "              ('actual_estimator__max_depth', 5),\n",
       "              ('actual_estimator__min_child_weight', 3),\n",
       "              ('actual_estimator__n_estimators', 76),\n",
       "              ('actual_estimator__reg_alpha', 0.03460719601699172),\n",
       "              ('actual_estimator__reg_lambda', 0.0217987128658132),\n",
       "              ('actual_estimator__scale_pos_weight', 19.87518025423055),\n",
       "              ('actual_estimator__subsample', 0.912943865106493)]),\n",
       " OrderedDict([('actual_estimator__colsample_bytree', 0.9183401295523448),\n",
       "              ('actual_estimator__learning_rate', 0.0001823779385478625),\n",
       "              ('actual_estimator__max_depth', 2),\n",
       "              ('actual_estimator__min_child_weight', 2),\n",
       "              ('actual_estimator__n_estimators', 188),\n",
       "              ('actual_estimator__reg_alpha', 0.0022639757208819623),\n",
       "              ('actual_estimator__reg_lambda', 5.363767136198569e-09),\n",
       "              ('actual_estimator__scale_pos_weight', 20.888996051411727),\n",
       "              ('actual_estimator__subsample', 0.5757389397450785)]),\n",
       " OrderedDict([('actual_estimator__colsample_bytree', 0.6603983698656328),\n",
       "              ('actual_estimator__learning_rate', 0.00012026732285122964),\n",
       "              ('actual_estimator__max_depth', 7),\n",
       "              ('actual_estimator__min_child_weight', 2),\n",
       "              ('actual_estimator__n_estimators', 15),\n",
       "              ('actual_estimator__reg_alpha', 3.73977360974128e-05),\n",
       "              ('actual_estimator__reg_lambda', 0.02577142862974375),\n",
       "              ('actual_estimator__scale_pos_weight', 31.05745430783135),\n",
       "              ('actual_estimator__subsample', 0.6078890470865207)])]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get CV results \n",
    "\n",
    "tuner.cv_results_['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2ee79a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08439859, 0.09188053, 0.91936331, 0.08692822, 0.91936331,\n",
       "       0.87533189, 0.70458635, 0.15639085, 0.09512165, 0.11420083])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee8ce7",
   "metadata": {},
   "source": [
    "As with random forests and other tree-based methods, we can get the feature\n",
    "importances. With an xgboost model (assuming we are using the sklearn API for\n",
    "xgboost as we are here), this can be retrieved from xgb_model.get_booster().get_\n",
    "score() or best_xgb.feature_importances_, which gives us the feature importance\n",
    "by weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d67ffd6",
   "metadata": {},
   "source": [
    "### XGBoost with the xgboost package\n",
    "\n",
    "We can also use the xgboost package directly to create and train a model. For this,\n",
    "we first import the package with the alias xgb as is the convention in the xgboost\n",
    "documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c0390a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Next, convert data to a DMMatrix xgboost data type\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train, label = y_train)\n",
    "\n",
    "dtest = xgb.DMatrix(x_test, label = y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0490de83",
   "metadata": {},
   "source": [
    "> We give the features as the first argument and the label as the second argument\n",
    "(which we also provide with the keyword label here, although it's not required).\n",
    "Now we can train the model:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2ef45d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.train(params={'objective': 'binary:logistic'}, dtrain=dtrain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8542b2cf",
   "metadata": {},
   "source": [
    "The first argument is the hyperparameters, set with the params keyword. The\n",
    "only thing we must set here is the objective function, which is 'binary:logistic'\n",
    "for binary classification (by default it's 'req:squarederror' for regression with\n",
    "a squared error loss function). We can also set other hyperparameters if we wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1888c276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9180765805877115\n",
      "0.912\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "train_preds = xgb_model.predict(dtrain)\n",
    "\n",
    "test_preds = xgb_model.predict(dtest)\n",
    "\n",
    "\n",
    "print(accuracy_score(y_train, train_preds > 0.5))\n",
    "print(accuracy_score(y_test, test_preds > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3e1804",
   "metadata": {},
   "source": [
    "The predictions from the model are probability predictions, so we need to provide a\n",
    "threshold from which to round predictions up to 1. In this case, we use a value of 0.5,\n",
    "which is the default for other models. Notice that we also need to give a DMatrix as\n",
    "our data for the predict function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcfdc46",
   "metadata": {},
   "source": [
    "### The XGBoost scikit-learn API\n",
    "\n",
    "The xgboost package also has an sklearn API which makes it easy to use xgboost\n",
    "in the same way we do with scikit-learn models. This can make it easier to use with\n",
    "sklearn pipelines and other tools that depend on a model behaving as others do in\n",
    "sklearn. To use this API, we create our model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3f3fd07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "fit_model = xgb_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34031cd1",
   "metadata": {},
   "source": [
    "This allows us to provide pandas DataFrames or other data structures besides\n",
    "DMatrix datatypes to the fit function. Then we can evaluate the model in the same\n",
    "way as other sklearn models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1e8cf049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9973285841495992"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_model.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e12ef6f",
   "metadata": {},
   "source": [
    "This computes the accuracy of the model. Of course, we can do something similar\n",
    "for regression with xgb.XGBRegressor(). The xgboost documentation also lays out\n",
    "more details on the use of the xgboost package, including the sklearn API methods\n",
    "we touched on here. As with sklearn models, we also have predict and predict_\n",
    "proba methods with our xgboost models in the sklearn API style, which can give\n",
    "us predicted values and probabilities, respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a94ad",
   "metadata": {},
   "source": [
    "### LightGBM\n",
    "\n",
    "LightGBM is a newer algorithm that includes some improvements compared with\n",
    "XGBoost, although it does not always outperform XGBoost in practice. It creates the\n",
    "decision trees in the ensemble differently using novel techniques (described in the\n",
    "original paper: https://papers.nips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf), which allows it to run faster and use less memory than\n",
    "XGBoost. It also can handle missing values and categorical data natively. It was\n",
    "created by Microsoft and is what Azure's ML GUI uses when a boosted decision\n",
    "tree ML algorithm is chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b095a8e",
   "metadata": {},
   "source": [
    "> One note on LightGBM is that we should convert object\n",
    "datatypes to category in pandas DataFrames if we wish to\n",
    "use categorical features directly (for example, without one-hot\n",
    "encoding them). We can efficiently do this like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b487049a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_82226_row2_col0, #T_82226_row2_col1, #T_82226_row2_col2, #T_82226_row2_col3, #T_82226_row2_col4, #T_82226_row2_col5, #T_82226_row2_col6 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_82226\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_82226_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_82226_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_82226_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_82226_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_82226_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_82226_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_82226_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_82226_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_82226_row0_col0\" class=\"data row0 col0\" >0.9194</td>\n",
       "      <td id=\"T_82226_row0_col1\" class=\"data row0 col1\" >0.6360</td>\n",
       "      <td id=\"T_82226_row0_col2\" class=\"data row0 col2\" >0.0000</td>\n",
       "      <td id=\"T_82226_row0_col3\" class=\"data row0 col3\" >0.0000</td>\n",
       "      <td id=\"T_82226_row0_col4\" class=\"data row0 col4\" >0.0000</td>\n",
       "      <td id=\"T_82226_row0_col5\" class=\"data row0 col5\" >0.0000</td>\n",
       "      <td id=\"T_82226_row0_col6\" class=\"data row0 col6\" >0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_82226_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_82226_row1_col0\" class=\"data row1 col0\" >0.9194</td>\n",
       "      <td id=\"T_82226_row1_col1\" class=\"data row1 col1\" >0.6348</td>\n",
       "      <td id=\"T_82226_row1_col2\" class=\"data row1 col2\" >0.0000</td>\n",
       "      <td id=\"T_82226_row1_col3\" class=\"data row1 col3\" >0.0000</td>\n",
       "      <td id=\"T_82226_row1_col4\" class=\"data row1 col4\" >0.0000</td>\n",
       "      <td id=\"T_82226_row1_col5\" class=\"data row1 col5\" >0.0000</td>\n",
       "      <td id=\"T_82226_row1_col6\" class=\"data row1 col6\" >0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_82226_level0_row2\" class=\"row_heading level0 row2\" >Mean</th>\n",
       "      <td id=\"T_82226_row2_col0\" class=\"data row2 col0\" >0.9194</td>\n",
       "      <td id=\"T_82226_row2_col1\" class=\"data row2 col1\" >0.6354</td>\n",
       "      <td id=\"T_82226_row2_col2\" class=\"data row2 col2\" >0.0000</td>\n",
       "      <td id=\"T_82226_row2_col3\" class=\"data row2 col3\" >0.0000</td>\n",
       "      <td id=\"T_82226_row2_col4\" class=\"data row2 col4\" >0.0000</td>\n",
       "      <td id=\"T_82226_row2_col5\" class=\"data row2 col5\" >0.0000</td>\n",
       "      <td id=\"T_82226_row2_col6\" class=\"data row2 col6\" >0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_82226_level0_row3\" class=\"row_heading level0 row3\" >SD</th>\n",
       "      <td id=\"T_82226_row3_col0\" class=\"data row3 col0\" >0.0000</td>\n",
       "      <td id=\"T_82226_row3_col1\" class=\"data row3 col1\" >0.0006</td>\n",
       "      <td id=\"T_82226_row3_col2\" class=\"data row3 col2\" >0.0000</td>\n",
       "      <td id=\"T_82226_row3_col3\" class=\"data row3 col3\" >0.0000</td>\n",
       "      <td id=\"T_82226_row3_col4\" class=\"data row3 col4\" >0.0000</td>\n",
       "      <td id=\"T_82226_row3_col5\" class=\"data row3 col5\" >0.0000</td>\n",
       "      <td id=\"T_82226_row3_col6\" class=\"data row3 col6\" >0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x286e3103670>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#for col in final_df.select_dtypes(include = [\"object\"]):\n",
    "#    final_df[col] = final_df[col].astype(\"category\")\n",
    "\n",
    "\n",
    "\n",
    "light_gbm = create_model('lightgbm', fold=3)\n",
    "\n",
    "best_lgbm, tuner = tune_model(light_gbm,\n",
    "                            fold=2,\n",
    "                            search_library='scikit-optimize',\n",
    "                            return_tuner=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "25824fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 0.518814\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[2]\tvalid_0's auc: 0.514441\n",
      "[3]\tvalid_0's auc: 0.537764\n",
      "[4]\tvalid_0's auc: 0.540725\n",
      "[5]\tvalid_0's auc: 0.553298\n",
      "[6]\tvalid_0's auc: 0.547467\n",
      "[7]\tvalid_0's auc: 0.556715\n",
      "[8]\tvalid_0's auc: 0.548014\n",
      "[9]\tvalid_0's auc: 0.55102\n",
      "[10]\tvalid_0's auc: 0.551066\n",
      "[11]\tvalid_0's auc: 0.557261\n",
      "[12]\tvalid_0's auc: 0.555075\n",
      "[13]\tvalid_0's auc: 0.554756\n",
      "[14]\tvalid_0's auc: 0.550109\n",
      "[15]\tvalid_0's auc: 0.555302\n",
      "[16]\tvalid_0's auc: 0.552205\n",
      "[17]\tvalid_0's auc: 0.548469\n",
      "[18]\tvalid_0's auc: 0.556487\n",
      "[19]\tvalid_0's auc: 0.559311\n",
      "[20]\tvalid_0's auc: 0.561316\n",
      "[21]\tvalid_0's auc: 0.565415\n",
      "[22]\tvalid_0's auc: 0.568058\n",
      "[23]\tvalid_0's auc: 0.574435\n",
      "[24]\tvalid_0's auc: 0.575893\n",
      "[25]\tvalid_0's auc: 0.580995\n",
      "[26]\tvalid_0's auc: 0.587919\n",
      "[27]\tvalid_0's auc: 0.590288\n",
      "[28]\tvalid_0's auc: 0.589832\n",
      "[29]\tvalid_0's auc: 0.592748\n",
      "[30]\tvalid_0's auc: 0.590379\n",
      "[31]\tvalid_0's auc: 0.589923\n",
      "[32]\tvalid_0's auc: 0.590015\n",
      "[33]\tvalid_0's auc: 0.58801\n",
      "[34]\tvalid_0's auc: 0.589832\n",
      "[35]\tvalid_0's auc: 0.587008\n",
      "[36]\tvalid_0's auc: 0.58473\n",
      "[37]\tvalid_0's auc: 0.58555\n",
      "[38]\tvalid_0's auc: 0.583637\n",
      "[39]\tvalid_0's auc: 0.583181\n",
      "[40]\tvalid_0's auc: 0.586917\n",
      "[41]\tvalid_0's auc: 0.582726\n",
      "[42]\tvalid_0's auc: 0.587372\n",
      "[43]\tvalid_0's auc: 0.587828\n",
      "[44]\tvalid_0's auc: 0.588284\n",
      "[45]\tvalid_0's auc: 0.588557\n",
      "[46]\tvalid_0's auc: 0.584275\n",
      "[47]\tvalid_0's auc: 0.582908\n",
      "[48]\tvalid_0's auc: 0.584457\n",
      "[49]\tvalid_0's auc: 0.582179\n",
      "[50]\tvalid_0's auc: 0.582453\n",
      "[51]\tvalid_0's auc: 0.579537\n",
      "[52]\tvalid_0's auc: 0.579264\n",
      "[53]\tvalid_0's auc: 0.57316\n",
      "[54]\tvalid_0's auc: 0.570973\n",
      "[55]\tvalid_0's auc: 0.56988\n",
      "[56]\tvalid_0's auc: 0.567966\n",
      "[57]\tvalid_0's auc: 0.566873\n",
      "[58]\tvalid_0's auc: 0.567602\n",
      "[59]\tvalid_0's auc: 0.568422\n",
      "[60]\tvalid_0's auc: 0.566873\n",
      "[61]\tvalid_0's auc: 0.567147\n",
      "[62]\tvalid_0's auc: 0.567147\n",
      "[63]\tvalid_0's auc: 0.569151\n",
      "[64]\tvalid_0's auc: 0.567147\n",
      "[65]\tvalid_0's auc: 0.567693\n",
      "[66]\tvalid_0's auc: 0.56414\n",
      "[67]\tvalid_0's auc: 0.56086\n",
      "[68]\tvalid_0's auc: 0.559585\n",
      "[69]\tvalid_0's auc: 0.558309\n",
      "[70]\tvalid_0's auc: 0.557853\n",
      "[71]\tvalid_0's auc: 0.561042\n",
      "[72]\tvalid_0's auc: 0.554483\n",
      "[73]\tvalid_0's auc: 0.553663\n",
      "[74]\tvalid_0's auc: 0.546829\n",
      "[75]\tvalid_0's auc: 0.540907\n",
      "[76]\tvalid_0's auc: 0.53781\n",
      "[77]\tvalid_0's auc: 0.536443\n",
      "[78]\tvalid_0's auc: 0.538721\n",
      "[79]\tvalid_0's auc: 0.539359\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's auc: 0.592748\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "train_data = lgb.Dataset(x_train, label = y_train)\n",
    "test_data = lgb.Dataset(x_test, label = y_test)\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"is_unbalance\": \"true\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"num_leaves\": 63,\n",
    "    \"feature_fraction\": 0.5,\n",
    "    \"bagging_fraction\": 0.5,\n",
    "    \"bagging_freq\": 20,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"verbose\": -1\n",
    "}\n",
    "\n",
    "\n",
    "model_lgbm = lightgbm.train(parameters,\n",
    "                            train_data,\n",
    "                            valid_sets = test_data,\n",
    "                            num_boost_round=5000,\n",
    "                            early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e928544f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Train: 0.8185\n",
      "AUC Train: 0.5927\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_train_pred = model_lgbm.predict(x_train)\n",
    "y_test_pred = model_lgbm.predict(x_test)\n",
    "\n",
    "\n",
    "print(f\"AUC Train: {roc_auc_score(y_train, y_train_pred):.4f}\")\n",
    "print(f\"AUC Train: {roc_auc_score(y_test, y_test_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321d6c2a",
   "metadata": {},
   "source": [
    "## CatBoost\n",
    "\n",
    "One last boosting algorithm we'll cover is CatBoost, which is the newest of the\n",
    "boosting algorithms discussed here. CatBoost is similar to XGBoost, using boosted\n",
    "decision trees, but has some advantages:\n",
    "\n",
    "- Ostensibly less hyperparameter tuning than XGBoost (simpler to tune)\n",
    "- Can handle missing data and categorical values natively\n",
    "- Trains quickly, similar to LightGBM\n",
    "\n",
    "\n",
    "There is a catboost package in Python where we can use the package directly, but\n",
    "we will first show usage through pycaret. If using the base catboost package, we\n",
    "can also plot some model metrics and show them as the model trains (such as the\n",
    "accuracy as it fits more trees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a1cae407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7f673_row2_col0, #T_7f673_row2_col1, #T_7f673_row2_col2, #T_7f673_row2_col3, #T_7f673_row2_col4, #T_7f673_row2_col5, #T_7f673_row2_col6 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7f673\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7f673_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_7f673_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_7f673_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_7f673_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_7f673_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_7f673_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_7f673_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7f673_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7f673_row0_col0\" class=\"data row0 col0\" >0.9194</td>\n",
       "      <td id=\"T_7f673_row0_col1\" class=\"data row0 col1\" >0.6379</td>\n",
       "      <td id=\"T_7f673_row0_col2\" class=\"data row0 col2\" >0.0000</td>\n",
       "      <td id=\"T_7f673_row0_col3\" class=\"data row0 col3\" >0.0000</td>\n",
       "      <td id=\"T_7f673_row0_col4\" class=\"data row0 col4\" >0.0000</td>\n",
       "      <td id=\"T_7f673_row0_col5\" class=\"data row0 col5\" >0.0000</td>\n",
       "      <td id=\"T_7f673_row0_col6\" class=\"data row0 col6\" >0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f673_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7f673_row1_col0\" class=\"data row1 col0\" >0.9194</td>\n",
       "      <td id=\"T_7f673_row1_col1\" class=\"data row1 col1\" >0.6358</td>\n",
       "      <td id=\"T_7f673_row1_col2\" class=\"data row1 col2\" >0.0000</td>\n",
       "      <td id=\"T_7f673_row1_col3\" class=\"data row1 col3\" >0.0000</td>\n",
       "      <td id=\"T_7f673_row1_col4\" class=\"data row1 col4\" >0.0000</td>\n",
       "      <td id=\"T_7f673_row1_col5\" class=\"data row1 col5\" >0.0000</td>\n",
       "      <td id=\"T_7f673_row1_col6\" class=\"data row1 col6\" >0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f673_level0_row2\" class=\"row_heading level0 row2\" >Mean</th>\n",
       "      <td id=\"T_7f673_row2_col0\" class=\"data row2 col0\" >0.9194</td>\n",
       "      <td id=\"T_7f673_row2_col1\" class=\"data row2 col1\" >0.6369</td>\n",
       "      <td id=\"T_7f673_row2_col2\" class=\"data row2 col2\" >0.0000</td>\n",
       "      <td id=\"T_7f673_row2_col3\" class=\"data row2 col3\" >0.0000</td>\n",
       "      <td id=\"T_7f673_row2_col4\" class=\"data row2 col4\" >0.0000</td>\n",
       "      <td id=\"T_7f673_row2_col5\" class=\"data row2 col5\" >0.0000</td>\n",
       "      <td id=\"T_7f673_row2_col6\" class=\"data row2 col6\" >0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f673_level0_row3\" class=\"row_heading level0 row3\" >SD</th>\n",
       "      <td id=\"T_7f673_row3_col0\" class=\"data row3 col0\" >0.0000</td>\n",
       "      <td id=\"T_7f673_row3_col1\" class=\"data row3 col1\" >0.0011</td>\n",
       "      <td id=\"T_7f673_row3_col2\" class=\"data row3 col2\" >0.0000</td>\n",
       "      <td id=\"T_7f673_row3_col3\" class=\"data row3 col3\" >0.0000</td>\n",
       "      <td id=\"T_7f673_row3_col4\" class=\"data row3 col4\" >0.0000</td>\n",
       "      <td id=\"T_7f673_row3_col5\" class=\"data row3 col5\" >0.0000</td>\n",
       "      <td id=\"T_7f673_row3_col6\" class=\"data row3 col6\" >0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x286e1f81d60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "catboost_model = create_model('catboost', fold=3)\n",
    "\n",
    "best_cb, tuner = tune_model(catboost_model,\n",
    "                            fold=2,\n",
    "                            search_library='scikit-optimize',\n",
    "                            return_tuner=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146859a0",
   "metadata": {},
   "source": [
    "The results from the tuning with an accuracy of 0.9194 versus no information rate of 0.9193 – this is not really a significant result, but it is interesting that CatBoost shows a hint of performing better than XGBoost and LightGBM here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849470db",
   "metadata": {},
   "source": [
    "### Using CatBoost Natively\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "f7ea9325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d330f5caea4adc8007c33c011209fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.010825\n",
      "0:\tlearn: 0.6821363\ttotal: 3.94ms\tremaining: 3.94s\n",
      "1:\tlearn: 0.6712810\ttotal: 6.31ms\tremaining: 3.15s\n",
      "2:\tlearn: 0.6607985\ttotal: 9.74ms\tremaining: 3.24s\n",
      "3:\tlearn: 0.6503155\ttotal: 12.2ms\tremaining: 3.03s\n",
      "4:\tlearn: 0.6400864\ttotal: 15.3ms\tremaining: 3.04s\n",
      "5:\tlearn: 0.6298525\ttotal: 18.4ms\tremaining: 3.05s\n",
      "6:\tlearn: 0.6204033\ttotal: 21.5ms\tremaining: 3.05s\n",
      "7:\tlearn: 0.6114720\ttotal: 24.4ms\tremaining: 3.02s\n",
      "8:\tlearn: 0.6023788\ttotal: 26.8ms\tremaining: 2.95s\n",
      "9:\tlearn: 0.5938344\ttotal: 28.9ms\tremaining: 2.87s\n",
      "10:\tlearn: 0.5856042\ttotal: 30ms\tremaining: 2.7s\n",
      "11:\tlearn: 0.5775859\ttotal: 33.7ms\tremaining: 2.77s\n",
      "12:\tlearn: 0.5696630\ttotal: 35.9ms\tremaining: 2.73s\n",
      "13:\tlearn: 0.5620026\ttotal: 43.6ms\tremaining: 3.07s\n",
      "14:\tlearn: 0.5543107\ttotal: 50ms\tremaining: 3.28s\n",
      "15:\tlearn: 0.5471496\ttotal: 53.4ms\tremaining: 3.28s\n",
      "16:\tlearn: 0.5396157\ttotal: 56.6ms\tremaining: 3.27s\n",
      "17:\tlearn: 0.5324515\ttotal: 63.9ms\tremaining: 3.48s\n",
      "18:\tlearn: 0.5254633\ttotal: 70.5ms\tremaining: 3.64s\n",
      "19:\tlearn: 0.5192708\ttotal: 72.9ms\tremaining: 3.57s\n",
      "20:\tlearn: 0.5126581\ttotal: 77.1ms\tremaining: 3.6s\n",
      "21:\tlearn: 0.5065915\ttotal: 81.8ms\tremaining: 3.64s\n",
      "22:\tlearn: 0.5001871\ttotal: 89.5ms\tremaining: 3.8s\n",
      "23:\tlearn: 0.4939215\ttotal: 94.5ms\tremaining: 3.84s\n",
      "24:\tlearn: 0.4881487\ttotal: 98.8ms\tremaining: 3.85s\n",
      "25:\tlearn: 0.4827970\ttotal: 106ms\tremaining: 3.97s\n",
      "26:\tlearn: 0.4771955\ttotal: 117ms\tremaining: 4.22s\n",
      "27:\tlearn: 0.4714808\ttotal: 125ms\tremaining: 4.34s\n",
      "28:\tlearn: 0.4664238\ttotal: 133ms\tremaining: 4.44s\n",
      "29:\tlearn: 0.4613198\ttotal: 141ms\tremaining: 4.56s\n",
      "30:\tlearn: 0.4565015\ttotal: 146ms\tremaining: 4.58s\n",
      "31:\tlearn: 0.4514957\ttotal: 154ms\tremaining: 4.64s\n",
      "32:\tlearn: 0.4469432\ttotal: 160ms\tremaining: 4.68s\n",
      "33:\tlearn: 0.4421154\ttotal: 163ms\tremaining: 4.64s\n",
      "34:\tlearn: 0.4377864\ttotal: 170ms\tremaining: 4.68s\n",
      "35:\tlearn: 0.4335306\ttotal: 174ms\tremaining: 4.66s\n",
      "36:\tlearn: 0.4292699\ttotal: 187ms\tremaining: 4.86s\n",
      "37:\tlearn: 0.4249402\ttotal: 192ms\tremaining: 4.85s\n",
      "38:\tlearn: 0.4211533\ttotal: 198ms\tremaining: 4.88s\n",
      "39:\tlearn: 0.4170340\ttotal: 206ms\tremaining: 4.95s\n",
      "40:\tlearn: 0.4134922\ttotal: 211ms\tremaining: 4.94s\n",
      "41:\tlearn: 0.4096165\ttotal: 215ms\tremaining: 4.89s\n",
      "42:\tlearn: 0.4063851\ttotal: 217ms\tremaining: 4.82s\n",
      "43:\tlearn: 0.4030271\ttotal: 221ms\tremaining: 4.81s\n",
      "44:\tlearn: 0.3997941\ttotal: 227ms\tremaining: 4.83s\n",
      "45:\tlearn: 0.3963011\ttotal: 234ms\tremaining: 4.86s\n",
      "46:\tlearn: 0.3931161\ttotal: 241ms\tremaining: 4.89s\n",
      "47:\tlearn: 0.3899762\ttotal: 246ms\tremaining: 4.88s\n",
      "48:\tlearn: 0.3870546\ttotal: 259ms\tremaining: 5.03s\n",
      "49:\tlearn: 0.3840015\ttotal: 266ms\tremaining: 5.06s\n",
      "50:\tlearn: 0.3809608\ttotal: 275ms\tremaining: 5.11s\n",
      "51:\tlearn: 0.3781098\ttotal: 281ms\tremaining: 5.12s\n",
      "52:\tlearn: 0.3749536\ttotal: 289ms\tremaining: 5.17s\n",
      "53:\tlearn: 0.3724140\ttotal: 298ms\tremaining: 5.22s\n",
      "54:\tlearn: 0.3696211\ttotal: 306ms\tremaining: 5.26s\n",
      "55:\tlearn: 0.3670567\ttotal: 312ms\tremaining: 5.26s\n",
      "56:\tlearn: 0.3646220\ttotal: 315ms\tremaining: 5.21s\n",
      "57:\tlearn: 0.3620813\ttotal: 318ms\tremaining: 5.16s\n",
      "58:\tlearn: 0.3599391\ttotal: 319ms\tremaining: 5.09s\n",
      "59:\tlearn: 0.3573321\ttotal: 323ms\tremaining: 5.05s\n",
      "60:\tlearn: 0.3551796\ttotal: 326ms\tremaining: 5.01s\n",
      "61:\tlearn: 0.3527943\ttotal: 329ms\tremaining: 4.98s\n",
      "62:\tlearn: 0.3509351\ttotal: 331ms\tremaining: 4.92s\n",
      "63:\tlearn: 0.3490438\ttotal: 333ms\tremaining: 4.87s\n",
      "64:\tlearn: 0.3470469\ttotal: 336ms\tremaining: 4.83s\n",
      "65:\tlearn: 0.3451757\ttotal: 338ms\tremaining: 4.79s\n",
      "66:\tlearn: 0.3431507\ttotal: 341ms\tremaining: 4.75s\n",
      "67:\tlearn: 0.3412683\ttotal: 344ms\tremaining: 4.72s\n",
      "68:\tlearn: 0.3394452\ttotal: 355ms\tremaining: 4.79s\n",
      "69:\tlearn: 0.3375173\ttotal: 364ms\tremaining: 4.83s\n",
      "70:\tlearn: 0.3356419\ttotal: 368ms\tremaining: 4.82s\n",
      "71:\tlearn: 0.3341304\ttotal: 370ms\tremaining: 4.77s\n",
      "72:\tlearn: 0.3324183\ttotal: 374ms\tremaining: 4.75s\n",
      "73:\tlearn: 0.3308545\ttotal: 379ms\tremaining: 4.75s\n",
      "74:\tlearn: 0.3293887\ttotal: 385ms\tremaining: 4.75s\n",
      "75:\tlearn: 0.3278846\ttotal: 389ms\tremaining: 4.73s\n",
      "76:\tlearn: 0.3262355\ttotal: 393ms\tremaining: 4.71s\n",
      "77:\tlearn: 0.3249074\ttotal: 401ms\tremaining: 4.74s\n",
      "78:\tlearn: 0.3235045\ttotal: 405ms\tremaining: 4.72s\n",
      "79:\tlearn: 0.3221915\ttotal: 408ms\tremaining: 4.7s\n",
      "80:\tlearn: 0.3208107\ttotal: 412ms\tremaining: 4.68s\n",
      "81:\tlearn: 0.3194884\ttotal: 416ms\tremaining: 4.66s\n",
      "82:\tlearn: 0.3177583\ttotal: 423ms\tremaining: 4.67s\n",
      "83:\tlearn: 0.3163917\ttotal: 429ms\tremaining: 4.67s\n",
      "84:\tlearn: 0.3152207\ttotal: 442ms\tremaining: 4.76s\n",
      "85:\tlearn: 0.3140989\ttotal: 447ms\tremaining: 4.75s\n",
      "86:\tlearn: 0.3129738\ttotal: 450ms\tremaining: 4.72s\n",
      "87:\tlearn: 0.3114831\ttotal: 454ms\tremaining: 4.7s\n",
      "88:\tlearn: 0.3104906\ttotal: 458ms\tremaining: 4.68s\n",
      "89:\tlearn: 0.3092246\ttotal: 472ms\tremaining: 4.77s\n",
      "90:\tlearn: 0.3080853\ttotal: 480ms\tremaining: 4.8s\n",
      "91:\tlearn: 0.3068886\ttotal: 485ms\tremaining: 4.79s\n",
      "92:\tlearn: 0.3059333\ttotal: 491ms\tremaining: 4.79s\n",
      "93:\tlearn: 0.3047612\ttotal: 501ms\tremaining: 4.83s\n",
      "94:\tlearn: 0.3037593\ttotal: 507ms\tremaining: 4.83s\n",
      "95:\tlearn: 0.3028750\ttotal: 512ms\tremaining: 4.82s\n",
      "96:\tlearn: 0.3019034\ttotal: 517ms\tremaining: 4.82s\n",
      "97:\tlearn: 0.3009395\ttotal: 523ms\tremaining: 4.81s\n",
      "98:\tlearn: 0.2999207\ttotal: 526ms\tremaining: 4.79s\n",
      "99:\tlearn: 0.2988979\ttotal: 529ms\tremaining: 4.76s\n",
      "100:\tlearn: 0.2981038\ttotal: 532ms\tremaining: 4.73s\n",
      "101:\tlearn: 0.2973534\ttotal: 540ms\tremaining: 4.75s\n",
      "102:\tlearn: 0.2964877\ttotal: 548ms\tremaining: 4.77s\n",
      "103:\tlearn: 0.2956288\ttotal: 558ms\tremaining: 4.81s\n",
      "104:\tlearn: 0.2947905\ttotal: 564ms\tremaining: 4.81s\n",
      "105:\tlearn: 0.2940164\ttotal: 572ms\tremaining: 4.82s\n",
      "106:\tlearn: 0.2933032\ttotal: 574ms\tremaining: 4.79s\n",
      "107:\tlearn: 0.2924927\ttotal: 581ms\tremaining: 4.8s\n",
      "108:\tlearn: 0.2916991\ttotal: 587ms\tremaining: 4.8s\n",
      "109:\tlearn: 0.2908964\ttotal: 601ms\tremaining: 4.86s\n",
      "110:\tlearn: 0.2900942\ttotal: 614ms\tremaining: 4.92s\n",
      "111:\tlearn: 0.2893247\ttotal: 617ms\tremaining: 4.89s\n",
      "112:\tlearn: 0.2888121\ttotal: 619ms\tremaining: 4.86s\n",
      "113:\tlearn: 0.2880133\ttotal: 628ms\tremaining: 4.88s\n",
      "114:\tlearn: 0.2871998\ttotal: 645ms\tremaining: 4.96s\n",
      "115:\tlearn: 0.2865320\ttotal: 652ms\tremaining: 4.97s\n",
      "116:\tlearn: 0.2856290\ttotal: 656ms\tremaining: 4.95s\n",
      "117:\tlearn: 0.2851376\ttotal: 659ms\tremaining: 4.92s\n",
      "118:\tlearn: 0.2843836\ttotal: 664ms\tremaining: 4.92s\n",
      "119:\tlearn: 0.2836474\ttotal: 668ms\tremaining: 4.9s\n",
      "120:\tlearn: 0.2829232\ttotal: 674ms\tremaining: 4.89s\n",
      "121:\tlearn: 0.2823083\ttotal: 681ms\tremaining: 4.9s\n",
      "122:\tlearn: 0.2818681\ttotal: 687ms\tremaining: 4.9s\n",
      "123:\tlearn: 0.2813927\ttotal: 696ms\tremaining: 4.91s\n",
      "124:\tlearn: 0.2808732\ttotal: 704ms\tremaining: 4.92s\n",
      "125:\tlearn: 0.2803426\ttotal: 710ms\tremaining: 4.92s\n",
      "126:\tlearn: 0.2797309\ttotal: 717ms\tremaining: 4.93s\n",
      "127:\tlearn: 0.2791321\ttotal: 736ms\tremaining: 5.01s\n",
      "128:\tlearn: 0.2785409\ttotal: 739ms\tremaining: 4.99s\n",
      "129:\tlearn: 0.2780990\ttotal: 742ms\tremaining: 4.97s\n",
      "130:\tlearn: 0.2775456\ttotal: 753ms\tremaining: 4.99s\n",
      "131:\tlearn: 0.2769942\ttotal: 764ms\tremaining: 5.03s\n",
      "132:\tlearn: 0.2764286\ttotal: 769ms\tremaining: 5.01s\n",
      "133:\tlearn: 0.2758184\ttotal: 776ms\tremaining: 5.01s\n",
      "134:\tlearn: 0.2754285\ttotal: 781ms\tremaining: 5s\n",
      "135:\tlearn: 0.2749865\ttotal: 784ms\tremaining: 4.98s\n",
      "136:\tlearn: 0.2745391\ttotal: 790ms\tremaining: 4.98s\n",
      "137:\tlearn: 0.2740371\ttotal: 796ms\tremaining: 4.97s\n",
      "138:\tlearn: 0.2736996\ttotal: 801ms\tremaining: 4.96s\n",
      "139:\tlearn: 0.2732443\ttotal: 804ms\tremaining: 4.94s\n",
      "140:\tlearn: 0.2728348\ttotal: 807ms\tremaining: 4.92s\n",
      "141:\tlearn: 0.2725539\ttotal: 813ms\tremaining: 4.91s\n",
      "142:\tlearn: 0.2720869\ttotal: 816ms\tremaining: 4.89s\n",
      "143:\tlearn: 0.2717909\ttotal: 822ms\tremaining: 4.88s\n",
      "144:\tlearn: 0.2715821\ttotal: 824ms\tremaining: 4.86s\n",
      "145:\tlearn: 0.2712015\ttotal: 829ms\tremaining: 4.85s\n",
      "146:\tlearn: 0.2706988\ttotal: 837ms\tremaining: 4.85s\n",
      "147:\tlearn: 0.2703519\ttotal: 839ms\tremaining: 4.83s\n",
      "148:\tlearn: 0.2699325\ttotal: 843ms\tremaining: 4.81s\n",
      "149:\tlearn: 0.2695801\ttotal: 848ms\tremaining: 4.8s\n",
      "150:\tlearn: 0.2692672\ttotal: 851ms\tremaining: 4.79s\n",
      "151:\tlearn: 0.2687840\ttotal: 854ms\tremaining: 4.76s\n",
      "152:\tlearn: 0.2684761\ttotal: 857ms\tremaining: 4.75s\n",
      "153:\tlearn: 0.2680391\ttotal: 860ms\tremaining: 4.73s\n",
      "154:\tlearn: 0.2677037\ttotal: 864ms\tremaining: 4.71s\n",
      "155:\tlearn: 0.2671289\ttotal: 870ms\tremaining: 4.71s\n",
      "156:\tlearn: 0.2666833\ttotal: 873ms\tremaining: 4.68s\n",
      "157:\tlearn: 0.2663737\ttotal: 876ms\tremaining: 4.67s\n",
      "158:\tlearn: 0.2660083\ttotal: 883ms\tremaining: 4.67s\n",
      "159:\tlearn: 0.2656947\ttotal: 893ms\tremaining: 4.69s\n",
      "160:\tlearn: 0.2653555\ttotal: 896ms\tremaining: 4.67s\n",
      "161:\tlearn: 0.2648517\ttotal: 899ms\tremaining: 4.65s\n",
      "162:\tlearn: 0.2642987\ttotal: 902ms\tremaining: 4.63s\n",
      "163:\tlearn: 0.2640264\ttotal: 908ms\tremaining: 4.63s\n",
      "164:\tlearn: 0.2637237\ttotal: 916ms\tremaining: 4.63s\n",
      "165:\tlearn: 0.2634765\ttotal: 919ms\tremaining: 4.62s\n",
      "166:\tlearn: 0.2632040\ttotal: 922ms\tremaining: 4.6s\n",
      "167:\tlearn: 0.2627570\ttotal: 926ms\tremaining: 4.59s\n",
      "168:\tlearn: 0.2623552\ttotal: 933ms\tremaining: 4.59s\n",
      "169:\tlearn: 0.2620275\ttotal: 937ms\tremaining: 4.58s\n",
      "170:\tlearn: 0.2617648\ttotal: 941ms\tremaining: 4.56s\n",
      "171:\tlearn: 0.2614125\ttotal: 944ms\tremaining: 4.54s\n",
      "172:\tlearn: 0.2611495\ttotal: 948ms\tremaining: 4.53s\n",
      "173:\tlearn: 0.2608979\ttotal: 952ms\tremaining: 4.52s\n",
      "174:\tlearn: 0.2605424\ttotal: 956ms\tremaining: 4.51s\n",
      "175:\tlearn: 0.2603775\ttotal: 959ms\tremaining: 4.49s\n",
      "176:\tlearn: 0.2601099\ttotal: 962ms\tremaining: 4.47s\n",
      "177:\tlearn: 0.2598481\ttotal: 969ms\tremaining: 4.47s\n",
      "178:\tlearn: 0.2596123\ttotal: 972ms\tremaining: 4.46s\n",
      "179:\tlearn: 0.2593126\ttotal: 976ms\tremaining: 4.45s\n",
      "180:\tlearn: 0.2592246\ttotal: 977ms\tremaining: 4.42s\n",
      "181:\tlearn: 0.2588867\ttotal: 981ms\tremaining: 4.41s\n",
      "182:\tlearn: 0.2584716\ttotal: 984ms\tremaining: 4.39s\n",
      "183:\tlearn: 0.2581624\ttotal: 988ms\tremaining: 4.38s\n",
      "184:\tlearn: 0.2578804\ttotal: 991ms\tremaining: 4.37s\n",
      "185:\tlearn: 0.2575676\ttotal: 995ms\tremaining: 4.35s\n",
      "186:\tlearn: 0.2574195\ttotal: 998ms\tremaining: 4.34s\n",
      "187:\tlearn: 0.2569707\ttotal: 1.03s\tremaining: 4.47s\n",
      "188:\tlearn: 0.2567013\ttotal: 1.04s\tremaining: 4.46s\n",
      "189:\tlearn: 0.2564708\ttotal: 1.05s\tremaining: 4.5s\n",
      "190:\tlearn: 0.2563164\ttotal: 1.06s\tremaining: 4.5s\n",
      "191:\tlearn: 0.2562188\ttotal: 1.06s\tremaining: 4.48s\n",
      "192:\tlearn: 0.2560094\ttotal: 1.07s\tremaining: 4.48s\n",
      "193:\tlearn: 0.2556536\ttotal: 1.08s\tremaining: 4.48s\n",
      "194:\tlearn: 0.2554738\ttotal: 1.08s\tremaining: 4.47s\n",
      "195:\tlearn: 0.2550666\ttotal: 1.09s\tremaining: 4.48s\n",
      "196:\tlearn: 0.2549262\ttotal: 1.09s\tremaining: 4.46s\n",
      "197:\tlearn: 0.2546613\ttotal: 1.1s\tremaining: 4.45s\n",
      "198:\tlearn: 0.2544338\ttotal: 1.1s\tremaining: 4.44s\n",
      "199:\tlearn: 0.2541587\ttotal: 1.1s\tremaining: 4.42s\n",
      "200:\tlearn: 0.2539604\ttotal: 1.11s\tremaining: 4.41s\n",
      "201:\tlearn: 0.2537768\ttotal: 1.11s\tremaining: 4.4s\n",
      "202:\tlearn: 0.2536219\ttotal: 1.12s\tremaining: 4.38s\n",
      "203:\tlearn: 0.2534572\ttotal: 1.12s\tremaining: 4.37s\n",
      "204:\tlearn: 0.2531717\ttotal: 1.12s\tremaining: 4.36s\n",
      "205:\tlearn: 0.2530317\ttotal: 1.13s\tremaining: 4.34s\n",
      "206:\tlearn: 0.2527463\ttotal: 1.13s\tremaining: 4.33s\n",
      "207:\tlearn: 0.2525302\ttotal: 1.13s\tremaining: 4.32s\n",
      "208:\tlearn: 0.2522081\ttotal: 1.14s\tremaining: 4.31s\n",
      "209:\tlearn: 0.2518998\ttotal: 1.15s\tremaining: 4.31s\n",
      "210:\tlearn: 0.2517322\ttotal: 1.15s\tremaining: 4.3s\n",
      "211:\tlearn: 0.2514535\ttotal: 1.15s\tremaining: 4.29s\n",
      "212:\tlearn: 0.2512160\ttotal: 1.17s\tremaining: 4.32s\n",
      "213:\tlearn: 0.2509939\ttotal: 1.17s\tremaining: 4.31s\n",
      "214:\tlearn: 0.2508686\ttotal: 1.18s\tremaining: 4.31s\n",
      "215:\tlearn: 0.2507796\ttotal: 1.18s\tremaining: 4.3s\n",
      "216:\tlearn: 0.2505671\ttotal: 1.19s\tremaining: 4.28s\n",
      "217:\tlearn: 0.2501791\ttotal: 1.19s\tremaining: 4.27s\n",
      "218:\tlearn: 0.2499371\ttotal: 1.2s\tremaining: 4.27s\n",
      "219:\tlearn: 0.2497758\ttotal: 1.2s\tremaining: 4.26s\n",
      "220:\tlearn: 0.2496438\ttotal: 1.2s\tremaining: 4.25s\n",
      "221:\tlearn: 0.2494191\ttotal: 1.21s\tremaining: 4.23s\n",
      "222:\tlearn: 0.2491620\ttotal: 1.21s\tremaining: 4.22s\n",
      "223:\tlearn: 0.2488041\ttotal: 1.22s\tremaining: 4.21s\n",
      "224:\tlearn: 0.2486367\ttotal: 1.22s\tremaining: 4.21s\n",
      "225:\tlearn: 0.2483482\ttotal: 1.23s\tremaining: 4.21s\n",
      "226:\tlearn: 0.2481190\ttotal: 1.23s\tremaining: 4.19s\n",
      "227:\tlearn: 0.2479886\ttotal: 1.23s\tremaining: 4.17s\n",
      "228:\tlearn: 0.2477162\ttotal: 1.24s\tremaining: 4.16s\n",
      "229:\tlearn: 0.2476425\ttotal: 1.24s\tremaining: 4.15s\n",
      "230:\tlearn: 0.2474581\ttotal: 1.25s\tremaining: 4.16s\n",
      "231:\tlearn: 0.2472747\ttotal: 1.25s\tremaining: 4.14s\n",
      "232:\tlearn: 0.2471124\ttotal: 1.25s\tremaining: 4.13s\n",
      "233:\tlearn: 0.2468823\ttotal: 1.26s\tremaining: 4.13s\n",
      "234:\tlearn: 0.2466067\ttotal: 1.26s\tremaining: 4.12s\n",
      "235:\tlearn: 0.2465135\ttotal: 1.27s\tremaining: 4.11s\n",
      "236:\tlearn: 0.2462987\ttotal: 1.28s\tremaining: 4.13s\n",
      "237:\tlearn: 0.2460992\ttotal: 1.29s\tremaining: 4.12s\n",
      "238:\tlearn: 0.2456551\ttotal: 1.29s\tremaining: 4.12s\n",
      "239:\tlearn: 0.2454759\ttotal: 1.3s\tremaining: 4.13s\n",
      "240:\tlearn: 0.2452093\ttotal: 1.31s\tremaining: 4.14s\n",
      "241:\tlearn: 0.2450769\ttotal: 1.32s\tremaining: 4.12s\n",
      "242:\tlearn: 0.2448743\ttotal: 1.32s\tremaining: 4.11s\n",
      "243:\tlearn: 0.2444488\ttotal: 1.32s\tremaining: 4.1s\n",
      "244:\tlearn: 0.2441695\ttotal: 1.33s\tremaining: 4.09s\n",
      "245:\tlearn: 0.2438765\ttotal: 1.33s\tremaining: 4.08s\n",
      "246:\tlearn: 0.2437913\ttotal: 1.33s\tremaining: 4.07s\n",
      "247:\tlearn: 0.2436388\ttotal: 1.34s\tremaining: 4.07s\n",
      "248:\tlearn: 0.2435104\ttotal: 1.35s\tremaining: 4.06s\n",
      "249:\tlearn: 0.2433297\ttotal: 1.35s\tremaining: 4.05s\n",
      "250:\tlearn: 0.2431485\ttotal: 1.35s\tremaining: 4.03s\n",
      "251:\tlearn: 0.2430489\ttotal: 1.35s\tremaining: 4.02s\n",
      "252:\tlearn: 0.2430277\ttotal: 1.35s\tremaining: 4s\n",
      "253:\tlearn: 0.2428154\ttotal: 1.36s\tremaining: 3.99s\n",
      "254:\tlearn: 0.2424228\ttotal: 1.36s\tremaining: 3.98s\n",
      "255:\tlearn: 0.2422278\ttotal: 1.37s\tremaining: 3.98s\n",
      "256:\tlearn: 0.2421343\ttotal: 1.37s\tremaining: 3.97s\n",
      "257:\tlearn: 0.2419071\ttotal: 1.38s\tremaining: 3.96s\n",
      "258:\tlearn: 0.2417912\ttotal: 1.38s\tremaining: 3.95s\n",
      "259:\tlearn: 0.2417028\ttotal: 1.4s\tremaining: 3.99s\n",
      "260:\tlearn: 0.2413531\ttotal: 1.41s\tremaining: 3.98s\n",
      "261:\tlearn: 0.2412435\ttotal: 1.41s\tremaining: 3.97s\n",
      "262:\tlearn: 0.2410340\ttotal: 1.42s\tremaining: 3.97s\n",
      "263:\tlearn: 0.2408231\ttotal: 1.42s\tremaining: 3.96s\n",
      "264:\tlearn: 0.2407193\ttotal: 1.43s\tremaining: 3.96s\n",
      "265:\tlearn: 0.2404171\ttotal: 1.45s\tremaining: 3.99s\n",
      "266:\tlearn: 0.2402124\ttotal: 1.45s\tremaining: 3.98s\n",
      "267:\tlearn: 0.2400886\ttotal: 1.45s\tremaining: 3.97s\n",
      "268:\tlearn: 0.2399828\ttotal: 1.46s\tremaining: 3.96s\n",
      "269:\tlearn: 0.2397356\ttotal: 1.47s\tremaining: 3.96s\n",
      "270:\tlearn: 0.2396763\ttotal: 1.47s\tremaining: 3.95s\n",
      "271:\tlearn: 0.2394891\ttotal: 1.48s\tremaining: 3.95s\n",
      "272:\tlearn: 0.2392448\ttotal: 1.48s\tremaining: 3.95s\n",
      "273:\tlearn: 0.2389185\ttotal: 1.49s\tremaining: 3.94s\n",
      "274:\tlearn: 0.2386404\ttotal: 1.5s\tremaining: 3.94s\n",
      "275:\tlearn: 0.2384343\ttotal: 1.5s\tremaining: 3.95s\n",
      "276:\tlearn: 0.2383102\ttotal: 1.51s\tremaining: 3.94s\n",
      "277:\tlearn: 0.2381442\ttotal: 1.51s\tremaining: 3.94s\n",
      "278:\tlearn: 0.2379550\ttotal: 1.52s\tremaining: 3.93s\n",
      "279:\tlearn: 0.2378517\ttotal: 1.53s\tremaining: 3.93s\n",
      "280:\tlearn: 0.2376641\ttotal: 1.53s\tremaining: 3.92s\n",
      "281:\tlearn: 0.2374147\ttotal: 1.54s\tremaining: 3.92s\n",
      "282:\tlearn: 0.2371682\ttotal: 1.54s\tremaining: 3.91s\n",
      "283:\tlearn: 0.2370325\ttotal: 1.55s\tremaining: 3.9s\n",
      "284:\tlearn: 0.2368604\ttotal: 1.56s\tremaining: 3.91s\n",
      "285:\tlearn: 0.2366926\ttotal: 1.56s\tremaining: 3.9s\n",
      "286:\tlearn: 0.2365170\ttotal: 1.57s\tremaining: 3.89s\n",
      "287:\tlearn: 0.2363963\ttotal: 1.57s\tremaining: 3.89s\n",
      "288:\tlearn: 0.2362327\ttotal: 1.58s\tremaining: 3.89s\n",
      "289:\tlearn: 0.2361526\ttotal: 1.59s\tremaining: 3.89s\n",
      "290:\tlearn: 0.2360361\ttotal: 1.6s\tremaining: 3.9s\n",
      "291:\tlearn: 0.2359352\ttotal: 1.61s\tremaining: 3.89s\n",
      "292:\tlearn: 0.2357895\ttotal: 1.61s\tremaining: 3.88s\n",
      "293:\tlearn: 0.2355744\ttotal: 1.61s\tremaining: 3.87s\n",
      "294:\tlearn: 0.2352085\ttotal: 1.61s\tremaining: 3.86s\n",
      "295:\tlearn: 0.2350823\ttotal: 1.62s\tremaining: 3.86s\n",
      "296:\tlearn: 0.2348911\ttotal: 1.63s\tremaining: 3.85s\n",
      "297:\tlearn: 0.2347682\ttotal: 1.63s\tremaining: 3.84s\n",
      "298:\tlearn: 0.2346974\ttotal: 1.63s\tremaining: 3.83s\n",
      "299:\tlearn: 0.2346496\ttotal: 1.64s\tremaining: 3.82s\n",
      "300:\tlearn: 0.2344743\ttotal: 1.64s\tremaining: 3.81s\n",
      "301:\tlearn: 0.2342969\ttotal: 1.64s\tremaining: 3.8s\n",
      "302:\tlearn: 0.2340243\ttotal: 1.65s\tremaining: 3.79s\n",
      "303:\tlearn: 0.2338218\ttotal: 1.65s\tremaining: 3.79s\n",
      "304:\tlearn: 0.2336667\ttotal: 1.66s\tremaining: 3.78s\n",
      "305:\tlearn: 0.2334194\ttotal: 1.66s\tremaining: 3.77s\n",
      "306:\tlearn: 0.2330696\ttotal: 1.67s\tremaining: 3.76s\n",
      "307:\tlearn: 0.2327141\ttotal: 1.67s\tremaining: 3.75s\n",
      "308:\tlearn: 0.2325683\ttotal: 1.67s\tremaining: 3.73s\n",
      "309:\tlearn: 0.2324249\ttotal: 1.67s\tremaining: 3.73s\n",
      "310:\tlearn: 0.2322949\ttotal: 1.68s\tremaining: 3.73s\n",
      "311:\tlearn: 0.2321757\ttotal: 1.68s\tremaining: 3.71s\n",
      "312:\tlearn: 0.2318829\ttotal: 1.69s\tremaining: 3.7s\n",
      "313:\tlearn: 0.2317384\ttotal: 1.69s\tremaining: 3.69s\n",
      "314:\tlearn: 0.2315903\ttotal: 1.69s\tremaining: 3.68s\n",
      "315:\tlearn: 0.2314362\ttotal: 1.7s\tremaining: 3.68s\n",
      "316:\tlearn: 0.2313000\ttotal: 1.71s\tremaining: 3.67s\n",
      "317:\tlearn: 0.2312337\ttotal: 1.71s\tremaining: 3.66s\n",
      "318:\tlearn: 0.2311825\ttotal: 1.71s\tremaining: 3.65s\n",
      "319:\tlearn: 0.2310479\ttotal: 1.71s\tremaining: 3.64s\n",
      "320:\tlearn: 0.2309245\ttotal: 1.72s\tremaining: 3.63s\n",
      "321:\tlearn: 0.2306869\ttotal: 1.72s\tremaining: 3.63s\n",
      "322:\tlearn: 0.2304961\ttotal: 1.73s\tremaining: 3.63s\n",
      "323:\tlearn: 0.2304063\ttotal: 1.73s\tremaining: 3.62s\n",
      "324:\tlearn: 0.2302774\ttotal: 1.74s\tremaining: 3.6s\n",
      "325:\tlearn: 0.2301862\ttotal: 1.74s\tremaining: 3.6s\n",
      "326:\tlearn: 0.2300679\ttotal: 1.74s\tremaining: 3.59s\n",
      "327:\tlearn: 0.2300207\ttotal: 1.75s\tremaining: 3.58s\n",
      "328:\tlearn: 0.2298987\ttotal: 1.75s\tremaining: 3.57s\n",
      "329:\tlearn: 0.2296752\ttotal: 1.76s\tremaining: 3.58s\n",
      "330:\tlearn: 0.2294440\ttotal: 1.76s\tremaining: 3.57s\n",
      "331:\tlearn: 0.2293250\ttotal: 1.77s\tremaining: 3.56s\n",
      "332:\tlearn: 0.2292034\ttotal: 1.77s\tremaining: 3.55s\n",
      "333:\tlearn: 0.2290802\ttotal: 1.78s\tremaining: 3.54s\n",
      "334:\tlearn: 0.2288524\ttotal: 1.78s\tremaining: 3.54s\n",
      "335:\tlearn: 0.2286983\ttotal: 1.78s\tremaining: 3.52s\n",
      "336:\tlearn: 0.2285652\ttotal: 1.8s\tremaining: 3.53s\n",
      "337:\tlearn: 0.2283608\ttotal: 1.8s\tremaining: 3.53s\n",
      "338:\tlearn: 0.2282706\ttotal: 1.8s\tremaining: 3.52s\n",
      "339:\tlearn: 0.2280651\ttotal: 1.81s\tremaining: 3.51s\n",
      "340:\tlearn: 0.2279657\ttotal: 1.81s\tremaining: 3.5s\n",
      "341:\tlearn: 0.2277054\ttotal: 1.81s\tremaining: 3.49s\n",
      "342:\tlearn: 0.2273856\ttotal: 1.82s\tremaining: 3.49s\n",
      "343:\tlearn: 0.2271603\ttotal: 1.83s\tremaining: 3.48s\n",
      "344:\tlearn: 0.2268605\ttotal: 1.83s\tremaining: 3.47s\n",
      "345:\tlearn: 0.2266632\ttotal: 1.83s\tremaining: 3.47s\n",
      "346:\tlearn: 0.2265146\ttotal: 1.85s\tremaining: 3.49s\n",
      "347:\tlearn: 0.2261056\ttotal: 1.86s\tremaining: 3.49s\n",
      "348:\tlearn: 0.2260148\ttotal: 1.86s\tremaining: 3.48s\n",
      "349:\tlearn: 0.2258758\ttotal: 1.88s\tremaining: 3.48s\n",
      "350:\tlearn: 0.2257106\ttotal: 1.88s\tremaining: 3.48s\n",
      "351:\tlearn: 0.2256161\ttotal: 1.88s\tremaining: 3.47s\n",
      "352:\tlearn: 0.2254292\ttotal: 1.89s\tremaining: 3.46s\n",
      "353:\tlearn: 0.2252568\ttotal: 1.89s\tremaining: 3.45s\n",
      "354:\tlearn: 0.2251406\ttotal: 1.9s\tremaining: 3.45s\n",
      "355:\tlearn: 0.2250611\ttotal: 1.9s\tremaining: 3.44s\n",
      "356:\tlearn: 0.2249453\ttotal: 1.91s\tremaining: 3.43s\n",
      "357:\tlearn: 0.2246698\ttotal: 1.91s\tremaining: 3.42s\n",
      "358:\tlearn: 0.2246183\ttotal: 1.92s\tremaining: 3.42s\n",
      "359:\tlearn: 0.2245550\ttotal: 1.92s\tremaining: 3.41s\n",
      "360:\tlearn: 0.2244933\ttotal: 1.92s\tremaining: 3.4s\n",
      "361:\tlearn: 0.2243019\ttotal: 1.94s\tremaining: 3.42s\n",
      "362:\tlearn: 0.2242916\ttotal: 1.94s\tremaining: 3.41s\n",
      "363:\tlearn: 0.2242857\ttotal: 1.95s\tremaining: 3.41s\n",
      "364:\tlearn: 0.2241955\ttotal: 1.95s\tremaining: 3.4s\n",
      "365:\tlearn: 0.2240434\ttotal: 1.96s\tremaining: 3.39s\n",
      "366:\tlearn: 0.2239046\ttotal: 1.97s\tremaining: 3.39s\n",
      "367:\tlearn: 0.2237448\ttotal: 1.97s\tremaining: 3.38s\n",
      "368:\tlearn: 0.2236812\ttotal: 1.97s\tremaining: 3.37s\n",
      "369:\tlearn: 0.2234442\ttotal: 1.98s\tremaining: 3.36s\n",
      "370:\tlearn: 0.2232860\ttotal: 1.99s\tremaining: 3.38s\n",
      "371:\tlearn: 0.2231762\ttotal: 2s\tremaining: 3.37s\n",
      "372:\tlearn: 0.2230501\ttotal: 2s\tremaining: 3.36s\n",
      "373:\tlearn: 0.2230113\ttotal: 2.01s\tremaining: 3.36s\n",
      "374:\tlearn: 0.2227127\ttotal: 2.01s\tremaining: 3.35s\n",
      "375:\tlearn: 0.2226192\ttotal: 2.02s\tremaining: 3.36s\n",
      "376:\tlearn: 0.2223872\ttotal: 2.03s\tremaining: 3.35s\n",
      "377:\tlearn: 0.2221446\ttotal: 2.03s\tremaining: 3.34s\n",
      "378:\tlearn: 0.2219251\ttotal: 2.03s\tremaining: 3.33s\n",
      "379:\tlearn: 0.2217966\ttotal: 2.04s\tremaining: 3.32s\n",
      "380:\tlearn: 0.2217376\ttotal: 2.04s\tremaining: 3.31s\n",
      "381:\tlearn: 0.2216367\ttotal: 2.04s\tremaining: 3.3s\n",
      "382:\tlearn: 0.2215447\ttotal: 2.04s\tremaining: 3.29s\n",
      "383:\tlearn: 0.2213886\ttotal: 2.05s\tremaining: 3.28s\n",
      "384:\tlearn: 0.2213495\ttotal: 2.05s\tremaining: 3.28s\n",
      "385:\tlearn: 0.2212698\ttotal: 2.06s\tremaining: 3.27s\n",
      "386:\tlearn: 0.2211823\ttotal: 2.06s\tremaining: 3.26s\n",
      "387:\tlearn: 0.2210521\ttotal: 2.06s\tremaining: 3.25s\n",
      "388:\tlearn: 0.2209202\ttotal: 2.07s\tremaining: 3.25s\n",
      "389:\tlearn: 0.2208294\ttotal: 2.07s\tremaining: 3.24s\n",
      "390:\tlearn: 0.2206075\ttotal: 2.08s\tremaining: 3.23s\n",
      "391:\tlearn: 0.2204604\ttotal: 2.08s\tremaining: 3.22s\n",
      "392:\tlearn: 0.2203645\ttotal: 2.08s\tremaining: 3.22s\n",
      "393:\tlearn: 0.2202825\ttotal: 2.09s\tremaining: 3.21s\n",
      "394:\tlearn: 0.2202137\ttotal: 2.09s\tremaining: 3.2s\n",
      "395:\tlearn: 0.2198855\ttotal: 2.09s\tremaining: 3.19s\n",
      "396:\tlearn: 0.2197879\ttotal: 2.1s\tremaining: 3.19s\n",
      "397:\tlearn: 0.2196566\ttotal: 2.1s\tremaining: 3.18s\n",
      "398:\tlearn: 0.2195234\ttotal: 2.11s\tremaining: 3.17s\n",
      "399:\tlearn: 0.2194041\ttotal: 2.11s\tremaining: 3.17s\n",
      "400:\tlearn: 0.2193758\ttotal: 2.12s\tremaining: 3.17s\n",
      "401:\tlearn: 0.2192257\ttotal: 2.12s\tremaining: 3.16s\n",
      "402:\tlearn: 0.2190786\ttotal: 2.13s\tremaining: 3.15s\n",
      "403:\tlearn: 0.2188574\ttotal: 2.13s\tremaining: 3.15s\n",
      "404:\tlearn: 0.2187413\ttotal: 2.14s\tremaining: 3.14s\n",
      "405:\tlearn: 0.2185934\ttotal: 2.14s\tremaining: 3.14s\n",
      "406:\tlearn: 0.2183952\ttotal: 2.15s\tremaining: 3.13s\n",
      "407:\tlearn: 0.2182891\ttotal: 2.15s\tremaining: 3.12s\n",
      "408:\tlearn: 0.2181799\ttotal: 2.15s\tremaining: 3.11s\n",
      "409:\tlearn: 0.2180707\ttotal: 2.16s\tremaining: 3.1s\n",
      "410:\tlearn: 0.2179033\ttotal: 2.16s\tremaining: 3.1s\n",
      "411:\tlearn: 0.2175762\ttotal: 2.16s\tremaining: 3.09s\n",
      "412:\tlearn: 0.2174362\ttotal: 2.16s\tremaining: 3.08s\n",
      "413:\tlearn: 0.2172388\ttotal: 2.17s\tremaining: 3.07s\n",
      "414:\tlearn: 0.2170665\ttotal: 2.18s\tremaining: 3.07s\n",
      "415:\tlearn: 0.2169084\ttotal: 2.18s\tremaining: 3.06s\n",
      "416:\tlearn: 0.2168139\ttotal: 2.19s\tremaining: 3.06s\n",
      "417:\tlearn: 0.2167063\ttotal: 2.19s\tremaining: 3.05s\n",
      "418:\tlearn: 0.2165891\ttotal: 2.19s\tremaining: 3.04s\n",
      "419:\tlearn: 0.2163825\ttotal: 2.2s\tremaining: 3.04s\n",
      "420:\tlearn: 0.2162076\ttotal: 2.22s\tremaining: 3.05s\n",
      "421:\tlearn: 0.2159947\ttotal: 2.22s\tremaining: 3.04s\n",
      "422:\tlearn: 0.2156835\ttotal: 2.22s\tremaining: 3.03s\n",
      "423:\tlearn: 0.2155395\ttotal: 2.23s\tremaining: 3.03s\n",
      "424:\tlearn: 0.2154196\ttotal: 2.24s\tremaining: 3.03s\n",
      "425:\tlearn: 0.2153694\ttotal: 2.24s\tremaining: 3.02s\n",
      "426:\tlearn: 0.2152359\ttotal: 2.26s\tremaining: 3.03s\n",
      "427:\tlearn: 0.2151310\ttotal: 2.26s\tremaining: 3.02s\n",
      "428:\tlearn: 0.2150350\ttotal: 2.27s\tremaining: 3.02s\n",
      "429:\tlearn: 0.2147834\ttotal: 2.27s\tremaining: 3.01s\n",
      "430:\tlearn: 0.2147137\ttotal: 2.28s\tremaining: 3s\n",
      "431:\tlearn: 0.2146212\ttotal: 2.28s\tremaining: 3s\n",
      "432:\tlearn: 0.2145220\ttotal: 2.28s\tremaining: 2.99s\n",
      "433:\tlearn: 0.2144034\ttotal: 2.29s\tremaining: 2.98s\n",
      "434:\tlearn: 0.2143536\ttotal: 2.3s\tremaining: 2.98s\n",
      "435:\tlearn: 0.2140907\ttotal: 2.3s\tremaining: 2.98s\n",
      "436:\tlearn: 0.2140141\ttotal: 2.31s\tremaining: 2.98s\n",
      "437:\tlearn: 0.2137243\ttotal: 2.31s\tremaining: 2.96s\n",
      "438:\tlearn: 0.2135110\ttotal: 2.32s\tremaining: 2.96s\n",
      "439:\tlearn: 0.2132949\ttotal: 2.32s\tremaining: 2.95s\n",
      "440:\tlearn: 0.2131315\ttotal: 2.32s\tremaining: 2.95s\n",
      "441:\tlearn: 0.2130065\ttotal: 2.33s\tremaining: 2.94s\n",
      "442:\tlearn: 0.2130017\ttotal: 2.33s\tremaining: 2.93s\n",
      "443:\tlearn: 0.2127247\ttotal: 2.34s\tremaining: 2.93s\n",
      "444:\tlearn: 0.2125937\ttotal: 2.34s\tremaining: 2.92s\n",
      "445:\tlearn: 0.2125854\ttotal: 2.34s\tremaining: 2.91s\n",
      "446:\tlearn: 0.2124495\ttotal: 2.35s\tremaining: 2.9s\n",
      "447:\tlearn: 0.2122472\ttotal: 2.35s\tremaining: 2.9s\n",
      "448:\tlearn: 0.2122309\ttotal: 2.35s\tremaining: 2.89s\n",
      "449:\tlearn: 0.2120980\ttotal: 2.36s\tremaining: 2.88s\n",
      "450:\tlearn: 0.2119417\ttotal: 2.36s\tremaining: 2.87s\n",
      "451:\tlearn: 0.2118033\ttotal: 2.36s\tremaining: 2.86s\n",
      "452:\tlearn: 0.2116273\ttotal: 2.37s\tremaining: 2.86s\n",
      "453:\tlearn: 0.2114224\ttotal: 2.37s\tremaining: 2.85s\n",
      "454:\tlearn: 0.2114158\ttotal: 2.38s\tremaining: 2.85s\n",
      "455:\tlearn: 0.2113410\ttotal: 2.39s\tremaining: 2.85s\n",
      "456:\tlearn: 0.2111088\ttotal: 2.39s\tremaining: 2.84s\n",
      "457:\tlearn: 0.2110729\ttotal: 2.4s\tremaining: 2.84s\n",
      "458:\tlearn: 0.2108039\ttotal: 2.4s\tremaining: 2.83s\n",
      "459:\tlearn: 0.2106188\ttotal: 2.41s\tremaining: 2.83s\n",
      "460:\tlearn: 0.2105795\ttotal: 2.41s\tremaining: 2.82s\n",
      "461:\tlearn: 0.2105758\ttotal: 2.41s\tremaining: 2.81s\n",
      "462:\tlearn: 0.2104744\ttotal: 2.42s\tremaining: 2.8s\n",
      "463:\tlearn: 0.2100950\ttotal: 2.42s\tremaining: 2.8s\n",
      "464:\tlearn: 0.2099573\ttotal: 2.42s\tremaining: 2.79s\n",
      "465:\tlearn: 0.2097691\ttotal: 2.43s\tremaining: 2.78s\n",
      "466:\tlearn: 0.2097295\ttotal: 2.43s\tremaining: 2.77s\n",
      "467:\tlearn: 0.2096376\ttotal: 2.43s\tremaining: 2.77s\n",
      "468:\tlearn: 0.2094872\ttotal: 2.44s\tremaining: 2.76s\n",
      "469:\tlearn: 0.2093685\ttotal: 2.45s\tremaining: 2.76s\n",
      "470:\tlearn: 0.2091391\ttotal: 2.46s\tremaining: 2.76s\n",
      "471:\tlearn: 0.2089960\ttotal: 2.46s\tremaining: 2.75s\n",
      "472:\tlearn: 0.2089603\ttotal: 2.46s\tremaining: 2.74s\n",
      "473:\tlearn: 0.2088787\ttotal: 2.46s\tremaining: 2.73s\n",
      "474:\tlearn: 0.2088030\ttotal: 2.47s\tremaining: 2.73s\n",
      "475:\tlearn: 0.2085637\ttotal: 2.47s\tremaining: 2.72s\n",
      "476:\tlearn: 0.2082634\ttotal: 2.48s\tremaining: 2.71s\n",
      "477:\tlearn: 0.2080935\ttotal: 2.48s\tremaining: 2.71s\n",
      "478:\tlearn: 0.2079447\ttotal: 2.49s\tremaining: 2.71s\n",
      "479:\tlearn: 0.2078837\ttotal: 2.5s\tremaining: 2.7s\n",
      "480:\tlearn: 0.2078239\ttotal: 2.5s\tremaining: 2.7s\n",
      "481:\tlearn: 0.2076489\ttotal: 2.5s\tremaining: 2.69s\n",
      "482:\tlearn: 0.2075748\ttotal: 2.51s\tremaining: 2.68s\n",
      "483:\tlearn: 0.2075473\ttotal: 2.51s\tremaining: 2.68s\n",
      "484:\tlearn: 0.2074543\ttotal: 2.52s\tremaining: 2.67s\n",
      "485:\tlearn: 0.2073182\ttotal: 2.52s\tremaining: 2.67s\n",
      "486:\tlearn: 0.2071308\ttotal: 2.53s\tremaining: 2.66s\n",
      "487:\tlearn: 0.2070165\ttotal: 2.53s\tremaining: 2.66s\n",
      "488:\tlearn: 0.2068909\ttotal: 2.54s\tremaining: 2.65s\n",
      "489:\tlearn: 0.2067899\ttotal: 2.54s\tremaining: 2.65s\n",
      "490:\tlearn: 0.2066772\ttotal: 2.55s\tremaining: 2.65s\n",
      "491:\tlearn: 0.2064992\ttotal: 2.56s\tremaining: 2.64s\n",
      "492:\tlearn: 0.2062841\ttotal: 2.57s\tremaining: 2.64s\n",
      "493:\tlearn: 0.2061534\ttotal: 2.57s\tremaining: 2.63s\n",
      "494:\tlearn: 0.2060136\ttotal: 2.58s\tremaining: 2.63s\n",
      "495:\tlearn: 0.2057980\ttotal: 2.58s\tremaining: 2.63s\n",
      "496:\tlearn: 0.2057597\ttotal: 2.59s\tremaining: 2.62s\n",
      "497:\tlearn: 0.2056358\ttotal: 2.59s\tremaining: 2.61s\n",
      "498:\tlearn: 0.2055912\ttotal: 2.59s\tremaining: 2.6s\n",
      "499:\tlearn: 0.2054690\ttotal: 2.6s\tremaining: 2.6s\n",
      "500:\tlearn: 0.2053108\ttotal: 2.61s\tremaining: 2.6s\n",
      "501:\tlearn: 0.2053095\ttotal: 2.61s\tremaining: 2.59s\n",
      "502:\tlearn: 0.2051423\ttotal: 2.62s\tremaining: 2.58s\n",
      "503:\tlearn: 0.2050271\ttotal: 2.62s\tremaining: 2.58s\n",
      "504:\tlearn: 0.2049462\ttotal: 2.62s\tremaining: 2.57s\n",
      "505:\tlearn: 0.2047587\ttotal: 2.63s\tremaining: 2.56s\n",
      "506:\tlearn: 0.2046026\ttotal: 2.63s\tremaining: 2.56s\n",
      "507:\tlearn: 0.2045680\ttotal: 2.63s\tremaining: 2.55s\n",
      "508:\tlearn: 0.2045327\ttotal: 2.64s\tremaining: 2.55s\n",
      "509:\tlearn: 0.2044080\ttotal: 2.64s\tremaining: 2.54s\n",
      "510:\tlearn: 0.2042794\ttotal: 2.65s\tremaining: 2.53s\n",
      "511:\tlearn: 0.2042350\ttotal: 2.65s\tremaining: 2.53s\n",
      "512:\tlearn: 0.2041609\ttotal: 2.67s\tremaining: 2.53s\n",
      "513:\tlearn: 0.2040419\ttotal: 2.67s\tremaining: 2.52s\n",
      "514:\tlearn: 0.2039260\ttotal: 2.67s\tremaining: 2.52s\n",
      "515:\tlearn: 0.2037563\ttotal: 2.68s\tremaining: 2.51s\n",
      "516:\tlearn: 0.2037090\ttotal: 2.69s\tremaining: 2.51s\n",
      "517:\tlearn: 0.2034436\ttotal: 2.69s\tremaining: 2.5s\n",
      "518:\tlearn: 0.2033397\ttotal: 2.69s\tremaining: 2.5s\n",
      "519:\tlearn: 0.2033386\ttotal: 2.69s\tremaining: 2.49s\n",
      "520:\tlearn: 0.2032061\ttotal: 2.69s\tremaining: 2.48s\n",
      "521:\tlearn: 0.2031355\ttotal: 2.7s\tremaining: 2.47s\n",
      "522:\tlearn: 0.2029453\ttotal: 2.71s\tremaining: 2.47s\n",
      "523:\tlearn: 0.2028471\ttotal: 2.71s\tremaining: 2.46s\n",
      "524:\tlearn: 0.2025457\ttotal: 2.72s\tremaining: 2.46s\n",
      "525:\tlearn: 0.2023712\ttotal: 2.72s\tremaining: 2.45s\n",
      "526:\tlearn: 0.2022421\ttotal: 2.73s\tremaining: 2.45s\n",
      "527:\tlearn: 0.2020639\ttotal: 2.73s\tremaining: 2.44s\n",
      "528:\tlearn: 0.2019526\ttotal: 2.73s\tremaining: 2.43s\n",
      "529:\tlearn: 0.2018337\ttotal: 2.73s\tremaining: 2.42s\n",
      "530:\tlearn: 0.2016946\ttotal: 2.74s\tremaining: 2.42s\n",
      "531:\tlearn: 0.2015532\ttotal: 2.75s\tremaining: 2.42s\n",
      "532:\tlearn: 0.2013892\ttotal: 2.75s\tremaining: 2.41s\n",
      "533:\tlearn: 0.2012868\ttotal: 2.75s\tremaining: 2.4s\n",
      "534:\tlearn: 0.2011257\ttotal: 2.76s\tremaining: 2.4s\n",
      "535:\tlearn: 0.2009077\ttotal: 2.76s\tremaining: 2.39s\n",
      "536:\tlearn: 0.2008807\ttotal: 2.76s\tremaining: 2.38s\n",
      "537:\tlearn: 0.2007922\ttotal: 2.77s\tremaining: 2.38s\n",
      "538:\tlearn: 0.2005709\ttotal: 2.77s\tremaining: 2.37s\n",
      "539:\tlearn: 0.2003769\ttotal: 2.78s\tremaining: 2.37s\n",
      "540:\tlearn: 0.2003001\ttotal: 2.79s\tremaining: 2.36s\n",
      "541:\tlearn: 0.2001344\ttotal: 2.79s\tremaining: 2.36s\n",
      "542:\tlearn: 0.1999723\ttotal: 2.79s\tremaining: 2.35s\n",
      "543:\tlearn: 0.1998358\ttotal: 2.8s\tremaining: 2.35s\n",
      "544:\tlearn: 0.1996718\ttotal: 2.8s\tremaining: 2.34s\n",
      "545:\tlearn: 0.1995940\ttotal: 2.8s\tremaining: 2.33s\n",
      "546:\tlearn: 0.1994776\ttotal: 2.81s\tremaining: 2.33s\n",
      "547:\tlearn: 0.1992762\ttotal: 2.82s\tremaining: 2.32s\n",
      "548:\tlearn: 0.1991192\ttotal: 2.82s\tremaining: 2.32s\n",
      "549:\tlearn: 0.1989142\ttotal: 2.83s\tremaining: 2.31s\n",
      "550:\tlearn: 0.1987560\ttotal: 2.83s\tremaining: 2.31s\n",
      "551:\tlearn: 0.1985649\ttotal: 2.84s\tremaining: 2.31s\n",
      "552:\tlearn: 0.1983685\ttotal: 2.85s\tremaining: 2.31s\n",
      "553:\tlearn: 0.1982573\ttotal: 2.86s\tremaining: 2.3s\n",
      "554:\tlearn: 0.1982027\ttotal: 2.87s\tremaining: 2.3s\n",
      "555:\tlearn: 0.1980175\ttotal: 2.88s\tremaining: 2.3s\n",
      "556:\tlearn: 0.1979445\ttotal: 2.89s\tremaining: 2.3s\n",
      "557:\tlearn: 0.1977828\ttotal: 2.92s\tremaining: 2.31s\n",
      "558:\tlearn: 0.1976518\ttotal: 2.93s\tremaining: 2.31s\n",
      "559:\tlearn: 0.1974228\ttotal: 2.94s\tremaining: 2.31s\n",
      "560:\tlearn: 0.1972713\ttotal: 2.94s\tremaining: 2.3s\n",
      "561:\tlearn: 0.1970024\ttotal: 2.94s\tremaining: 2.29s\n",
      "562:\tlearn: 0.1969017\ttotal: 2.94s\tremaining: 2.29s\n",
      "563:\tlearn: 0.1968182\ttotal: 2.95s\tremaining: 2.28s\n",
      "564:\tlearn: 0.1967129\ttotal: 2.95s\tremaining: 2.27s\n",
      "565:\tlearn: 0.1964126\ttotal: 2.95s\tremaining: 2.27s\n",
      "566:\tlearn: 0.1962957\ttotal: 2.98s\tremaining: 2.27s\n",
      "567:\tlearn: 0.1962739\ttotal: 2.98s\tremaining: 2.27s\n",
      "568:\tlearn: 0.1960360\ttotal: 2.98s\tremaining: 2.26s\n",
      "569:\tlearn: 0.1959366\ttotal: 2.99s\tremaining: 2.25s\n",
      "570:\tlearn: 0.1958336\ttotal: 2.99s\tremaining: 2.25s\n",
      "571:\tlearn: 0.1956692\ttotal: 3s\tremaining: 2.24s\n",
      "572:\tlearn: 0.1955430\ttotal: 3s\tremaining: 2.23s\n",
      "573:\tlearn: 0.1954198\ttotal: 3.01s\tremaining: 2.23s\n",
      "574:\tlearn: 0.1953035\ttotal: 3.01s\tremaining: 2.23s\n",
      "575:\tlearn: 0.1952120\ttotal: 3.01s\tremaining: 2.22s\n",
      "576:\tlearn: 0.1949366\ttotal: 3.02s\tremaining: 2.21s\n",
      "577:\tlearn: 0.1947678\ttotal: 3.02s\tremaining: 2.21s\n",
      "578:\tlearn: 0.1946486\ttotal: 3.02s\tremaining: 2.2s\n",
      "579:\tlearn: 0.1944092\ttotal: 3.03s\tremaining: 2.19s\n",
      "580:\tlearn: 0.1943423\ttotal: 3.03s\tremaining: 2.19s\n",
      "581:\tlearn: 0.1941163\ttotal: 3.04s\tremaining: 2.18s\n",
      "582:\tlearn: 0.1939133\ttotal: 3.04s\tremaining: 2.18s\n",
      "583:\tlearn: 0.1937391\ttotal: 3.05s\tremaining: 2.17s\n",
      "584:\tlearn: 0.1936280\ttotal: 3.06s\tremaining: 2.17s\n",
      "585:\tlearn: 0.1935411\ttotal: 3.06s\tremaining: 2.16s\n",
      "586:\tlearn: 0.1934214\ttotal: 3.06s\tremaining: 2.16s\n",
      "587:\tlearn: 0.1932917\ttotal: 3.07s\tremaining: 2.15s\n",
      "588:\tlearn: 0.1931228\ttotal: 3.07s\tremaining: 2.14s\n",
      "589:\tlearn: 0.1929543\ttotal: 3.07s\tremaining: 2.14s\n",
      "590:\tlearn: 0.1929031\ttotal: 3.08s\tremaining: 2.13s\n",
      "591:\tlearn: 0.1928583\ttotal: 3.08s\tremaining: 2.12s\n",
      "592:\tlearn: 0.1927388\ttotal: 3.08s\tremaining: 2.11s\n",
      "593:\tlearn: 0.1926675\ttotal: 3.09s\tremaining: 2.11s\n",
      "594:\tlearn: 0.1925537\ttotal: 3.09s\tremaining: 2.1s\n",
      "595:\tlearn: 0.1923150\ttotal: 3.09s\tremaining: 2.1s\n",
      "596:\tlearn: 0.1922455\ttotal: 3.1s\tremaining: 2.09s\n",
      "597:\tlearn: 0.1921003\ttotal: 3.1s\tremaining: 2.08s\n",
      "598:\tlearn: 0.1920017\ttotal: 3.1s\tremaining: 2.08s\n",
      "599:\tlearn: 0.1919087\ttotal: 3.1s\tremaining: 2.07s\n",
      "600:\tlearn: 0.1917062\ttotal: 3.11s\tremaining: 2.06s\n",
      "601:\tlearn: 0.1916544\ttotal: 3.11s\tremaining: 2.06s\n",
      "602:\tlearn: 0.1914244\ttotal: 3.11s\tremaining: 2.05s\n",
      "603:\tlearn: 0.1913356\ttotal: 3.12s\tremaining: 2.04s\n",
      "604:\tlearn: 0.1911654\ttotal: 3.12s\tremaining: 2.04s\n",
      "605:\tlearn: 0.1910156\ttotal: 3.13s\tremaining: 2.03s\n",
      "606:\tlearn: 0.1908750\ttotal: 3.13s\tremaining: 2.03s\n",
      "607:\tlearn: 0.1907695\ttotal: 3.13s\tremaining: 2.02s\n",
      "608:\tlearn: 0.1906934\ttotal: 3.14s\tremaining: 2.01s\n",
      "609:\tlearn: 0.1905776\ttotal: 3.14s\tremaining: 2.01s\n",
      "610:\tlearn: 0.1905177\ttotal: 3.15s\tremaining: 2s\n",
      "611:\tlearn: 0.1903238\ttotal: 3.16s\tremaining: 2s\n",
      "612:\tlearn: 0.1901851\ttotal: 3.16s\tremaining: 2s\n",
      "613:\tlearn: 0.1900826\ttotal: 3.17s\tremaining: 1.99s\n",
      "614:\tlearn: 0.1900373\ttotal: 3.17s\tremaining: 1.98s\n",
      "615:\tlearn: 0.1898853\ttotal: 3.17s\tremaining: 1.98s\n",
      "616:\tlearn: 0.1898541\ttotal: 3.18s\tremaining: 1.97s\n",
      "617:\tlearn: 0.1897199\ttotal: 3.18s\tremaining: 1.97s\n",
      "618:\tlearn: 0.1896235\ttotal: 3.19s\tremaining: 1.96s\n",
      "619:\tlearn: 0.1895062\ttotal: 3.19s\tremaining: 1.96s\n",
      "620:\tlearn: 0.1893824\ttotal: 3.19s\tremaining: 1.95s\n",
      "621:\tlearn: 0.1891998\ttotal: 3.2s\tremaining: 1.95s\n",
      "622:\tlearn: 0.1891379\ttotal: 3.21s\tremaining: 1.94s\n",
      "623:\tlearn: 0.1891211\ttotal: 3.21s\tremaining: 1.94s\n",
      "624:\tlearn: 0.1889701\ttotal: 3.22s\tremaining: 1.93s\n",
      "625:\tlearn: 0.1888023\ttotal: 3.22s\tremaining: 1.93s\n",
      "626:\tlearn: 0.1887021\ttotal: 3.23s\tremaining: 1.92s\n",
      "627:\tlearn: 0.1885237\ttotal: 3.23s\tremaining: 1.91s\n",
      "628:\tlearn: 0.1883774\ttotal: 3.23s\tremaining: 1.91s\n",
      "629:\tlearn: 0.1882607\ttotal: 3.24s\tremaining: 1.9s\n",
      "630:\tlearn: 0.1881445\ttotal: 3.24s\tremaining: 1.89s\n",
      "631:\tlearn: 0.1880089\ttotal: 3.24s\tremaining: 1.89s\n",
      "632:\tlearn: 0.1878620\ttotal: 3.25s\tremaining: 1.88s\n",
      "633:\tlearn: 0.1877598\ttotal: 3.25s\tremaining: 1.88s\n",
      "634:\tlearn: 0.1877284\ttotal: 3.25s\tremaining: 1.87s\n",
      "635:\tlearn: 0.1876036\ttotal: 3.26s\tremaining: 1.87s\n",
      "636:\tlearn: 0.1875092\ttotal: 3.27s\tremaining: 1.86s\n",
      "637:\tlearn: 0.1873445\ttotal: 3.27s\tremaining: 1.85s\n",
      "638:\tlearn: 0.1872457\ttotal: 3.27s\tremaining: 1.85s\n",
      "639:\tlearn: 0.1870896\ttotal: 3.28s\tremaining: 1.85s\n",
      "640:\tlearn: 0.1870004\ttotal: 3.29s\tremaining: 1.84s\n",
      "641:\tlearn: 0.1868614\ttotal: 3.29s\tremaining: 1.83s\n",
      "642:\tlearn: 0.1867660\ttotal: 3.29s\tremaining: 1.83s\n",
      "643:\tlearn: 0.1865847\ttotal: 3.29s\tremaining: 1.82s\n",
      "644:\tlearn: 0.1865354\ttotal: 3.31s\tremaining: 1.82s\n",
      "645:\tlearn: 0.1863061\ttotal: 3.31s\tremaining: 1.82s\n",
      "646:\tlearn: 0.1861545\ttotal: 3.32s\tremaining: 1.81s\n",
      "647:\tlearn: 0.1859352\ttotal: 3.32s\tremaining: 1.8s\n",
      "648:\tlearn: 0.1858087\ttotal: 3.33s\tremaining: 1.8s\n",
      "649:\tlearn: 0.1855898\ttotal: 3.33s\tremaining: 1.79s\n",
      "650:\tlearn: 0.1853855\ttotal: 3.33s\tremaining: 1.79s\n",
      "651:\tlearn: 0.1852505\ttotal: 3.34s\tremaining: 1.78s\n",
      "652:\tlearn: 0.1851038\ttotal: 3.35s\tremaining: 1.78s\n",
      "653:\tlearn: 0.1849817\ttotal: 3.36s\tremaining: 1.77s\n",
      "654:\tlearn: 0.1848170\ttotal: 3.36s\tremaining: 1.77s\n",
      "655:\tlearn: 0.1847354\ttotal: 3.36s\tremaining: 1.76s\n",
      "656:\tlearn: 0.1846549\ttotal: 3.37s\tremaining: 1.76s\n",
      "657:\tlearn: 0.1844606\ttotal: 3.37s\tremaining: 1.75s\n",
      "658:\tlearn: 0.1843474\ttotal: 3.38s\tremaining: 1.75s\n",
      "659:\tlearn: 0.1842466\ttotal: 3.39s\tremaining: 1.75s\n",
      "660:\tlearn: 0.1841059\ttotal: 3.39s\tremaining: 1.74s\n",
      "661:\tlearn: 0.1840215\ttotal: 3.41s\tremaining: 1.74s\n",
      "662:\tlearn: 0.1839464\ttotal: 3.41s\tremaining: 1.73s\n",
      "663:\tlearn: 0.1838363\ttotal: 3.42s\tremaining: 1.73s\n",
      "664:\tlearn: 0.1836015\ttotal: 3.42s\tremaining: 1.72s\n",
      "665:\tlearn: 0.1834832\ttotal: 3.42s\tremaining: 1.72s\n",
      "666:\tlearn: 0.1832715\ttotal: 3.43s\tremaining: 1.71s\n",
      "667:\tlearn: 0.1831175\ttotal: 3.43s\tremaining: 1.71s\n",
      "668:\tlearn: 0.1829688\ttotal: 3.44s\tremaining: 1.7s\n",
      "669:\tlearn: 0.1827912\ttotal: 3.44s\tremaining: 1.69s\n",
      "670:\tlearn: 0.1827152\ttotal: 3.45s\tremaining: 1.69s\n",
      "671:\tlearn: 0.1826601\ttotal: 3.45s\tremaining: 1.69s\n",
      "672:\tlearn: 0.1825593\ttotal: 3.46s\tremaining: 1.68s\n",
      "673:\tlearn: 0.1824615\ttotal: 3.46s\tremaining: 1.67s\n",
      "674:\tlearn: 0.1822625\ttotal: 3.47s\tremaining: 1.67s\n",
      "675:\tlearn: 0.1820594\ttotal: 3.47s\tremaining: 1.66s\n",
      "676:\tlearn: 0.1818771\ttotal: 3.47s\tremaining: 1.66s\n",
      "677:\tlearn: 0.1817622\ttotal: 3.48s\tremaining: 1.65s\n",
      "678:\tlearn: 0.1814914\ttotal: 3.48s\tremaining: 1.65s\n",
      "679:\tlearn: 0.1813584\ttotal: 3.49s\tremaining: 1.64s\n",
      "680:\tlearn: 0.1811858\ttotal: 3.5s\tremaining: 1.64s\n",
      "681:\tlearn: 0.1810272\ttotal: 3.5s\tremaining: 1.63s\n",
      "682:\tlearn: 0.1807415\ttotal: 3.51s\tremaining: 1.63s\n",
      "683:\tlearn: 0.1806087\ttotal: 3.51s\tremaining: 1.62s\n",
      "684:\tlearn: 0.1805133\ttotal: 3.51s\tremaining: 1.61s\n",
      "685:\tlearn: 0.1804115\ttotal: 3.52s\tremaining: 1.61s\n",
      "686:\tlearn: 0.1803074\ttotal: 3.52s\tremaining: 1.6s\n",
      "687:\tlearn: 0.1801594\ttotal: 3.52s\tremaining: 1.6s\n",
      "688:\tlearn: 0.1801063\ttotal: 3.52s\tremaining: 1.59s\n",
      "689:\tlearn: 0.1800102\ttotal: 3.53s\tremaining: 1.58s\n",
      "690:\tlearn: 0.1799418\ttotal: 3.53s\tremaining: 1.58s\n",
      "691:\tlearn: 0.1798397\ttotal: 3.53s\tremaining: 1.57s\n",
      "692:\tlearn: 0.1797224\ttotal: 3.54s\tremaining: 1.57s\n",
      "693:\tlearn: 0.1795551\ttotal: 3.54s\tremaining: 1.56s\n",
      "694:\tlearn: 0.1794502\ttotal: 3.54s\tremaining: 1.55s\n",
      "695:\tlearn: 0.1792731\ttotal: 3.54s\tremaining: 1.55s\n",
      "696:\tlearn: 0.1791811\ttotal: 3.54s\tremaining: 1.54s\n",
      "697:\tlearn: 0.1791168\ttotal: 3.55s\tremaining: 1.53s\n",
      "698:\tlearn: 0.1789297\ttotal: 3.55s\tremaining: 1.53s\n",
      "699:\tlearn: 0.1787635\ttotal: 3.56s\tremaining: 1.52s\n",
      "700:\tlearn: 0.1785926\ttotal: 3.56s\tremaining: 1.52s\n",
      "701:\tlearn: 0.1784454\ttotal: 3.56s\tremaining: 1.51s\n",
      "702:\tlearn: 0.1783140\ttotal: 3.57s\tremaining: 1.51s\n",
      "703:\tlearn: 0.1781598\ttotal: 3.57s\tremaining: 1.5s\n",
      "704:\tlearn: 0.1780871\ttotal: 3.57s\tremaining: 1.49s\n",
      "705:\tlearn: 0.1780030\ttotal: 3.58s\tremaining: 1.49s\n",
      "706:\tlearn: 0.1779236\ttotal: 3.58s\tremaining: 1.48s\n",
      "707:\tlearn: 0.1776944\ttotal: 3.58s\tremaining: 1.48s\n",
      "708:\tlearn: 0.1776257\ttotal: 3.58s\tremaining: 1.47s\n",
      "709:\tlearn: 0.1774370\ttotal: 3.59s\tremaining: 1.46s\n",
      "710:\tlearn: 0.1773247\ttotal: 3.59s\tremaining: 1.46s\n",
      "711:\tlearn: 0.1772065\ttotal: 3.6s\tremaining: 1.45s\n",
      "712:\tlearn: 0.1770076\ttotal: 3.6s\tremaining: 1.45s\n",
      "713:\tlearn: 0.1768499\ttotal: 3.6s\tremaining: 1.44s\n",
      "714:\tlearn: 0.1767008\ttotal: 3.6s\tremaining: 1.44s\n",
      "715:\tlearn: 0.1765832\ttotal: 3.61s\tremaining: 1.43s\n",
      "716:\tlearn: 0.1765528\ttotal: 3.62s\tremaining: 1.43s\n",
      "717:\tlearn: 0.1764441\ttotal: 3.63s\tremaining: 1.42s\n",
      "718:\tlearn: 0.1763216\ttotal: 3.63s\tremaining: 1.42s\n",
      "719:\tlearn: 0.1762289\ttotal: 3.63s\tremaining: 1.41s\n",
      "720:\tlearn: 0.1761315\ttotal: 3.63s\tremaining: 1.41s\n",
      "721:\tlearn: 0.1760667\ttotal: 3.65s\tremaining: 1.4s\n",
      "722:\tlearn: 0.1759446\ttotal: 3.65s\tremaining: 1.4s\n",
      "723:\tlearn: 0.1757812\ttotal: 3.66s\tremaining: 1.39s\n",
      "724:\tlearn: 0.1756532\ttotal: 3.66s\tremaining: 1.39s\n",
      "725:\tlearn: 0.1753957\ttotal: 3.66s\tremaining: 1.38s\n",
      "726:\tlearn: 0.1751159\ttotal: 3.67s\tremaining: 1.38s\n",
      "727:\tlearn: 0.1749102\ttotal: 3.67s\tremaining: 1.37s\n",
      "728:\tlearn: 0.1748600\ttotal: 3.67s\tremaining: 1.37s\n",
      "729:\tlearn: 0.1747257\ttotal: 3.68s\tremaining: 1.36s\n",
      "730:\tlearn: 0.1746251\ttotal: 3.68s\tremaining: 1.35s\n",
      "731:\tlearn: 0.1745503\ttotal: 3.69s\tremaining: 1.35s\n",
      "732:\tlearn: 0.1744385\ttotal: 3.69s\tremaining: 1.35s\n",
      "733:\tlearn: 0.1743587\ttotal: 3.7s\tremaining: 1.34s\n",
      "734:\tlearn: 0.1741670\ttotal: 3.7s\tremaining: 1.33s\n",
      "735:\tlearn: 0.1740420\ttotal: 3.71s\tremaining: 1.33s\n",
      "736:\tlearn: 0.1739671\ttotal: 3.71s\tremaining: 1.32s\n",
      "737:\tlearn: 0.1738519\ttotal: 3.71s\tremaining: 1.32s\n",
      "738:\tlearn: 0.1737087\ttotal: 3.72s\tremaining: 1.31s\n",
      "739:\tlearn: 0.1736055\ttotal: 3.73s\tremaining: 1.31s\n",
      "740:\tlearn: 0.1733861\ttotal: 3.73s\tremaining: 1.3s\n",
      "741:\tlearn: 0.1732867\ttotal: 3.73s\tremaining: 1.3s\n",
      "742:\tlearn: 0.1731636\ttotal: 3.73s\tremaining: 1.29s\n",
      "743:\tlearn: 0.1729751\ttotal: 3.74s\tremaining: 1.28s\n",
      "744:\tlearn: 0.1728165\ttotal: 3.74s\tremaining: 1.28s\n",
      "745:\tlearn: 0.1727323\ttotal: 3.74s\tremaining: 1.27s\n",
      "746:\tlearn: 0.1726376\ttotal: 3.75s\tremaining: 1.27s\n",
      "747:\tlearn: 0.1725702\ttotal: 3.76s\tremaining: 1.27s\n",
      "748:\tlearn: 0.1724853\ttotal: 3.76s\tremaining: 1.26s\n",
      "749:\tlearn: 0.1722796\ttotal: 3.76s\tremaining: 1.25s\n",
      "750:\tlearn: 0.1721509\ttotal: 3.77s\tremaining: 1.25s\n",
      "751:\tlearn: 0.1720501\ttotal: 3.77s\tremaining: 1.24s\n",
      "752:\tlearn: 0.1719861\ttotal: 3.77s\tremaining: 1.24s\n",
      "753:\tlearn: 0.1718594\ttotal: 3.78s\tremaining: 1.23s\n",
      "754:\tlearn: 0.1716826\ttotal: 3.79s\tremaining: 1.23s\n",
      "755:\tlearn: 0.1715826\ttotal: 3.79s\tremaining: 1.22s\n",
      "756:\tlearn: 0.1714481\ttotal: 3.8s\tremaining: 1.22s\n",
      "757:\tlearn: 0.1714118\ttotal: 3.8s\tremaining: 1.21s\n",
      "758:\tlearn: 0.1713266\ttotal: 3.81s\tremaining: 1.21s\n",
      "759:\tlearn: 0.1712391\ttotal: 3.81s\tremaining: 1.2s\n",
      "760:\tlearn: 0.1711615\ttotal: 3.82s\tremaining: 1.2s\n",
      "761:\tlearn: 0.1710309\ttotal: 3.83s\tremaining: 1.2s\n",
      "762:\tlearn: 0.1709624\ttotal: 3.83s\tremaining: 1.19s\n",
      "763:\tlearn: 0.1709306\ttotal: 3.94s\tremaining: 1.22s\n",
      "764:\tlearn: 0.1707429\ttotal: 3.97s\tremaining: 1.22s\n",
      "765:\tlearn: 0.1706234\ttotal: 3.98s\tremaining: 1.22s\n",
      "766:\tlearn: 0.1705646\ttotal: 3.98s\tremaining: 1.21s\n",
      "767:\tlearn: 0.1705382\ttotal: 3.98s\tremaining: 1.2s\n",
      "768:\tlearn: 0.1704120\ttotal: 3.99s\tremaining: 1.2s\n",
      "769:\tlearn: 0.1703276\ttotal: 3.99s\tremaining: 1.19s\n",
      "770:\tlearn: 0.1701109\ttotal: 4s\tremaining: 1.19s\n",
      "771:\tlearn: 0.1700359\ttotal: 4s\tremaining: 1.18s\n",
      "772:\tlearn: 0.1699062\ttotal: 4s\tremaining: 1.18s\n",
      "773:\tlearn: 0.1698270\ttotal: 4s\tremaining: 1.17s\n",
      "774:\tlearn: 0.1697329\ttotal: 4.01s\tremaining: 1.16s\n",
      "775:\tlearn: 0.1695879\ttotal: 4.01s\tremaining: 1.16s\n",
      "776:\tlearn: 0.1695392\ttotal: 4.01s\tremaining: 1.15s\n",
      "777:\tlearn: 0.1694057\ttotal: 4.02s\tremaining: 1.15s\n",
      "778:\tlearn: 0.1693274\ttotal: 4.02s\tremaining: 1.14s\n",
      "779:\tlearn: 0.1691707\ttotal: 4.03s\tremaining: 1.14s\n",
      "780:\tlearn: 0.1691080\ttotal: 4.03s\tremaining: 1.13s\n",
      "781:\tlearn: 0.1689871\ttotal: 4.04s\tremaining: 1.13s\n",
      "782:\tlearn: 0.1688948\ttotal: 4.05s\tremaining: 1.12s\n",
      "783:\tlearn: 0.1687504\ttotal: 4.05s\tremaining: 1.12s\n",
      "784:\tlearn: 0.1686437\ttotal: 4.05s\tremaining: 1.11s\n",
      "785:\tlearn: 0.1685284\ttotal: 4.06s\tremaining: 1.1s\n",
      "786:\tlearn: 0.1683830\ttotal: 4.06s\tremaining: 1.1s\n",
      "787:\tlearn: 0.1683007\ttotal: 4.06s\tremaining: 1.09s\n",
      "788:\tlearn: 0.1681492\ttotal: 4.07s\tremaining: 1.09s\n",
      "789:\tlearn: 0.1679917\ttotal: 4.08s\tremaining: 1.08s\n",
      "790:\tlearn: 0.1679527\ttotal: 4.08s\tremaining: 1.08s\n",
      "791:\tlearn: 0.1678636\ttotal: 4.09s\tremaining: 1.07s\n",
      "792:\tlearn: 0.1677795\ttotal: 4.09s\tremaining: 1.07s\n",
      "793:\tlearn: 0.1677447\ttotal: 4.09s\tremaining: 1.06s\n",
      "794:\tlearn: 0.1676882\ttotal: 4.09s\tremaining: 1.05s\n",
      "795:\tlearn: 0.1675505\ttotal: 4.1s\tremaining: 1.05s\n",
      "796:\tlearn: 0.1674101\ttotal: 4.1s\tremaining: 1.04s\n",
      "797:\tlearn: 0.1673732\ttotal: 4.1s\tremaining: 1.04s\n",
      "798:\tlearn: 0.1673469\ttotal: 4.11s\tremaining: 1.03s\n",
      "799:\tlearn: 0.1672685\ttotal: 4.11s\tremaining: 1.03s\n",
      "800:\tlearn: 0.1671089\ttotal: 4.11s\tremaining: 1.02s\n",
      "801:\tlearn: 0.1669565\ttotal: 4.11s\tremaining: 1.01s\n",
      "802:\tlearn: 0.1668406\ttotal: 4.12s\tremaining: 1.01s\n",
      "803:\tlearn: 0.1666967\ttotal: 4.12s\tremaining: 1s\n",
      "804:\tlearn: 0.1665316\ttotal: 4.13s\tremaining: 1000ms\n",
      "805:\tlearn: 0.1664222\ttotal: 4.13s\tremaining: 994ms\n",
      "806:\tlearn: 0.1663286\ttotal: 4.13s\tremaining: 988ms\n",
      "807:\tlearn: 0.1661664\ttotal: 4.13s\tremaining: 983ms\n",
      "808:\tlearn: 0.1659959\ttotal: 4.14s\tremaining: 977ms\n",
      "809:\tlearn: 0.1658013\ttotal: 4.14s\tremaining: 971ms\n",
      "810:\tlearn: 0.1656614\ttotal: 4.14s\tremaining: 966ms\n",
      "811:\tlearn: 0.1656223\ttotal: 4.15s\tremaining: 960ms\n",
      "812:\tlearn: 0.1654916\ttotal: 4.15s\tremaining: 954ms\n",
      "813:\tlearn: 0.1654608\ttotal: 4.15s\tremaining: 949ms\n",
      "814:\tlearn: 0.1653617\ttotal: 4.15s\tremaining: 943ms\n",
      "815:\tlearn: 0.1653330\ttotal: 4.16s\tremaining: 937ms\n",
      "816:\tlearn: 0.1651667\ttotal: 4.16s\tremaining: 932ms\n",
      "817:\tlearn: 0.1650980\ttotal: 4.17s\tremaining: 927ms\n",
      "818:\tlearn: 0.1648210\ttotal: 4.17s\tremaining: 921ms\n",
      "819:\tlearn: 0.1646747\ttotal: 4.17s\tremaining: 916ms\n",
      "820:\tlearn: 0.1644825\ttotal: 4.18s\tremaining: 912ms\n",
      "821:\tlearn: 0.1643428\ttotal: 4.2s\tremaining: 909ms\n",
      "822:\tlearn: 0.1642644\ttotal: 4.2s\tremaining: 904ms\n",
      "823:\tlearn: 0.1642070\ttotal: 4.21s\tremaining: 899ms\n",
      "824:\tlearn: 0.1641101\ttotal: 4.21s\tremaining: 894ms\n",
      "825:\tlearn: 0.1640509\ttotal: 4.22s\tremaining: 889ms\n",
      "826:\tlearn: 0.1639940\ttotal: 4.23s\tremaining: 884ms\n",
      "827:\tlearn: 0.1638307\ttotal: 4.23s\tremaining: 879ms\n",
      "828:\tlearn: 0.1637738\ttotal: 4.24s\tremaining: 874ms\n",
      "829:\tlearn: 0.1637126\ttotal: 4.24s\tremaining: 869ms\n",
      "830:\tlearn: 0.1636231\ttotal: 4.25s\tremaining: 864ms\n",
      "831:\tlearn: 0.1634647\ttotal: 4.25s\tremaining: 859ms\n",
      "832:\tlearn: 0.1633475\ttotal: 4.27s\tremaining: 855ms\n",
      "833:\tlearn: 0.1632410\ttotal: 4.28s\tremaining: 851ms\n",
      "834:\tlearn: 0.1631110\ttotal: 4.28s\tremaining: 846ms\n",
      "835:\tlearn: 0.1629366\ttotal: 4.3s\tremaining: 843ms\n",
      "836:\tlearn: 0.1627998\ttotal: 4.3s\tremaining: 839ms\n",
      "837:\tlearn: 0.1626683\ttotal: 4.31s\tremaining: 833ms\n",
      "838:\tlearn: 0.1626286\ttotal: 4.32s\tremaining: 829ms\n",
      "839:\tlearn: 0.1625194\ttotal: 4.33s\tremaining: 824ms\n",
      "840:\tlearn: 0.1624077\ttotal: 4.33s\tremaining: 819ms\n",
      "841:\tlearn: 0.1623564\ttotal: 4.33s\tremaining: 813ms\n",
      "842:\tlearn: 0.1623537\ttotal: 4.34s\tremaining: 808ms\n",
      "843:\tlearn: 0.1621723\ttotal: 4.34s\tremaining: 802ms\n",
      "844:\tlearn: 0.1620572\ttotal: 4.34s\tremaining: 797ms\n",
      "845:\tlearn: 0.1618943\ttotal: 4.34s\tremaining: 791ms\n",
      "846:\tlearn: 0.1617974\ttotal: 4.35s\tremaining: 785ms\n",
      "847:\tlearn: 0.1617281\ttotal: 4.35s\tremaining: 780ms\n",
      "848:\tlearn: 0.1616236\ttotal: 4.35s\tremaining: 774ms\n",
      "849:\tlearn: 0.1615090\ttotal: 4.36s\tremaining: 769ms\n",
      "850:\tlearn: 0.1613527\ttotal: 4.36s\tremaining: 764ms\n",
      "851:\tlearn: 0.1612697\ttotal: 4.37s\tremaining: 758ms\n",
      "852:\tlearn: 0.1612279\ttotal: 4.37s\tremaining: 753ms\n",
      "853:\tlearn: 0.1611439\ttotal: 4.37s\tremaining: 747ms\n",
      "854:\tlearn: 0.1610026\ttotal: 4.37s\tremaining: 742ms\n",
      "855:\tlearn: 0.1609570\ttotal: 4.38s\tremaining: 737ms\n",
      "856:\tlearn: 0.1608989\ttotal: 4.4s\tremaining: 734ms\n",
      "857:\tlearn: 0.1607811\ttotal: 4.4s\tremaining: 728ms\n",
      "858:\tlearn: 0.1606907\ttotal: 4.4s\tremaining: 722ms\n",
      "859:\tlearn: 0.1605744\ttotal: 4.4s\tremaining: 717ms\n",
      "860:\tlearn: 0.1604588\ttotal: 4.41s\tremaining: 711ms\n",
      "861:\tlearn: 0.1603229\ttotal: 4.41s\tremaining: 706ms\n",
      "862:\tlearn: 0.1601496\ttotal: 4.42s\tremaining: 701ms\n",
      "863:\tlearn: 0.1600965\ttotal: 4.42s\tremaining: 696ms\n",
      "864:\tlearn: 0.1600165\ttotal: 4.42s\tremaining: 690ms\n",
      "865:\tlearn: 0.1598685\ttotal: 4.43s\tremaining: 685ms\n",
      "866:\tlearn: 0.1598031\ttotal: 4.43s\tremaining: 680ms\n",
      "867:\tlearn: 0.1596500\ttotal: 4.43s\tremaining: 674ms\n",
      "868:\tlearn: 0.1594465\ttotal: 4.44s\tremaining: 669ms\n",
      "869:\tlearn: 0.1594090\ttotal: 4.44s\tremaining: 664ms\n",
      "870:\tlearn: 0.1592480\ttotal: 4.45s\tremaining: 658ms\n",
      "871:\tlearn: 0.1591769\ttotal: 4.45s\tremaining: 653ms\n",
      "872:\tlearn: 0.1591475\ttotal: 4.45s\tremaining: 647ms\n",
      "873:\tlearn: 0.1591393\ttotal: 4.45s\tremaining: 642ms\n",
      "874:\tlearn: 0.1590821\ttotal: 4.46s\tremaining: 637ms\n",
      "875:\tlearn: 0.1589282\ttotal: 4.46s\tremaining: 631ms\n",
      "876:\tlearn: 0.1587956\ttotal: 4.46s\tremaining: 626ms\n",
      "877:\tlearn: 0.1586448\ttotal: 4.47s\tremaining: 621ms\n",
      "878:\tlearn: 0.1586017\ttotal: 4.47s\tremaining: 615ms\n",
      "879:\tlearn: 0.1583985\ttotal: 4.47s\tremaining: 610ms\n",
      "880:\tlearn: 0.1583060\ttotal: 4.47s\tremaining: 604ms\n",
      "881:\tlearn: 0.1582914\ttotal: 4.48s\tremaining: 599ms\n",
      "882:\tlearn: 0.1582886\ttotal: 4.48s\tremaining: 594ms\n",
      "883:\tlearn: 0.1581437\ttotal: 4.49s\tremaining: 589ms\n",
      "884:\tlearn: 0.1579978\ttotal: 4.49s\tremaining: 584ms\n",
      "885:\tlearn: 0.1578515\ttotal: 4.5s\tremaining: 579ms\n",
      "886:\tlearn: 0.1576945\ttotal: 4.5s\tremaining: 573ms\n",
      "887:\tlearn: 0.1575826\ttotal: 4.5s\tremaining: 568ms\n",
      "888:\tlearn: 0.1575276\ttotal: 4.51s\tremaining: 563ms\n",
      "889:\tlearn: 0.1573991\ttotal: 4.51s\tremaining: 557ms\n",
      "890:\tlearn: 0.1573051\ttotal: 4.51s\tremaining: 552ms\n",
      "891:\tlearn: 0.1572946\ttotal: 4.52s\tremaining: 547ms\n",
      "892:\tlearn: 0.1572707\ttotal: 4.52s\tremaining: 542ms\n",
      "893:\tlearn: 0.1571264\ttotal: 4.53s\tremaining: 537ms\n",
      "894:\tlearn: 0.1570837\ttotal: 4.54s\tremaining: 532ms\n",
      "895:\tlearn: 0.1569735\ttotal: 4.55s\tremaining: 528ms\n",
      "896:\tlearn: 0.1568851\ttotal: 4.57s\tremaining: 525ms\n",
      "897:\tlearn: 0.1567846\ttotal: 4.57s\tremaining: 519ms\n",
      "898:\tlearn: 0.1567146\ttotal: 4.58s\tremaining: 515ms\n",
      "899:\tlearn: 0.1566222\ttotal: 4.59s\tremaining: 510ms\n",
      "900:\tlearn: 0.1564926\ttotal: 4.6s\tremaining: 505ms\n",
      "901:\tlearn: 0.1563539\ttotal: 4.6s\tremaining: 500ms\n",
      "902:\tlearn: 0.1563339\ttotal: 4.61s\tremaining: 495ms\n",
      "903:\tlearn: 0.1562362\ttotal: 4.63s\tremaining: 491ms\n",
      "904:\tlearn: 0.1560455\ttotal: 4.63s\tremaining: 486ms\n",
      "905:\tlearn: 0.1559520\ttotal: 4.63s\tremaining: 481ms\n",
      "906:\tlearn: 0.1558083\ttotal: 4.64s\tremaining: 476ms\n",
      "907:\tlearn: 0.1557343\ttotal: 4.65s\tremaining: 472ms\n",
      "908:\tlearn: 0.1556889\ttotal: 4.66s\tremaining: 467ms\n",
      "909:\tlearn: 0.1555459\ttotal: 4.66s\tremaining: 461ms\n",
      "910:\tlearn: 0.1555126\ttotal: 4.67s\tremaining: 456ms\n",
      "911:\tlearn: 0.1553458\ttotal: 4.67s\tremaining: 451ms\n",
      "912:\tlearn: 0.1552036\ttotal: 4.67s\tremaining: 445ms\n",
      "913:\tlearn: 0.1551262\ttotal: 4.67s\tremaining: 440ms\n",
      "914:\tlearn: 0.1550318\ttotal: 4.68s\tremaining: 434ms\n",
      "915:\tlearn: 0.1549028\ttotal: 4.68s\tremaining: 429ms\n",
      "916:\tlearn: 0.1547593\ttotal: 4.68s\tremaining: 424ms\n",
      "917:\tlearn: 0.1547033\ttotal: 4.68s\tremaining: 418ms\n",
      "918:\tlearn: 0.1546172\ttotal: 4.69s\tremaining: 413ms\n",
      "919:\tlearn: 0.1545705\ttotal: 4.69s\tremaining: 408ms\n",
      "920:\tlearn: 0.1544845\ttotal: 4.69s\tremaining: 402ms\n",
      "921:\tlearn: 0.1544823\ttotal: 4.69s\tremaining: 397ms\n",
      "922:\tlearn: 0.1542327\ttotal: 4.7s\tremaining: 392ms\n",
      "923:\tlearn: 0.1541725\ttotal: 4.7s\tremaining: 387ms\n",
      "924:\tlearn: 0.1540807\ttotal: 4.71s\tremaining: 382ms\n",
      "925:\tlearn: 0.1539366\ttotal: 4.71s\tremaining: 377ms\n",
      "926:\tlearn: 0.1539120\ttotal: 4.72s\tremaining: 371ms\n",
      "927:\tlearn: 0.1537600\ttotal: 4.72s\tremaining: 366ms\n",
      "928:\tlearn: 0.1536128\ttotal: 4.72s\tremaining: 361ms\n",
      "929:\tlearn: 0.1534606\ttotal: 4.72s\tremaining: 356ms\n",
      "930:\tlearn: 0.1533650\ttotal: 4.73s\tremaining: 350ms\n",
      "931:\tlearn: 0.1533023\ttotal: 4.73s\tremaining: 345ms\n",
      "932:\tlearn: 0.1531658\ttotal: 4.73s\tremaining: 340ms\n",
      "933:\tlearn: 0.1530868\ttotal: 4.73s\tremaining: 335ms\n",
      "934:\tlearn: 0.1530654\ttotal: 4.74s\tremaining: 329ms\n",
      "935:\tlearn: 0.1529895\ttotal: 4.74s\tremaining: 324ms\n",
      "936:\tlearn: 0.1529563\ttotal: 4.74s\tremaining: 319ms\n",
      "937:\tlearn: 0.1529336\ttotal: 4.75s\tremaining: 314ms\n",
      "938:\tlearn: 0.1529163\ttotal: 4.75s\tremaining: 308ms\n",
      "939:\tlearn: 0.1528199\ttotal: 4.75s\tremaining: 303ms\n",
      "940:\tlearn: 0.1527870\ttotal: 4.76s\tremaining: 298ms\n",
      "941:\tlearn: 0.1526811\ttotal: 4.76s\tremaining: 293ms\n",
      "942:\tlearn: 0.1525799\ttotal: 4.77s\tremaining: 288ms\n",
      "943:\tlearn: 0.1525272\ttotal: 4.77s\tremaining: 283ms\n",
      "944:\tlearn: 0.1523454\ttotal: 4.77s\tremaining: 278ms\n",
      "945:\tlearn: 0.1522468\ttotal: 4.78s\tremaining: 273ms\n",
      "946:\tlearn: 0.1522042\ttotal: 4.78s\tremaining: 267ms\n",
      "947:\tlearn: 0.1520778\ttotal: 4.78s\tremaining: 262ms\n",
      "948:\tlearn: 0.1518614\ttotal: 4.79s\tremaining: 257ms\n",
      "949:\tlearn: 0.1517528\ttotal: 4.79s\tremaining: 252ms\n",
      "950:\tlearn: 0.1516320\ttotal: 4.79s\tremaining: 247ms\n",
      "951:\tlearn: 0.1515594\ttotal: 4.79s\tremaining: 242ms\n",
      "952:\tlearn: 0.1515102\ttotal: 4.8s\tremaining: 237ms\n",
      "953:\tlearn: 0.1513850\ttotal: 4.8s\tremaining: 232ms\n",
      "954:\tlearn: 0.1513137\ttotal: 4.8s\tremaining: 226ms\n",
      "955:\tlearn: 0.1512107\ttotal: 4.83s\tremaining: 222ms\n",
      "956:\tlearn: 0.1510748\ttotal: 4.83s\tremaining: 217ms\n",
      "957:\tlearn: 0.1510061\ttotal: 4.83s\tremaining: 212ms\n",
      "958:\tlearn: 0.1508513\ttotal: 4.84s\tremaining: 207ms\n",
      "959:\tlearn: 0.1507200\ttotal: 4.84s\tremaining: 202ms\n",
      "960:\tlearn: 0.1505626\ttotal: 4.84s\tremaining: 196ms\n",
      "961:\tlearn: 0.1504415\ttotal: 4.84s\tremaining: 191ms\n",
      "962:\tlearn: 0.1503358\ttotal: 4.85s\tremaining: 186ms\n",
      "963:\tlearn: 0.1501956\ttotal: 4.85s\tremaining: 181ms\n",
      "964:\tlearn: 0.1501594\ttotal: 4.85s\tremaining: 176ms\n",
      "965:\tlearn: 0.1501316\ttotal: 4.85s\tremaining: 171ms\n",
      "966:\tlearn: 0.1500616\ttotal: 4.85s\tremaining: 166ms\n",
      "967:\tlearn: 0.1499383\ttotal: 4.86s\tremaining: 161ms\n",
      "968:\tlearn: 0.1497975\ttotal: 4.86s\tremaining: 156ms\n",
      "969:\tlearn: 0.1497828\ttotal: 4.88s\tremaining: 151ms\n",
      "970:\tlearn: 0.1496985\ttotal: 4.88s\tremaining: 146ms\n",
      "971:\tlearn: 0.1495831\ttotal: 4.88s\tremaining: 141ms\n",
      "972:\tlearn: 0.1494614\ttotal: 4.89s\tremaining: 136ms\n",
      "973:\tlearn: 0.1494451\ttotal: 4.89s\tremaining: 131ms\n",
      "974:\tlearn: 0.1493902\ttotal: 4.89s\tremaining: 126ms\n",
      "975:\tlearn: 0.1492626\ttotal: 4.9s\tremaining: 120ms\n",
      "976:\tlearn: 0.1491627\ttotal: 4.9s\tremaining: 115ms\n",
      "977:\tlearn: 0.1490415\ttotal: 4.9s\tremaining: 110ms\n",
      "978:\tlearn: 0.1489076\ttotal: 4.9s\tremaining: 105ms\n",
      "979:\tlearn: 0.1488004\ttotal: 4.91s\tremaining: 100ms\n",
      "980:\tlearn: 0.1486304\ttotal: 4.91s\tremaining: 95.1ms\n",
      "981:\tlearn: 0.1486282\ttotal: 4.91s\tremaining: 90ms\n",
      "982:\tlearn: 0.1485081\ttotal: 4.91s\tremaining: 85ms\n",
      "983:\tlearn: 0.1483701\ttotal: 4.92s\tremaining: 80ms\n",
      "984:\tlearn: 0.1481243\ttotal: 4.92s\tremaining: 75ms\n",
      "985:\tlearn: 0.1480712\ttotal: 4.92s\tremaining: 69.9ms\n",
      "986:\tlearn: 0.1479632\ttotal: 4.93s\tremaining: 64.9ms\n",
      "987:\tlearn: 0.1478705\ttotal: 4.93s\tremaining: 59.9ms\n",
      "988:\tlearn: 0.1477981\ttotal: 4.93s\tremaining: 54.9ms\n",
      "989:\tlearn: 0.1477717\ttotal: 4.94s\tremaining: 49.9ms\n",
      "990:\tlearn: 0.1476923\ttotal: 4.94s\tremaining: 44.9ms\n",
      "991:\tlearn: 0.1475815\ttotal: 4.94s\tremaining: 39.9ms\n",
      "992:\tlearn: 0.1474535\ttotal: 4.94s\tremaining: 34.9ms\n",
      "993:\tlearn: 0.1474362\ttotal: 4.95s\tremaining: 29.9ms\n",
      "994:\tlearn: 0.1473894\ttotal: 4.95s\tremaining: 24.9ms\n",
      "995:\tlearn: 0.1472534\ttotal: 4.95s\tremaining: 19.9ms\n",
      "996:\tlearn: 0.1472079\ttotal: 4.95s\tremaining: 14.9ms\n",
      "997:\tlearn: 0.1470634\ttotal: 4.96s\tremaining: 9.94ms\n",
      "998:\tlearn: 0.1469845\ttotal: 4.96s\tremaining: 4.96ms\n",
      "999:\tlearn: 0.1469473\ttotal: 4.96s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x286e32f4550>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "\n",
    "cb_model = CatBoostClassifier()\n",
    "\n",
    "catboost_train_data = Pool(x_train, label = y_train)\n",
    "\n",
    "cb_model.fit(catboost_train_data, plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13f750",
   "metadata": {},
   "source": [
    "This outputs in real time the value of the metrics (by default, the loss function), the\n",
    "total runtime, and time remaining. We can also visualize the metrics by providing\n",
    "the argument plot=True to fit. Once completed, we can use other methods, such as\n",
    "score, to get the accuracy: cb_model.score(catboost_train_data). We can also use\n",
    "predict and predict_proba on new data with only features (no targets) like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "28ba0c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0], dtype=int64)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catboost_test_data = Pool(x_test)\n",
    "\n",
    "cb_model.predict(catboost_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becf11ed",
   "metadata": {},
   "source": [
    "First, we create a dataset with the catboost Pool class, giving it only our features.\n",
    "Then we simply use the predict method of our model on this data. There are many\n",
    "other methods of catboost models described in the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa497ca",
   "metadata": {},
   "source": [
    "## Using early stopping with boosting algorithms\n",
    "\n",
    "When training the XGBoost, LightGBM, and CatBoost models, we can use a practice\n",
    "called `early stopping` to prevent overfitting. This method is used when fitting\n",
    "the model. We provide a training dataset and a validation set, and our metrics\n",
    "are calculated on both datasets after each new tree is added to the model. If the\n",
    "validation metric does not improve for a specified number of rounds (new trees\n",
    "added), then the model training is stopped and the model iteration with the best\n",
    "validation score is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6b05d696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd51824e1c84cf3b3fd72f9d143d830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x286e3302a00>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import catboost\n",
    "\n",
    "\n",
    "new_cb = catboost.CatBoostClassifier(**best_cb.get_params())\n",
    "\n",
    "new_cb.set_params(n_estimators=1000)\n",
    "\n",
    "new_cb.fit(X=x_train,\n",
    "            y=y_train,\n",
    "            eval_set=(x_test, y_test),\n",
    "            early_stopping_rounds=10,\n",
    "            plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a325938d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_cb.tree_count_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2a8da4",
   "metadata": {},
   "source": [
    "We are using our parameters from the best CatBoost model we found from model\n",
    "tuning using pycaret, and the ** before the dictionary is dictionary unpacking.\n",
    "This expands each dictionary item as an argument to the function or class, so if our\n",
    "dictionary has an element best_cb.get_params()['n_estimators'] = 100, (or {'n_\n",
    "estimators': 10}) then this would be the same as providing n_estimators=100 in\n",
    "the CatBoostClassifier class. We then increase the number of estimators to a large\n",
    "value so we can see early stopping work. Finally, we fit the model to our train set\n",
    "and use the test set as the evaluation set. The plot=True argument shows the train\n",
    "and validation scores as it trains. We can see it stops early by examining the number\n",
    "of trees with new_cb.tree_count_, which returns 93.\n",
    "\n",
    "\n",
    "Although early stopping can be used to prevent overfitting, we can also use things\n",
    "like regularization (l2_leaf_reg for catboost, for example) and cross-validation to\n",
    "prevent and check for overfitting. In fact, using tune_model from pycaret provides a\n",
    "way to avoid overfitting, since we are optimizing the hyperparameters by using the\n",
    "metric score on a validation set."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ff07ffec6be8e65654415574640fa0404e3cc7993467f0a0ce9889968ad1102a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('practical_data_science_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
