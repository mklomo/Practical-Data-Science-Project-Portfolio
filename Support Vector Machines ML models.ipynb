{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM) Machine Learning Models\n",
    "\n",
    "\n",
    "Decision tree-based models tend to perform well for many problems. However, depending on our problem, other algorithms may work better. One widely used machine learning algorithm is the support vector\n",
    "machine (SVM).\n",
    "\n",
    "Like linear and logistic regression, SVMs have been around for a\n",
    "while – since 1963. SVMs can be used for regression and classification, sometimes called support vector regressors (SVRs) and support vector classifiers (SVCs).\n",
    "\n",
    "Although SVMs have been around for a while and have become less popular with\n",
    "the rise of other ML algorithms, it's still worth trying SVMs as one of your ML\n",
    "algorithms for supervised learning problems.\n",
    "\n",
    "SVMs have some advantages:\n",
    "- They work well with a high number of dimensions (many features)\n",
    "- They are memory-efficient, since they only use a subset of datapoints to classify new ones\n",
    "- Using kernels to transform data can make them more flexible for higherdimension and complex feature spaces\n",
    "\n",
    "However, there are some disadvantages:\n",
    "- Vanilla SVM implementations do not scale well with increasing features and datapoints (although there are implementations for big data with Spark and other software)\n",
    "- Probability estimates for class predictions need to be found with crossvalidation, which is computationally expensive\n",
    "\n",
    "\n",
    "There are four common kernels we'll see:\n",
    "- Linear – the first example we saw\n",
    "- Polynomial – can work for slightly more complex data\n",
    "- Radial basis function (RBF) – works for very complex data\n",
    "- Sigmoid – can work for complex data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM for regression\n",
    "\n",
    "SVMs work a little differently for regression and are called SVRs. Instead of trying to maximize the margin between the hyperplane and points of different classes, SVRs in essence fit a hyperplane to the data. This is similar to how linear regression works, although we are optimizing a different function with SVRs. Essentially, we try to minimize the difference between predictions of datapoints from the hyperplane and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with sklearn\n",
    "\n",
    "The sklearn package has a few different SVC and SVR implementations:\n",
    "- Linear SVMs (svm.LinearSVC, LinearSVR, linear_model.SGDClassifier, and SGDRegressor)\n",
    "- General SVMs (svm.SVC and SVR)\n",
    "- Nu SVMs (svm.NuSVC and NuSVR)\n",
    "\n",
    "The linear SVM can be implemented with svm.LinearSVC and svm.SVC, although the\n",
    "LinearSVC implementation is better (because it scales better to large datasets and has more flexibility, as described in the documentation: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html). The SVC implementation allows any kernel to be used, and has pre-made options for using different kernels: polynomial (poly), RBF (rbf), and sigmoid (sigmoid).\n",
    "\n",
    "The Nu SVMs introduce a hyperparameter, nu, which is an upper bound to the\n",
    "number of misclassified points for classification, and a lower bound to the number of support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using these is the same as any other sklearn supervised learning algorithm – create\n",
    "the model (with any chosen hyperparameters), train it, then use and evaluate it. First,\n",
    "let's load the credit card default data we've used previously and create train and test\n",
    "sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_excel(r'data\\sample - default of credit card clients.xls', skiprows=1, index_col='ID')\n",
    "\n",
    "target_column = 'default payment next month'\n",
    "\n",
    "features = df.drop(target_column, axis = 1)\n",
    "\n",
    "targets = df[target_column]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(features, targets, stratify=targets, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_train_x = scaler.fit_transform(train_x)\n",
    "scaled_test_x = scaler.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are loading the data and breaking it into train and test sets,using stratify in train_test_split to make sure the balance of binary targets stays the same between the train and test sets. We also prepared some scaled features. `For SVMs, there are a few caveats – one is that it is helpful to scale the data.`\n",
    "\n",
    "This is due to how the math behind SVMs works (which, again, is complex, and will\n",
    "require deeper/further study to fully understand). `One other caveat is that getting probability predictions is not built in, and is not available with LinearSVC.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look first at using LinearSVC with the default hyperparameters. Almost all of\n",
    "the configuration parameters have to do with the iterative solver for the function,\n",
    "such as `max_iter` (for the maximum number of iterations) and `tol` (which controls\n",
    "when the iteration has stopped, if the optimization has not improved enough after\n",
    "an iteration). The main hyperparameter to tune here is `C`, which is a regularization\n",
    "coefficient. Higher values of C mean less regulation (less prevention of overfitting),\n",
    "or a smaller-margin hyperplane (less separation between points and the hyperplane).\n",
    "A good visual explanation of this can be found here: https://stats.stackexchange.com/a/159051/120921."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7968\n",
      "0.7877333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INNO\\Anaconda3\\envs\\practical_data_science_env\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lsvc = LinearSVC()\n",
    "lsvc.fit(scaled_train_x, train_y)\n",
    "print(lsvc.score(scaled_train_x, train_y))\n",
    "print(lsvc.score(scaled_test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same pattern we followed from other sklearn models. Our accuracy here\n",
    "is 78.7% on the test set, slightly better than the no information rate of 78.3% (from\n",
    "`targets.value_counts(normalize=True)`). If we instead use the non-scaled data, our\n",
    "accuracy is much lower (around 50%).\n",
    "\n",
    "We can also implement a linear SVC with a few other methods – using the `linear_model.SGDClassifier` with the default value of `loss='hinge'`, or using the SVC or\n",
    "NuSVC models with `kernel='linear'`. SGD stands for stochastic gradient descent,\n",
    "since it is using gradient descent to optimize the model (the hyperplane for our SVC).\n",
    "With the SGD model, we do not have the C hyperparameter, though we can fine-tune\n",
    "the gradient descent and L1 and L2 losses used with the model in more detail ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have an alpha parameter for the `L1` and `L2` regularization that penalizesbigger values in the w vector from our hyperplane equation.\n",
    "\n",
    "While C penalizes misclassified points, alpha penalizes bigger coefficients in w. Both have the same effect of increasing the margin (the distance from the hyperplane to the nearest points) with bigger penalties (small C or bigger alpha values). \n",
    "\n",
    "For the SVC and NuSVC models, a different algorithm is used for the linear SVC that does not scale as well with bigger data. However, we can get probability estimates for predictions by setting the probability=True parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8272\n",
      "0.8181333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(probability=True)\n",
    "svc.fit(scaled_train_x, train_y)\n",
    "print(svc.score(scaled_train_x, train_y))\n",
    "print(svc.score(scaled_test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using probability=True, we can then use the predict_proba method of the\n",
    "model (such as svc.predict_proba(scaled_test_x)) to predict probabilities of\n",
    "classes. However, these probabilities are estimated from a cross-validation method,\n",
    "so may not always agree with the actual predictions. Interestingly, the different\n",
    "solver used with the SVC model (libsvm instead of liblinear with LinearSVC)\n",
    "seems to perform slightly better than the LinearSVC model here, with a test accuracy\n",
    "of 81.8%.\n",
    "\n",
    "We could also try using the SGDClassifier and the NuSVM models for\n",
    "comparison. The NuSVM model works in almost the same way as the SVC model,\n",
    "although it has the nu hyperparameter, which should be greater than 0 and less\n",
    "than or equal to 1. The nu hyperparameter determines the maximum fraction of\n",
    "misclassified points and the minimum fraction of support vectors.\n",
    "\n",
    "\n",
    "To use SVMs for regression in sklearn, the process is the same, although we\n",
    "don't have the probability or predict_proba options available and use the SVRbased\n",
    "classes (or SGDRegressor) instead of the SVC versions. We also don't have\n",
    "one hyperparameter that we had for classification, class_weight, which, if set to\n",
    "balanced, inversely weights points occurring to their class frequency. We do have\n",
    "another hyperparameter with SVR, which is epsilon. This determines a distance\n",
    "from the hyperplane where wrong predictions are not penalized. A bigger value\n",
    "of epsilon means the model will have more bias (less overfitting), while smaller\n",
    "epsilon makes the model fit the data more exactly (more variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning SVMs with PyCaret\n",
    "\n",
    "As with other models, we can easily tune them using pycaret. By default, pycaret\n",
    "uses the SGDClassifier or regressor for a linear SVM and uses SVC or SVR for an RBF\n",
    "kernel SVM. But we can use pycaret to tune any sklearn model; so, if want to tune\n",
    "the LinearSVC model we already tried, we can do it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_cef7d_row10_col0, #T_cef7d_row10_col1, #T_cef7d_row10_col2, #T_cef7d_row10_col3, #T_cef7d_row10_col4, #T_cef7d_row10_col5, #T_cef7d_row10_col6 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_cef7d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_cef7d_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_cef7d_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_cef7d_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_cef7d_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_cef7d_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_cef7d_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_cef7d_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_cef7d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_cef7d_row0_col0\" class=\"data row0 col0\" >0.8171</td>\n",
       "      <td id=\"T_cef7d_row0_col1\" class=\"data row0 col1\" >0.7178</td>\n",
       "      <td id=\"T_cef7d_row0_col2\" class=\"data row0 col2\" >0.3036</td>\n",
       "      <td id=\"T_cef7d_row0_col3\" class=\"data row0 col3\" >0.6538</td>\n",
       "      <td id=\"T_cef7d_row0_col4\" class=\"data row0 col4\" >0.4146</td>\n",
       "      <td id=\"T_cef7d_row0_col5\" class=\"data row0 col5\" >0.3231</td>\n",
       "      <td id=\"T_cef7d_row0_col6\" class=\"data row0 col6\" >0.3565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cef7d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_cef7d_row1_col0\" class=\"data row1 col0\" >0.8076</td>\n",
       "      <td id=\"T_cef7d_row1_col1\" class=\"data row1 col1\" >0.6615</td>\n",
       "      <td id=\"T_cef7d_row1_col2\" class=\"data row1 col2\" >0.3009</td>\n",
       "      <td id=\"T_cef7d_row1_col3\" class=\"data row1 col3\" >0.6071</td>\n",
       "      <td id=\"T_cef7d_row1_col4\" class=\"data row1 col4\" >0.4024</td>\n",
       "      <td id=\"T_cef7d_row1_col5\" class=\"data row1 col5\" >0.3029</td>\n",
       "      <td id=\"T_cef7d_row1_col6\" class=\"data row1 col6\" >0.3295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cef7d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_cef7d_row2_col0\" class=\"data row2 col0\" >0.8210</td>\n",
       "      <td id=\"T_cef7d_row2_col1\" class=\"data row2 col1\" >0.6830</td>\n",
       "      <td id=\"T_cef7d_row2_col2\" class=\"data row2 col2\" >0.3451</td>\n",
       "      <td id=\"T_cef7d_row2_col3\" class=\"data row2 col3\" >0.6610</td>\n",
       "      <td id=\"T_cef7d_row2_col4\" class=\"data row2 col4\" >0.4535</td>\n",
       "      <td id=\"T_cef7d_row2_col5\" class=\"data row2 col5\" >0.3588</td>\n",
       "      <td id=\"T_cef7d_row2_col6\" class=\"data row2 col6\" >0.3859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cef7d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_cef7d_row3_col0\" class=\"data row3 col0\" >0.8210</td>\n",
       "      <td id=\"T_cef7d_row3_col1\" class=\"data row3 col1\" >0.7132</td>\n",
       "      <td id=\"T_cef7d_row3_col2\" class=\"data row3 col2\" >0.2743</td>\n",
       "      <td id=\"T_cef7d_row3_col3\" class=\"data row3 col3\" >0.7209</td>\n",
       "      <td id=\"T_cef7d_row3_col4\" class=\"data row3 col4\" >0.3974</td>\n",
       "      <td id=\"T_cef7d_row3_col5\" class=\"data row3 col5\" >0.3163</td>\n",
       "      <td id=\"T_cef7d_row3_col6\" class=\"data row3 col6\" >0.3675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cef7d_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_cef7d_row4_col0\" class=\"data row4 col0\" >0.8190</td>\n",
       "      <td id=\"T_cef7d_row4_col1\" class=\"data row4 col1\" >0.6918</td>\n",
       "      <td id=\"T_cef7d_row4_col2\" class=\"data row4 col2\" >0.2920</td>\n",
       "      <td id=\"T_cef7d_row4_col3\" class=\"data row4 col3\" >0.6875</td>\n",
       "      <td id=\"T_cef7d_row4_col4\" class=\"data row4 col4\" >0.4099</td>\n",
       "      <td id=\"T_cef7d_row4_col5\" class=\"data row4 col5\" >0.3231</td>\n",
       "      <td id=\"T_cef7d_row4_col6\" class=\"data row4 col6\" >0.3645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cef7d_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_cef7d_row5_col0\" class=\"data row5 col0\" >0.8419</td>\n",
       "      <td id=\"T_cef7d_row5_col1\" class=\"data row5 col1\" >0.7029</td>\n",
       "      <td id=\"T_cef7d_row5_col2\" class=\"data row5 col2\" >0.3363</td>\n",
       "      <td id=\"T_cef7d_row5_col3\" class=\"data row5 col3\" >0.8261</td>\n",
       "      <td id=\"T_cef7d_row5_col4\" class=\"data row5 col4\" >0.4780</td>\n",
       "      <td id=\"T_cef7d_row5_col5\" class=\"data row5 col5\" >0.4037</td>\n",
       "      <td id=\"T_cef7d_row5_col6\" class=\"data row5 col6\" >0.4606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cef7d_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_cef7d_row6_col0\" class=\"data row6 col0\" >0.8076</td>\n",
       "      <td id=\"T_cef7d_row6_col1\" class=\"data row6 col1\" >0.7245</td>\n",
       "      <td id=\"T_cef7d_row6_col2\" class=\"data row6 col2\" >0.2212</td>\n",
       "      <td id=\"T_cef7d_row6_col3\" class=\"data row6 col3\" >0.6579</td>\n",
       "      <td id=\"T_cef7d_row6_col4\" class=\"data row6 col4\" >0.3311</td>\n",
       "      <td id=\"T_cef7d_row6_col5\" class=\"data row6 col5\" >0.2499</td>\n",
       "      <td id=\"T_cef7d_row6_col6\" class=\"data row6 col6\" >0.3009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cef7d_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_cef7d_row7_col0\" class=\"data row7 col0\" >0.8305</td>\n",
       "      <td id=\"T_cef7d_row7_col1\" class=\"data row7 col1\" >0.7437</td>\n",
       "      <td id=\"T_cef7d_row7_col2\" class=\"data row7 col2\" >0.3717</td>\n",
       "      <td id=\"T_cef7d_row7_col3\" class=\"data row7 col3\" >0.7000</td>\n",
       "      <td id=\"T_cef7d_row7_col4\" class=\"data row7 col4\" >0.4855</td>\n",
       "      <td id=\"T_cef7d_row7_col5\" class=\"data row7 col5\" >0.3953</td>\n",
       "      <td id=\"T_cef7d_row7_col6\" class=\"data row7 col6\" >0.4237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cef7d_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_cef7d_row8_col0\" class=\"data row8 col0\" >0.8248</td>\n",
       "      <td id=\"T_cef7d_row8_col1\" class=\"data row8 col1\" >0.7328</td>\n",
       "      <td id=\"T_cef7d_row8_col2\" class=\"data row8 col2\" >0.3274</td>\n",
       "      <td id=\"T_cef7d_row8_col3\" class=\"data row8 col3\" >0.6981</td>\n",
       "      <td id=\"T_cef7d_row8_col4\" class=\"data row8 col4\" >0.4458</td>\n",
       "      <td id=\"T_cef7d_row8_col5\" class=\"data row8 col5\" >0.3575</td>\n",
       "      <td id=\"T_cef7d_row8_col6\" class=\"data row8 col6\" >0.3937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cef7d_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_cef7d_row9_col0\" class=\"data row9 col0\" >0.8340</td>\n",
       "      <td id=\"T_cef7d_row9_col1\" class=\"data row9 col1\" >0.7536</td>\n",
       "      <td id=\"T_cef7d_row9_col2\" class=\"data row9 col2\" >0.3750</td>\n",
       "      <td id=\"T_cef7d_row9_col3\" class=\"data row9 col3\" >0.7119</td>\n",
       "      <td id=\"T_cef7d_row9_col4\" class=\"data row9 col4\" >0.4912</td>\n",
       "      <td id=\"T_cef7d_row9_col5\" class=\"data row9 col5\" >0.4032</td>\n",
       "      <td id=\"T_cef7d_row9_col6\" class=\"data row9 col6\" >0.4328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cef7d_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_cef7d_row10_col0\" class=\"data row10 col0\" >0.8224</td>\n",
       "      <td id=\"T_cef7d_row10_col1\" class=\"data row10 col1\" >0.7125</td>\n",
       "      <td id=\"T_cef7d_row10_col2\" class=\"data row10 col2\" >0.3148</td>\n",
       "      <td id=\"T_cef7d_row10_col3\" class=\"data row10 col3\" >0.6924</td>\n",
       "      <td id=\"T_cef7d_row10_col4\" class=\"data row10 col4\" >0.4310</td>\n",
       "      <td id=\"T_cef7d_row10_col5\" class=\"data row10 col5\" >0.3434</td>\n",
       "      <td id=\"T_cef7d_row10_col6\" class=\"data row10 col6\" >0.3816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cef7d_level0_row11\" class=\"row_heading level0 row11\" >SD</th>\n",
       "      <td id=\"T_cef7d_row11_col0\" class=\"data row11 col0\" >0.0103</td>\n",
       "      <td id=\"T_cef7d_row11_col1\" class=\"data row11 col1\" >0.0269</td>\n",
       "      <td id=\"T_cef7d_row11_col2\" class=\"data row11 col2\" >0.0443</td>\n",
       "      <td id=\"T_cef7d_row11_col3\" class=\"data row11 col3\" >0.0549</td>\n",
       "      <td id=\"T_cef7d_row11_col4\" class=\"data row11 col4\" >0.0471</td>\n",
       "      <td id=\"T_cef7d_row11_col5\" class=\"data row11 col5\" >0.0472</td>\n",
       "      <td id=\"T_cef7d_row11_col6\" class=\"data row11 col6\" >0.0460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2088db13370>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pycaret.distributions import UniformDistribution\n",
    "from pycaret.classification import setup, create_model, tune_model\n",
    "\n",
    "\n",
    "clf_setup = setup(data=df,\n",
    "                 target='default payment next month',\n",
    "                 normalize=True)\n",
    "\n",
    "\n",
    "rbfsvmc = create_model('rbfsvm')\n",
    "\n",
    "tuned_lscv = tune_model(rbfsvmc, search_library='scikit-optimize', custom_grid={\"C\": UniformDistribution(0, 500)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the necessary functions from the pycaret.classification module,\n",
    "then set up our pycaret space. We leave the defaults for the detected numeric\n",
    "or categorical columns and normalize our features (this normalization uses\n",
    "standardization by default). Then we create our SVM - Radial Kernel Classifier model and tune it with Bayesian search in the range 0 to 50. The result is a C value of around 2 with an accuracy of about 81% on the 10-fold cross-validation.\n",
    "\n",
    "\n",
    "The optimal model here is 82.24%. rbfsvm, which uses sklearn.svm.SVC with kernel='rbf', searches the C hyperparameter from 0 to 50, and tries using class_weight='balanced' and None.\n",
    "\n",
    "\n",
    "With 'balanced', this will set weights of classes as inversely proportional to their prevalence. rbfsvm takes much longer to run than LinearSVC or SGDClassifier and ends up with an accuracy of 81.6%. In this case, it looks like the best SVC model to use would be SGDClassifier (the svm model from pycaret), since it has similar accuracy to the other models but runs the fastest.\n",
    "\n",
    "\n",
    "The RBF kernel does have one more hyperparameter, which is gamma. This is set\n",
    "to auto with pycaret, although we could try tuning it as well. The gamma value\n",
    "affects how the data is transformed into a new dimension, with larger values of\n",
    "gamma tending to cause overfitting and smaller values causing underfitting. A very small value of gamma ends up being like a linear SVM. More details on the RBF hyperparameters are provided in the sklearn documentation: https://scikitlearn.org/stable/auto_examples/svm/plot_rbf_parameters.html.\n",
    "\n",
    "Using pycaret to optimize SVMs for regression is similar to classification, except we\n",
    "use the pycaret.regression module instead. However, the regression models do not\n",
    "have the class_weight hyperparameter, but do have an epsilon hyperparameter.\n",
    "This epsilon hyperparameter determines a distance from the hyperplane where\n",
    "wrong predictions are not penalized, and pycaret searches a space of 1-2 for this.\n",
    "There is also not a linear SVM available for SVR by default, although we can create\n",
    "our own LinearSVR model and use that."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ff07ffec6be8e65654415574640fa0404e3cc7993467f0a0ce9889968ad1102a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
