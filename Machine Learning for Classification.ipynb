{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e70ab9",
   "metadata": {},
   "source": [
    "# Machine Learning for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19846d8",
   "metadata": {},
   "source": [
    "## Machine Learning Classification Algorithms\n",
    "\n",
    "\n",
    "There are many machine learning algorithms, and new algorithms are being created\n",
    "all the time. Machine learning algorithms take input data and learn, fit, or train\n",
    "during a training phase. Then we use the statistical patterns learned from the data to\n",
    "make predictions during what is called \"inference.\"\n",
    "- Logistic Regression\n",
    "- Naiver Bayes\n",
    "- k-nearest neighbors (KNN)\n",
    "\n",
    "The idea with these algorithms is that we give them labeled training data. This\n",
    "means that we have our features (inputs) and a target or label (output). The target\n",
    "should be a class, which could be binary (1 or 0) or multiclass (0 through the number\n",
    "of classes). The numbers 0 and 1 (and others for multiclass classification) for the\n",
    "target correspond to our different classes.\n",
    "\n",
    "For binary classification, this can be\n",
    "something like a payment default, approval to take a loan, whether someone will\n",
    "click an ad online, or whether someone has a disease. For multiclass classification,\n",
    "this might be something like a budget category for payments from your bank\n",
    "account, the breed of a dog, or the emotion classification from a social media post\n",
    "(for example, sad, happy, angry, or scared). Our inputs, or features, should have\n",
    "some relationship with the target. For example, we might be interested in someone's\n",
    "annual income and length of tenure at their current job if we are predicting if they\n",
    "will default on a loan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4deb9bb",
   "metadata": {},
   "source": [
    "## Logistic Regression for Binary Classification\n",
    "\n",
    "\n",
    "Logistic regression has been around for a while – since 1958. But don't let its\n",
    "age fool you; sometimes the simplest algorithms can outperform more complex\n",
    "algorithms, such as neural networks. Logistic regression is primarily used for binary\n",
    "classification, although it can be used for multi-class classification as well. It's part of\n",
    "a group of models called generalized linear models (GLMs), that also includes linear\n",
    "regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a151e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "ID                                                                         \n",
       "1       20000    2          2         1   24      2      2     -1     -1   \n",
       "2      120000    2          2         2   26     -1      2      0      0   \n",
       "3       90000    2          2         2   34      0      0      0      0   \n",
       "4       50000    2          2         1   37      0      0      0      0   \n",
       "5       50000    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "    PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "ID         ...                                                                  \n",
       "1      -2  ...          0          0          0         0       689         0   \n",
       "2       0  ...       3272       3455       3261         0      1000      1000   \n",
       "3       0  ...      14331      14948      15549      1518      1500      1000   \n",
       "4       0  ...      28314      28959      29547      2000      2019      1200   \n",
       "5       0  ...      20940      19146      19131      2000     36681     10000   \n",
       "\n",
       "    PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n",
       "ID                                                            \n",
       "1          0         0         0                           1  \n",
       "2       1000         0      2000                           1  \n",
       "3       1000      1000      5000                           0  \n",
       "4       1100      1069      1000                           0  \n",
       "5       9000       689       679                           0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "data_path = r\"C:\\Users\\INNO\\Documents\\Python Development\\Practical Data Science\\data\\default of credit card clients.xls\"\n",
    "\n",
    "credit_df = pd.read_excel(data_path,\n",
    "                         skiprows=1,\n",
    "                         index_col=0 \n",
    "                         )\n",
    "\n",
    "\n",
    "# Preview the DF\n",
    "credit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aae885",
   "metadata": {},
   "source": [
    "The Kaggle data page for\n",
    "the dataset provides descriptions of the different columns in the dataset. There are\n",
    "features for 6 months of data from April to August in 2005. The PAY columns (like\n",
    "PAY_0) contain data on whether the payment was late for that month. The PAY_0\n",
    "column is for payment for August 2005, the latest month in the dataset. Other\n",
    "columns, such as BILL_AMT and PAY_AMT, contain the amount of the bill and\n",
    "payment for the 6 months in the dataset. Other columns should be self-explanatory\n",
    "from the column title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "834ce4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9704512e85247dbbedff565aa3d731b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bea1b6e1fca4b6f8557541c47d15484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b6fcd0de49406ebb1dd2aa14efde54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfcab72399444dcea7eb617f4124b905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Running an Auto EDA with ProfileReport\n",
    "\n",
    "report = ProfileReport(credit_df, interactions=None)\n",
    "\n",
    "report.to_file(\"data\\credit_data_Eda.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cbf990",
   "metadata": {},
   "source": [
    "Our target variable that we will be predicting is the \"default payment next month\"\n",
    "column, and all other columns will be features. We can see from looking at the\n",
    "correlations (for example, with df.corr().loc['default payment next month'])\n",
    "that some of the features have a relationship to the target – the Pearson correlation is\n",
    "around 0.1-0.3 for a handful of features, and the PAY features have a phik correlation\n",
    "around 0.5 with the target. We won't do any data cleaning, feature selection, or\n",
    "feature engineering yet. We can see from df.info() that there aren't any missing\n",
    "values. Both the sklearn and statsmodels implementations of logistic regression\n",
    "cannot handle missing values, so missing values need to be dealt with before using\n",
    "the model.\n",
    "\n",
    "\n",
    "To fit the logistic regression model, we need a set of training data broken down into\n",
    "features and targets. We can create our features and targets like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de2f979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = credit_df.drop(\"default payment next month\", axis = 1)\n",
    "\n",
    "train_targets = credit_df[\"default payment next month\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1476002",
   "metadata": {},
   "source": [
    "The first line drops the target column, keeping all other columns as features, and the\n",
    "second column only keeps the target column in the train_targets variable. Let's use\n",
    "the sklearn logistic regression implementation first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55c32ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, random_state=42)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Max Iter solves the warning problem\n",
    "lr_sklearn = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "lr_sklearn.fit(train_features, train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc71f001",
   "metadata": {},
   "source": [
    "All sklearn models work similarly. We first import the model class\n",
    "(LogisticRegression here), and then instantiate it (the second line of code here).\n",
    "When we create the model object, we can provide arguments to the class. For many\n",
    "models, there is a random_state argument, which sets the random seed for random\n",
    "processes. This will make our results reproducible if there are any random processes\n",
    "in the algorithm. We have other arguments we can set, although we aren't doing\n",
    "that yet. Once we have our initialized model object, we can train it on our data using\n",
    "the fit() method. This is where the \"machine learning\" part comes in – the model\n",
    "(machine) is learning from the data we give it. You will see that there is a warning\n",
    "emitted when we run this code, talking about iterations reaching their limit. We will\n",
    "explain more about what this means soon.\n",
    "\n",
    "Once the model has fit to our training data, we can evaluate its performance on the\n",
    "same data easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95a35d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7787333333333334"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_sklearn.score(train_features, train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e17470",
   "metadata": {},
   "source": [
    "The score() method of trained models calculates a default scoring metric. For\n",
    "classification algorithms such as logistic regression, this is usually accuracy, which\n",
    "is the number of correct predictions divided by the total number of data points. The\n",
    "score() method returns 0.7788 here, so our accuracy is around 78%. `To understand\n",
    "if that's any good, it helps to compare it to the majority class fraction, which we can\n",
    "find from train_targets.value_counts(normalize=True). This shows us that the\n",
    "class of 0 is 0.7788, so our model is doing no better than guessing that everything\n",
    "is a \"no default\" label.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7397c5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.7788\n",
       "1    0.2212\n",
       "Name: default payment next month, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize value_counts\n",
    "\n",
    "train_targets.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6550e82",
   "metadata": {},
   "source": [
    "### Getting Predictions from our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7a85819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = lr_sklearn.predict(train_features)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a965f8",
   "metadata": {},
   "source": [
    "The data we give the predict() method must be a 2D array, so if we only predict\n",
    "a single data point, we reshape it to be a 2D array with a single row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64a1f995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INNO\\Anaconda3\\envs\\practical_data_science_env\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_sklearn.predict((train_features.iloc[-1].values.reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55538d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INNO\\Anaconda3\\envs\\practical_data_science_env\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.71715546, 0.28284454]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict probability for the last row\n",
    "\n",
    "lr_sklearn.predict_proba((train_features.iloc[-1].values.reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de83be5",
   "metadata": {},
   "source": [
    "The predict function returns a NumPy array of values for class labels. For binary\n",
    "classification, this consists of 0s and 1s. If there are more than two classes, we will\n",
    "have 0 through n-1 for the number of classes.\n",
    "\n",
    "Instead of getting the exact class predictions, we can get probabilities for all the\n",
    "predictions with predict_proba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1613a3b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62009029, 0.37990971],\n",
       "       [0.69758665, 0.30241335],\n",
       "       [0.733449  , 0.266551  ],\n",
       "       ...,\n",
       "       [0.69538508, 0.30461492],\n",
       "       [0.9559074 , 0.0440926 ],\n",
       "       [0.71715546, 0.28284454]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_sklearn.predict_proba(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74041397",
   "metadata": {},
   "source": [
    "The first row shows the probability of class 0 is 0.54 (54%), and for class 1 is 0.46.\n",
    "These probabilities sum to one for each row. We can use the probabilities to choose\n",
    "a threshold for rounding our predictions. For example, the default threshold is >=0.5.\n",
    "In other words, we are selecting the probabilities that each data point is 1 (the second\n",
    "column), and then rounding it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41e8f9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_predictions = lr_sklearn.predict_proba(train_features)[ :, 1]\n",
    "\n",
    "proba_predictions = (proba_predictions >= 0.5).astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7739f651",
   "metadata": {},
   "source": [
    "The proba_predictions >= 0.5 statement creates a Boolean array, so we convert\n",
    "it back to an integer with astype('int'). Then we can check if this is equal to the\n",
    "predict method with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "701b9097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = lr_sklearn.predict(train_features)\n",
    "\n",
    "np.array_equal(predictions, np.round(proba_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e65ff",
   "metadata": {},
   "source": [
    "### Examining Feature Importances\n",
    "\n",
    "We can roughly estimate the importance of features by comparing the absolute value\n",
    "of the coefficients we find. We do need to scale our inputs before fitting the data with\n",
    "our model (if they are in different units), for example, with StandardScaler. A bigger\n",
    "coefficient means the feature is more \"important\" because it more strongly affects\n",
    "the prediction. It's important to scale the features prior to doing this so they are\n",
    "comparable (unless the features are all in the same units). We can scale our features,\n",
    "fit the logistic regression model, and plot the feature importances like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8be814a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x1bc3f8308b0>,\n",
       "  <matplotlib.axis.XTick at 0x1bc3f830850>,\n",
       "  <matplotlib.axis.XTick at 0x1bc3aaf9a60>,\n",
       "  <matplotlib.axis.XTick at 0x1bc3f888220>,\n",
       "  <matplotlib.axis.XTick at 0x1bc3f888970>,\n",
       "  <matplotlib.axis.XTick at 0x1bc3f88f100>,\n",
       "  <matplotlib.axis.XTick at 0x1bc3f88f850>,\n",
       "  <matplotlib.axis.XTick at 0x1bc3f88fc40>,\n",
       "  <matplotlib.axis.XTick at 0x1bc3f888c10>,\n",
       "  <matplotlib.axis.XTick at 0x1bc41110220>,\n",
       "  <matplotlib.axis.XTick at 0x1bc41110970>,\n",
       "  <matplotlib.axis.XTick at 0x1bc4111b100>,\n",
       "  <matplotlib.axis.XTick at 0x1bc4111b850>,\n",
       "  <matplotlib.axis.XTick at 0x1bc41124070>,\n",
       "  <matplotlib.axis.XTick at 0x1bc4111b940>,\n",
       "  <matplotlib.axis.XTick at 0x1bc411109a0>,\n",
       "  <matplotlib.axis.XTick at 0x1bc411245e0>,\n",
       "  <matplotlib.axis.XTick at 0x1bc41124c40>,\n",
       "  <matplotlib.axis.XTick at 0x1bc4112a3d0>,\n",
       "  <matplotlib.axis.XTick at 0x1bc4112ab20>,\n",
       "  <matplotlib.axis.XTick at 0x1bc411312b0>,\n",
       "  <matplotlib.axis.XTick at 0x1bc4112a880>,\n",
       "  <matplotlib.axis.XTick at 0x1bc3f8884c0>],\n",
       " [Text(0, 0, 'PAY_0'),\n",
       "  Text(1, 0, 'BILL_AMT1'),\n",
       "  Text(2, 0, 'PAY_AMT1'),\n",
       "  Text(3, 0, 'PAY_AMT2'),\n",
       "  Text(4, 0, 'BILL_AMT2'),\n",
       "  Text(5, 0, 'PAY_2'),\n",
       "  Text(6, 0, 'BILL_AMT3'),\n",
       "  Text(7, 0, 'LIMIT_BAL'),\n",
       "  Text(8, 0, 'PAY_3'),\n",
       "  Text(9, 0, 'MARRIAGE'),\n",
       "  Text(10, 0, 'EDUCATION'),\n",
       "  Text(11, 0, 'AGE'),\n",
       "  Text(12, 0, 'PAY_AMT4'),\n",
       "  Text(13, 0, 'SEX'),\n",
       "  Text(14, 0, 'PAY_AMT5'),\n",
       "  Text(15, 0, 'PAY_AMT3'),\n",
       "  Text(16, 0, 'PAY_5'),\n",
       "  Text(17, 0, 'BILL_AMT5'),\n",
       "  Text(18, 0, 'PAY_AMT6'),\n",
       "  Text(19, 0, 'PAY_4'),\n",
       "  Text(20, 0, 'BILL_AMT6'),\n",
       "  Text(21, 0, 'BILL_AMT4'),\n",
       "  Text(22, 0, 'PAY_6')])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_features = scaler.fit_transform(train_features)\n",
    "\n",
    "scaled_lr_model = LogisticRegression(random_state=42)\n",
    "\n",
    "scaled_lr_model.fit(scaled_features, train_targets)\n",
    "\n",
    "# Grab the coefficients\n",
    "logit_coef = np.exp(scaled_lr_model.coef_[0]) -1\n",
    "\n",
    "idx = abs(logit_coef).argsort()[::-1]\n",
    "\n",
    "plt.bar(range(len(idx)), logit_coef[idx])\n",
    "\n",
    "plt.xticks(range(len(idx)), train_features.columns[idx], rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2139a114",
   "metadata": {},
   "source": [
    "First, we import the scaler for standardizing the data and matplotlib for plotting.\n",
    "Then we scale the features and fit a logistic regression model to this data. We use\n",
    "the coef_ attribute of the model, indexing the first row with [0]. The coef_ array has\n",
    "rows containing coefficients for each target variable and columns for each feature.\n",
    "Since we only have one target variable, we only have one row. We exponentiate\n",
    "Euler's number (e) to the coefficient values with np.exp() in order to get the odds\n",
    "relationship to the target variable. We then subtract 1 from this value, since a value\n",
    "of 1 for the exponentiated coefficient means there is no relationship to the target.\n",
    "Values close to 0 in the logit_coef array then mean that there is little relationship\n",
    "between that feature and the target, and the farther away from 0 the values are, the\n",
    "more important the features to predicting the target.\n",
    "\n",
    "Next, we use argsort() on the absolute values of our coefficients to get the index\n",
    "values, and then reverse this array with [::-1] so that our coefficient values will\n",
    "be greatest to least. Finally, we plot the odds coefficients and label them with the\n",
    "column names. Our resulting feature importances look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec15c8",
   "metadata": {},
   "source": [
    "We can see that the most important feature is PAY_0, which is the number of months\n",
    "late for the most recent month's payment in the dataset, with a positive relationship\n",
    "to the target. Some other features from recent months have a negative relationship to\n",
    "the target, meaning the larger the value, the smaller the chance of default. For PAY_0,\n",
    "this makes intuitive sense – if a customer is late on their most recent payment, they\n",
    "probably have a higher chance of not paying their bill next month. As we get beyond\n",
    "the LIMIT_BAL feature, our odds ratios for features become very close to 1 (0 in the\n",
    "plot above) for the most part. We can use these feature importances to prune away\n",
    "some features by removing those with small coefficients or low relative importance.\n",
    "Here, we might try removing everything from PAY_2 and smaller on the feature\n",
    "importance plot, since the coefficient for PAY_2 has a large jump down in importance\n",
    "from the BILL_AMT_2 feature.\n",
    "\n",
    "This feature importance method can be useful for better understanding the data and\n",
    "performing feature selection, and we'll see how it relates to other algorithms as well.\n",
    "Another way to examine the significance of our coefficients is with a statistical test\n",
    "that gives us p-values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb5fe4a",
   "metadata": {},
   "source": [
    "### Using statmodels for Logistic Regression\n",
    "\n",
    "The sklearn package does not give us p-values, but the statsmodels implementation\n",
    "of logistic regression does. It uses a likelihood ratio test to calculate these p-values. We can compare these\n",
    "p-values to an alpha value (usually 0.05) to test for the significance of the result. If the\n",
    "p-value is less than 0.05, we can say that our null hypothesis is rejected. For logistic\n",
    "and linear regression, the null hypothesis is that the coefficients are no different from\n",
    "0. So, if our p-values are less than 0.05 for coefficients, we can consider keeping those\n",
    "coefficients and throwing out other coefficients with large p-values. The statsmodels\n",
    "approach to modeling is slightly different, and works like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33314053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.464620\n",
      "         Iterations 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>default payment next month</td> <th>  No. Observations:  </th>  <td> 30000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                      <td>Logit</td>           <th>  Df Residuals:      </th>  <td> 29976</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                      <td>MLE</td>            <th>  Df Model:          </th>  <td>    23</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>                 <td>Sat, 09 Apr 2022</td>      <th>  Pseudo R-squ.:     </th>  <td>0.1207</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                     <td>15:04:57</td>          <th>  Log-Likelihood:    </th> <td> -13939.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>                  <td>True</td>            <th>  LL-Null:           </th> <td> -15853.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>          <td>nonrobust</td>         <th>  LLR p-value:       </th>  <td> 0.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>     <td>   -0.6863</td> <td>    0.119</td> <td>   -5.784</td> <td> 0.000</td> <td>   -0.919</td> <td>   -0.454</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL</th> <td>-7.623e-07</td> <td> 1.57e-07</td> <td>   -4.859</td> <td> 0.000</td> <td>-1.07e-06</td> <td>-4.55e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SEX</th>       <td>   -0.1087</td> <td>    0.031</td> <td>   -3.541</td> <td> 0.000</td> <td>   -0.169</td> <td>   -0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>EDUCATION</th> <td>   -0.1016</td> <td>    0.021</td> <td>   -4.844</td> <td> 0.000</td> <td>   -0.143</td> <td>   -0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MARRIAGE</th>  <td>   -0.1544</td> <td>    0.032</td> <td>   -4.869</td> <td> 0.000</td> <td>   -0.216</td> <td>   -0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AGE</th>       <td>    0.0074</td> <td>    0.002</td> <td>    4.170</td> <td> 0.000</td> <td>    0.004</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_0</th>     <td>    0.5774</td> <td>    0.018</td> <td>   32.632</td> <td> 0.000</td> <td>    0.543</td> <td>    0.612</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_2</th>     <td>    0.0828</td> <td>    0.020</td> <td>    4.103</td> <td> 0.000</td> <td>    0.043</td> <td>    0.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_3</th>     <td>    0.0721</td> <td>    0.023</td> <td>    3.192</td> <td> 0.001</td> <td>    0.028</td> <td>    0.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_4</th>     <td>    0.0239</td> <td>    0.025</td> <td>    0.956</td> <td> 0.339</td> <td>   -0.025</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_5</th>     <td>    0.0340</td> <td>    0.027</td> <td>    1.266</td> <td> 0.206</td> <td>   -0.019</td> <td>    0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_6</th>     <td>    0.0080</td> <td>    0.022</td> <td>    0.363</td> <td> 0.716</td> <td>   -0.035</td> <td>    0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT1</th> <td>-5.492e-06</td> <td> 1.14e-06</td> <td>   -4.835</td> <td> 0.000</td> <td>-7.72e-06</td> <td>-3.27e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT2</th> <td> 2.356e-06</td> <td>  1.5e-06</td> <td>    1.566</td> <td> 0.117</td> <td>-5.92e-07</td> <td>  5.3e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT3</th> <td> 1.365e-06</td> <td> 1.32e-06</td> <td>    1.032</td> <td> 0.302</td> <td>-1.23e-06</td> <td> 3.96e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT4</th> <td>-1.821e-07</td> <td> 1.35e-06</td> <td>   -0.135</td> <td> 0.893</td> <td>-2.83e-06</td> <td> 2.46e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT5</th> <td> 6.155e-07</td> <td> 1.52e-06</td> <td>    0.405</td> <td> 0.685</td> <td>-2.36e-06</td> <td> 3.59e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT6</th> <td> 3.938e-07</td> <td> 1.19e-06</td> <td>    0.330</td> <td> 0.742</td> <td>-1.95e-06</td> <td> 2.74e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT1</th>  <td>-1.363e-05</td> <td> 2.31e-06</td> <td>   -5.913</td> <td> 0.000</td> <td>-1.81e-05</td> <td>-9.11e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT2</th>  <td>-9.616e-06</td> <td> 2.09e-06</td> <td>   -4.590</td> <td> 0.000</td> <td>-1.37e-05</td> <td>-5.51e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT3</th>  <td>-2.742e-06</td> <td> 1.72e-06</td> <td>   -1.592</td> <td> 0.111</td> <td>-6.12e-06</td> <td> 6.34e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT4</th>  <td>-4.023e-06</td> <td> 1.78e-06</td> <td>   -2.254</td> <td> 0.024</td> <td>-7.52e-06</td> <td>-5.25e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT5</th>  <td>-3.311e-06</td> <td> 1.78e-06</td> <td>   -1.864</td> <td> 0.062</td> <td>-6.79e-06</td> <td> 1.71e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT6</th>  <td>-2.064e-06</td> <td>  1.3e-06</td> <td>   -1.593</td> <td> 0.111</td> <td> -4.6e-06</td> <td> 4.76e-07</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                               Logit Regression Results                               \n",
       "======================================================================================\n",
       "Dep. Variable:     default payment next month   No. Observations:                30000\n",
       "Model:                                  Logit   Df Residuals:                    29976\n",
       "Method:                                   MLE   Df Model:                           23\n",
       "Date:                        Sat, 09 Apr 2022   Pseudo R-squ.:                  0.1207\n",
       "Time:                                15:04:57   Log-Likelihood:                -13939.\n",
       "converged:                               True   LL-Null:                       -15853.\n",
       "Covariance Type:                    nonrobust   LLR p-value:                     0.000\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -0.6863      0.119     -5.784      0.000      -0.919      -0.454\n",
       "LIMIT_BAL  -7.623e-07   1.57e-07     -4.859      0.000   -1.07e-06   -4.55e-07\n",
       "SEX           -0.1087      0.031     -3.541      0.000      -0.169      -0.049\n",
       "EDUCATION     -0.1016      0.021     -4.844      0.000      -0.143      -0.060\n",
       "MARRIAGE      -0.1544      0.032     -4.869      0.000      -0.216      -0.092\n",
       "AGE            0.0074      0.002      4.170      0.000       0.004       0.011\n",
       "PAY_0          0.5774      0.018     32.632      0.000       0.543       0.612\n",
       "PAY_2          0.0828      0.020      4.103      0.000       0.043       0.122\n",
       "PAY_3          0.0721      0.023      3.192      0.001       0.028       0.116\n",
       "PAY_4          0.0239      0.025      0.956      0.339      -0.025       0.073\n",
       "PAY_5          0.0340      0.027      1.266      0.206      -0.019       0.087\n",
       "PAY_6          0.0080      0.022      0.363      0.716      -0.035       0.051\n",
       "BILL_AMT1  -5.492e-06   1.14e-06     -4.835      0.000   -7.72e-06   -3.27e-06\n",
       "BILL_AMT2   2.356e-06    1.5e-06      1.566      0.117   -5.92e-07     5.3e-06\n",
       "BILL_AMT3   1.365e-06   1.32e-06      1.032      0.302   -1.23e-06    3.96e-06\n",
       "BILL_AMT4  -1.821e-07   1.35e-06     -0.135      0.893   -2.83e-06    2.46e-06\n",
       "BILL_AMT5   6.155e-07   1.52e-06      0.405      0.685   -2.36e-06    3.59e-06\n",
       "BILL_AMT6   3.938e-07   1.19e-06      0.330      0.742   -1.95e-06    2.74e-06\n",
       "PAY_AMT1   -1.363e-05   2.31e-06     -5.913      0.000   -1.81e-05   -9.11e-06\n",
       "PAY_AMT2   -9.616e-06   2.09e-06     -4.590      0.000   -1.37e-05   -5.51e-06\n",
       "PAY_AMT3   -2.742e-06   1.72e-06     -1.592      0.111   -6.12e-06    6.34e-07\n",
       "PAY_AMT4   -4.023e-06   1.78e-06     -2.254      0.024   -7.52e-06   -5.25e-07\n",
       "PAY_AMT5   -3.311e-06   1.78e-06     -1.864      0.062   -6.79e-06    1.71e-07\n",
       "PAY_AMT6   -2.064e-06    1.3e-06     -1.593      0.111    -4.6e-06    4.76e-07\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "lr_model = sm.Logit(train_targets, sm.add_constant(train_features))\n",
    "\n",
    "lr_results = lr_model.fit()\n",
    "\n",
    "lr_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7ce30a",
   "metadata": {},
   "source": [
    "We first import the statsmodels package's api module with the alias sm – this is the\n",
    "convention and how it is demonstrated in the statsmodels documentation. Then\n",
    "we set a random seed with NumPy. We will see shortly how random processes are\n",
    "used with logistic regression, but remember we also set a random seed with the\n",
    "sklearn implementation. Next, we instantiate the Logit class from statsmodels,\n",
    "giving it the targets as a first argument, and features second. Then we call the fit()\n",
    "method of this object (lr_model), which is where the machine learning part happens.\n",
    "This returns a new object with the results of the training, or fitting, process. \n",
    "\n",
    "In the upper-right column of the results, we have the number of observations, the\n",
    "degrees of freedom for the residuals (difference between predictions and actual\n",
    "values), the degrees of freedom for the model (number of coefficients minus 1), and\n",
    "some metrics for the model. The first metric, pseudo R-squared, approaches 1.0 for\n",
    "a perfect model (that perfectly predicts the data) and approaches 0 for a model that\n",
    "does not have any predictive power (cannot predict better than always guessing the\n",
    "majority class). Often, this value will be small and can be used to compare multiple\n",
    "models. The next three metrics have to do with log-likelihood, which is an equation\n",
    "we will cover shortly. It measures how well our model fits the data, and bigger is\n",
    "better. The LL-Null term is the log-likelihood of the \"null\" model. The null model\n",
    "is where we only use an intercept term (constant) in our logistic regression. Lastly,\n",
    "the LLR p-value is the log-likelihood ratio statistical test. If our p-value is smaller\n",
    "than a chosen alpha value (again, we usually compare this to an alpha value of 0.05),\n",
    "then the LLR test tells us that our model is statistically significantly better than the\n",
    "null model.\n",
    "\n",
    ">There is also another summary function, summary2(). This shows\n",
    "most of the same information, but also includes AIC and BIC.\n",
    "These are information criteria (IC). AIC is the Akaike Information\n",
    "Criterion, and BIC is the Bayesian Information Criterion. Both are\n",
    "better when smaller and can be used to compare models on the\n",
    "same dataset. Smaller is always better for IC – IC have an additive\n",
    "penalty term for the number of features, so the same accuracy with\n",
    "fewer predictors will have smaller IC values.\n",
    "\n",
    "Next, we have rows for each of our features (also called exogenous variables,\n",
    "independent variables, or covariates). The coef column shows the coefficient value\n",
    "for the logistic regression equation, followed by the standard error (an estimate of\n",
    "the standard deviation), a z-value from a Wald test, a p-value from the same test, and\n",
    "95% confidence intervals (we expect the coef value to lie within these bounds 95%\n",
    "of the time when fitting to samples of the data). We can use the p-values to select\n",
    "features by only keeping features where p < 0.05:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf73639f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.465728\n",
      "         Iterations 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>default payment next month</td> <th>  No. Observations:  </th>  <td> 30000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                      <td>Logit</td>           <th>  Df Residuals:      </th>  <td> 29988</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                      <td>MLE</td>            <th>  Df Model:          </th>  <td>    11</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>                 <td>Sat, 09 Apr 2022</td>      <th>  Pseudo R-squ.:     </th>  <td>0.1186</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                     <td>14:39:28</td>          <th>  Log-Likelihood:    </th> <td> -13972.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>                  <td>True</td>            <th>  LL-Null:           </th> <td> -15853.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>          <td>nonrobust</td>         <th>  LLR p-value:       </th>  <td> 0.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>     <td>   -0.3661</td> <td>    0.088</td> <td>   -4.146</td> <td> 0.000</td> <td>   -0.539</td> <td>   -0.193</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL</th> <td>-8.283e-07</td> <td> 1.51e-07</td> <td>   -5.469</td> <td> 0.000</td> <td>-1.13e-06</td> <td>-5.31e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SEX</th>       <td>   -0.1218</td> <td>    0.030</td> <td>   -4.012</td> <td> 0.000</td> <td>   -0.181</td> <td>   -0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>EDUCATION</th> <td>   -0.0892</td> <td>    0.021</td> <td>   -4.341</td> <td> 0.000</td> <td>   -0.130</td> <td>   -0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MARRIAGE</th>  <td>   -0.2091</td> <td>    0.029</td> <td>   -7.136</td> <td> 0.000</td> <td>   -0.267</td> <td>   -0.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_0</th>     <td>    0.5921</td> <td>    0.018</td> <td>   33.763</td> <td> 0.000</td> <td>    0.558</td> <td>    0.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_2</th>     <td>    0.0837</td> <td>    0.020</td> <td>    4.199</td> <td> 0.000</td> <td>    0.045</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_3</th>     <td>    0.1193</td> <td>    0.018</td> <td>    6.489</td> <td> 0.000</td> <td>    0.083</td> <td>    0.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT1</th> <td>-1.751e-06</td> <td> 2.64e-07</td> <td>   -6.630</td> <td> 0.000</td> <td>-2.27e-06</td> <td>-1.23e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT1</th>  <td>-1.176e-05</td> <td> 2.08e-06</td> <td>   -5.649</td> <td> 0.000</td> <td>-1.58e-05</td> <td>-7.68e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT2</th>  <td>-8.234e-06</td> <td> 1.79e-06</td> <td>   -4.594</td> <td> 0.000</td> <td>-1.17e-05</td> <td>-4.72e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT4</th>  <td>-4.455e-06</td> <td> 1.55e-06</td> <td>   -2.876</td> <td> 0.004</td> <td>-7.49e-06</td> <td>-1.42e-06</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                               Logit Regression Results                               \n",
       "======================================================================================\n",
       "Dep. Variable:     default payment next month   No. Observations:                30000\n",
       "Model:                                  Logit   Df Residuals:                    29988\n",
       "Method:                                   MLE   Df Model:                           11\n",
       "Date:                        Sat, 09 Apr 2022   Pseudo R-squ.:                  0.1186\n",
       "Time:                                14:39:28   Log-Likelihood:                -13972.\n",
       "converged:                               True   LL-Null:                       -15853.\n",
       "Covariance Type:                    nonrobust   LLR p-value:                     0.000\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -0.3661      0.088     -4.146      0.000      -0.539      -0.193\n",
       "LIMIT_BAL  -8.283e-07   1.51e-07     -5.469      0.000   -1.13e-06   -5.31e-07\n",
       "SEX           -0.1218      0.030     -4.012      0.000      -0.181      -0.062\n",
       "EDUCATION     -0.0892      0.021     -4.341      0.000      -0.130      -0.049\n",
       "MARRIAGE      -0.2091      0.029     -7.136      0.000      -0.267      -0.152\n",
       "PAY_0          0.5921      0.018     33.763      0.000       0.558       0.626\n",
       "PAY_2          0.0837      0.020      4.199      0.000       0.045       0.123\n",
       "PAY_3          0.1193      0.018      6.489      0.000       0.083       0.155\n",
       "BILL_AMT1  -1.751e-06   2.64e-07     -6.630      0.000   -2.27e-06   -1.23e-06\n",
       "PAY_AMT1   -1.176e-05   2.08e-06     -5.649      0.000   -1.58e-05   -7.68e-06\n",
       "PAY_AMT2   -8.234e-06   1.79e-06     -4.594      0.000   -1.17e-05   -4.72e-06\n",
       "PAY_AMT4   -4.455e-06   1.55e-06     -2.876      0.004   -7.49e-06   -1.42e-06\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features = sm.add_constant(train_features.loc[ :, (lr_results.pvalues < 0.05)])\n",
    "\n",
    "lr_model_trimmed = sm.Logit(train_targets, selected_features)\n",
    "\n",
    "lr_trimmed_results = lr_model_trimmed.fit()\n",
    "\n",
    "lr_trimmed_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb416b16",
   "metadata": {},
   "source": [
    "First, we generate a pandas DataFrame for our statsmodels-ready features with\n",
    "sm.add_constant(train_features). Then we select all rows and columns where the\n",
    "p-values are less than 0.05 with loc. We can fit the model and evaluate metrics with\n",
    "summary() and summary2(). From this, we see the performance of the model is nearly\n",
    "the same as the model with all features. So, we can use our model on fewer features\n",
    "(12 plus a constant term) instead of the model on the full set of features. If model\n",
    "performance is nearly the same with fewer features, it's better to choose the simpler\n",
    "model to minimize overfitting concerns and to minimize required resources.\n",
    "\n",
    "With statsmodels, we have a predict function just like with sklearn. This can be\n",
    "used from our results object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d0d040a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1        0.551270\n",
       "2        0.148310\n",
       "3        0.203144\n",
       "4        0.237579\n",
       "5        0.118267\n",
       "           ...   \n",
       "29996    0.147306\n",
       "29997    0.104594\n",
       "29998    0.848566\n",
       "29999    0.172756\n",
       "30000    0.260281\n",
       "Length: 30000, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_trimmed_results.predict(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ee5280",
   "metadata": {},
   "source": [
    "It returns a pandas series of predicted probabilities that the class is 1. We can round\n",
    "this up based on a threshold like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1d820d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1        0\n",
       "2        1\n",
       "3        1\n",
       "4        1\n",
       "5        1\n",
       "        ..\n",
       "29996    1\n",
       "29997    1\n",
       "29998    0\n",
       "29999    1\n",
       "30000    1\n",
       "Length: 30000, dtype: int32"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = (lr_trimmed_results.predict(selected_features) < 0.5).astype(\"int\")\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e54f94",
   "metadata": {},
   "source": [
    ">This will give us a pandas series of binary values that can be compared directly\n",
    "to the 0 and 1 targets. We can use this with metrics such as accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea259821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19113333333333332"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(train_targets, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6afec0b",
   "metadata": {},
   "source": [
    "With many sklearn metrics like this one, we provide the true values as the first\n",
    "argument and predicted values as the second argument. However, it's always best\n",
    "to check the documentation to be sure. We find our accuracy is around 81% (0.8091),\n",
    "which is a little better than the sklearn model.\n",
    "You may have noticed that the coefficients, as well as the accuracy score from the\n",
    "sklearn and statsmodels logistic regression models, are different. This primarily\n",
    "has to do with two settings: the optimization algorithm and regularization. To\n",
    "understand the optimization algorithm, let's look at how logistic regression finds\n",
    "its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bc41b429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2682</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13559</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>49291</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>35835</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>220000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>208365</td>\n",
       "      <td>88004</td>\n",
       "      <td>31237</td>\n",
       "      <td>15980</td>\n",
       "      <td>8500</td>\n",
       "      <td>20000</td>\n",
       "      <td>5003</td>\n",
       "      <td>3047</td>\n",
       "      <td>5000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>150000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>3502</td>\n",
       "      <td>8979</td>\n",
       "      <td>5190</td>\n",
       "      <td>0</td>\n",
       "      <td>1837</td>\n",
       "      <td>3526</td>\n",
       "      <td>8998</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>1.0</td>\n",
       "      <td>30000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>2758</td>\n",
       "      <td>20878</td>\n",
       "      <td>20582</td>\n",
       "      <td>19357</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22000</td>\n",
       "      <td>4200</td>\n",
       "      <td>2000</td>\n",
       "      <td>3100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>80000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>76304</td>\n",
       "      <td>52774</td>\n",
       "      <td>11855</td>\n",
       "      <td>48944</td>\n",
       "      <td>85900</td>\n",
       "      <td>3409</td>\n",
       "      <td>1178</td>\n",
       "      <td>1926</td>\n",
       "      <td>52964</td>\n",
       "      <td>1804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>49764</td>\n",
       "      <td>36535</td>\n",
       "      <td>32428</td>\n",
       "      <td>15313</td>\n",
       "      <td>2078</td>\n",
       "      <td>1800</td>\n",
       "      <td>1430</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       const  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  \\\n",
       "ID                                                                            \n",
       "1        1.0      20000    2          2         1   24      2      2     -1   \n",
       "2        1.0     120000    2          2         2   26     -1      2      0   \n",
       "3        1.0      90000    2          2         2   34      0      0      0   \n",
       "4        1.0      50000    2          2         1   37      0      0      0   \n",
       "5        1.0      50000    1          2         1   57     -1      0     -1   \n",
       "...      ...        ...  ...        ...       ...  ...    ...    ...    ...   \n",
       "29996    1.0     220000    1          3         1   39      0      0      0   \n",
       "29997    1.0     150000    1          3         2   43     -1     -1     -1   \n",
       "29998    1.0      30000    1          2         2   37      4      3      2   \n",
       "29999    1.0      80000    1          3         1   41      1     -1      0   \n",
       "30000    1.0      50000    1          2         1   46      0      0      0   \n",
       "\n",
       "       PAY_4  ...  BILL_AMT3  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  \\\n",
       "ID            ...                                                         \n",
       "1         -1  ...        689          0          0          0         0   \n",
       "2          0  ...       2682       3272       3455       3261         0   \n",
       "3          0  ...      13559      14331      14948      15549      1518   \n",
       "4          0  ...      49291      28314      28959      29547      2000   \n",
       "5          0  ...      35835      20940      19146      19131      2000   \n",
       "...      ...  ...        ...        ...        ...        ...       ...   \n",
       "29996      0  ...     208365      88004      31237      15980      8500   \n",
       "29997     -1  ...       3502       8979       5190          0      1837   \n",
       "29998     -1  ...       2758      20878      20582      19357         0   \n",
       "29999      0  ...      76304      52774      11855      48944     85900   \n",
       "30000      0  ...      49764      36535      32428      15313      2078   \n",
       "\n",
       "       PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \n",
       "ID                                                       \n",
       "1           689         0         0         0         0  \n",
       "2          1000      1000      1000         0      2000  \n",
       "3          1500      1000      1000      1000      5000  \n",
       "4          2019      1200      1100      1069      1000  \n",
       "5         36681     10000      9000       689       679  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "29996     20000      5003      3047      5000      1000  \n",
       "29997      3526      8998       129         0         0  \n",
       "29998         0     22000      4200      2000      3100  \n",
       "29999      3409      1178      1926     52964      1804  \n",
       "30000      1800      1430      1000      1000      1000  \n",
       "\n",
       "[30000 rows x 24 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.add_constant(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28361d6d",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation, Optimizers, and the Logistic Regression Algorithm\n",
    "\n",
    "\n",
    "Logistic regression uses an iterative optimization process to calculate the coefficient\n",
    "for each feature and intercept. As we saw from statsmodels, it is also using\n",
    "something called maximum likelihood estimation, or MLE. This MLE process relies\n",
    "on a likelihood function, which we are trying to maximize. \n",
    "\n",
    "Now that we have our log-likelihood or loss function, we can use it to fit our model\n",
    "with the logistic regression algorithm. To get the parameters (coefficients) for our\n",
    "logistic regression, we first initialize them to a value (for example, coefficients are\n",
    "initialized to 0 by default in statsmodels). Then we calculate predictions for our\n",
    "targets from the logistic regression equation and calculate the value of the likelihood\n",
    "function. Next, we change the parameters so that the likelihood function moves\n",
    "toward 0. We can do this with various optimizers. One that you will see with other\n",
    "algorithms (such as neural networks) is gradient descent. However, several other\n",
    "optimizers can be used. All of them change the parameters (coefficients) so that\n",
    "the log-likelihood function is optimized (maximized in our case, moving the loglikehood\n",
    "value toward 0). Maximizing the log-likelihood function by changing\n",
    "parameters is like letting a balloon rise to the center of a dome if the balloon is\n",
    "our model and the dome is the likelihood surface over all possible parameters.\n",
    "The mathematics of the optimizers gets complex and we will not delve into it, but\n",
    "we can change these optimizers as a parameter of our sklearn and statsmodels\n",
    "functions. To recap, the logistic regression algorithm is:\n",
    "- Initialize Coefficients\n",
    "- Predict target values (y-hat values for each data point)\n",
    "- Compute the log-likelihood or loss function\n",
    "- Use an optimizer to update the co-efficients so that the loss function is minimized\n",
    "- Repeat until the change in loss function is sufficiently small\n",
    "\n",
    "With the statsmodels logistic regression fit method, the argument for the optimizer\n",
    "is method. By default, it's newton for the Newton-Raphson optimizer. We can also\n",
    "set a maxiter argument for the maximum number of iterations the optimizer will\n",
    "undergo. If we see a warning with ConvergenceWarning: Maximum Likelihood\n",
    "optimization failed to converge., we can try other optimizers and/or increase\n",
    "the maxiter argument to see whether we can get the model to converge.\n",
    "\n",
    "With sklearn we can use the newton-cg method, which we specify with the solver\n",
    "argument. This is not exactly the same as the default Newton-Raphson method used\n",
    "in statsmodels, but is close. We also need to increase the max_iter argument so that\n",
    "the algorithm can converge:\n",
    "\n",
    "`lr_sklearn = LogisticRegression(solver='newton-cg', max_iter=1000)\n",
    "lr_sklearn.fit(train_features, train_targets)\n",
    "`\n",
    "\n",
    "However, we still get warnings here about the line search algorithm (our optimizer)\n",
    "not converging. The coefficients are more similar to the statsmodels results, so\n",
    "it seems it came close to converging and there may be a bug or peculiarity in the\n",
    "sklearn code, causing these warnings to show up with newton-cg. We can see from\n",
    "lr_sklearn.n_iter_ that 185 iterations were undertaken, less than our limit of 1000 –\n",
    "so it seems it should've converged. Interestingly, the statsmodels newton solver only\n",
    "takes 7 iterations to converge.\n",
    "\n",
    "Another difference between the default logistic regression settings in sklearn and\n",
    "statsmodels is regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4474470",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Regularization adds a penalty term to the log-likelihood function (also called our\n",
    "loss function). This penalty term moves the log-likelihood values further from 0 as\n",
    "coefficients get larger. There are three primary ways of enacting regularization with\n",
    "logistic regression: L1 (also called Lasso), L2 (also called Ridge), and Elastic Net (L1 +\n",
    "L2) regularization. Regularization prevents overfitting and gives us a dial to tune the\n",
    "bias-variance trade-off for our model.\n",
    "\n",
    "\n",
    "For usual log-likelihood, L1 regularization subtracts the term (lambda * sum(abs(b)) the absolute value of a coefficient and gamma (.... ) is a constant value we choose.\n",
    "Some implementations use negative log-likelihood, and we flip all the signs (so the\n",
    "regularization term would be added instead of subtracted). As we saw earlier, bigger\n",
    "coefficients mean a feature has a bigger influence on the target. If we have a lot of\n",
    "big coefficients, we may be fitting to noise in the data rather than the actual patterns.\n",
    "With L1 regularization, some coefficients can be regularized down to 0, providing\n",
    "some feature selection for us.\n",
    "\n",
    "\n",
    "L2 regularization uses the penalty term (lambda * sum(abs(b)^2)\n",
    "which results in shrinking coefficients but not reducing them to 0. Elastic Net combines the two. When using regularization, it's important to scale our features since the penalty terms penalize\n",
    "the raw magnitude of the coefficients. The only exception to this, as with PCA and\n",
    "logistic regression feature importances, is if our features are in the same units – then\n",
    "we might consider not scaling them.\n",
    "\n",
    "In sklearn, the penalty argument specifies the regularization method. By\n",
    "default, it uses L2 regularization. There is also a C argument, which is the inverse\n",
    "of the regularization strength – so a larger C value means less regulation. The\n",
    "documentation for the LogisticRegression class describes these parameters, and\n",
    "also states which solvers (optimizers) can be used with the different penalty terms\n",
    "(there are restrictions). If we set the value of C to a smaller number than the default\n",
    "of 1 and use L1 regularization, we can force some coefficients to be 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a0ba3446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09177091, -0.03149387, -0.04190868, -0.05807321,  0.04759691,\n",
       "         0.6387541 ,  0.09262123,  0.09002068,  0.02370525,  0.03696442,\n",
       "         0.00536184, -0.11607406,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        , -0.12242634, -0.09716088, -0.0206883 ,\n",
       "        -0.03081571, -0.02785315, -0.01939507]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_features = scaler.fit_transform(train_features)\n",
    "\n",
    "lr_sklearn = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", C=0.01)\n",
    "\n",
    "lr_sklearn.fit(scaled_features, train_targets)\n",
    "\n",
    "lr_sklearn.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ab10b5",
   "metadata": {},
   "source": [
    "We first standardize the data in preparation with StandardScaler as we've done\n",
    "before, and then set the value of C to 0.01 for a strong regularization effect. After\n",
    "fitting the model, we can see that the BILL_AMT features 2-6 have been regularized to\n",
    "0. We could consider dropping these features going forward, especially since they\n",
    "also had large p-values from the statsmodels results.\n",
    "\n",
    "To use regularization with statsmodels, we use the fit_regularized() method\n",
    "of Logit objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e070b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.47567931369409666\n",
      "            Iterations: 57\n",
      "            Function evaluations: 57\n",
      "            Gradient evaluations: 57\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>default payment next month</td> <th>  No. Observations:  </th>  <td> 30000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                      <td>Logit</td>           <th>  Df Residuals:      </th>  <td> 29981</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                      <td>MLE</td>            <th>  Df Model:          </th>  <td>    18</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>                 <td>Sat, 09 Apr 2022</td>      <th>  Pseudo R-squ.:     </th>  <td>0.1187</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                     <td>15:57:23</td>          <th>  Log-Likelihood:    </th> <td> -13970.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>                  <td>True</td>            <th>  LL-Null:           </th> <td> -15853.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>          <td>nonrobust</td>         <th>  LLR p-value:       </th>  <td> 0.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>     <td>   -1.4080</td> <td>    0.016</td> <td>  -88.843</td> <td> 0.000</td> <td>   -1.439</td> <td>   -1.377</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL</th> <td>   -0.0918</td> <td>    0.020</td> <td>   -4.634</td> <td> 0.000</td> <td>   -0.131</td> <td>   -0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SEX</th>       <td>   -0.0315</td> <td>    0.015</td> <td>   -2.122</td> <td> 0.034</td> <td>   -0.061</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>EDUCATION</th> <td>   -0.0419</td> <td>    0.016</td> <td>   -2.600</td> <td> 0.009</td> <td>   -0.074</td> <td>   -0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MARRIAGE</th>  <td>   -0.0581</td> <td>    0.016</td> <td>   -3.562</td> <td> 0.000</td> <td>   -0.090</td> <td>   -0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AGE</th>       <td>    0.0476</td> <td>    0.016</td> <td>    2.933</td> <td> 0.003</td> <td>    0.016</td> <td>    0.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_0</th>     <td>    0.6387</td> <td>    0.020</td> <td>   32.484</td> <td> 0.000</td> <td>    0.600</td> <td>    0.677</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_2</th>     <td>    0.0926</td> <td>    0.024</td> <td>    3.869</td> <td> 0.000</td> <td>    0.046</td> <td>    0.139</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_3</th>     <td>    0.0901</td> <td>    0.027</td> <td>    3.384</td> <td> 0.001</td> <td>    0.038</td> <td>    0.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_4</th>     <td>    0.0237</td> <td>    0.029</td> <td>    0.821</td> <td> 0.412</td> <td>   -0.033</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_5</th>     <td>    0.0370</td> <td>    0.030</td> <td>    1.230</td> <td> 0.219</td> <td>   -0.022</td> <td>    0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_6</th>     <td>    0.0053</td> <td>    0.025</td> <td>    0.214</td> <td> 0.831</td> <td>   -0.044</td> <td>    0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT1</th> <td>   -0.1160</td> <td>    0.019</td> <td>   -6.130</td> <td> 0.000</td> <td>   -0.153</td> <td>   -0.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT2</th> <td>         0</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT3</th> <td>         0</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT4</th> <td>         0</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT5</th> <td>         0</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT6</th> <td>         0</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT1</th>  <td>   -0.1224</td> <td>    0.028</td> <td>   -4.330</td> <td> 0.000</td> <td>   -0.178</td> <td>   -0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT2</th>  <td>   -0.0972</td> <td>    0.033</td> <td>   -2.974</td> <td> 0.003</td> <td>   -0.161</td> <td>   -0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT3</th>  <td>   -0.0207</td> <td>    0.022</td> <td>   -0.919</td> <td> 0.358</td> <td>   -0.065</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT4</th>  <td>   -0.0308</td> <td>    0.021</td> <td>   -1.455</td> <td> 0.146</td> <td>   -0.072</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT5</th>  <td>   -0.0278</td> <td>    0.020</td> <td>   -1.378</td> <td> 0.168</td> <td>   -0.067</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT6</th>  <td>   -0.0194</td> <td>    0.020</td> <td>   -0.955</td> <td> 0.339</td> <td>   -0.059</td> <td>    0.020</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                               Logit Regression Results                               \n",
       "======================================================================================\n",
       "Dep. Variable:     default payment next month   No. Observations:                30000\n",
       "Model:                                  Logit   Df Residuals:                    29981\n",
       "Method:                                   MLE   Df Model:                           18\n",
       "Date:                        Sat, 09 Apr 2022   Pseudo R-squ.:                  0.1187\n",
       "Time:                                15:57:23   Log-Likelihood:                -13970.\n",
       "converged:                               True   LL-Null:                       -15853.\n",
       "Covariance Type:                    nonrobust   LLR p-value:                     0.000\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -1.4080      0.016    -88.843      0.000      -1.439      -1.377\n",
       "LIMIT_BAL     -0.0918      0.020     -4.634      0.000      -0.131      -0.053\n",
       "SEX           -0.0315      0.015     -2.122      0.034      -0.061      -0.002\n",
       "EDUCATION     -0.0419      0.016     -2.600      0.009      -0.074      -0.010\n",
       "MARRIAGE      -0.0581      0.016     -3.562      0.000      -0.090      -0.026\n",
       "AGE            0.0476      0.016      2.933      0.003       0.016       0.079\n",
       "PAY_0          0.6387      0.020     32.484      0.000       0.600       0.677\n",
       "PAY_2          0.0926      0.024      3.869      0.000       0.046       0.139\n",
       "PAY_3          0.0901      0.027      3.384      0.001       0.038       0.142\n",
       "PAY_4          0.0237      0.029      0.821      0.412      -0.033       0.080\n",
       "PAY_5          0.0370      0.030      1.230      0.219      -0.022       0.096\n",
       "PAY_6          0.0053      0.025      0.214      0.831      -0.044       0.054\n",
       "BILL_AMT1     -0.1160      0.019     -6.130      0.000      -0.153      -0.079\n",
       "BILL_AMT2           0        nan        nan        nan         nan         nan\n",
       "BILL_AMT3           0        nan        nan        nan         nan         nan\n",
       "BILL_AMT4           0        nan        nan        nan         nan         nan\n",
       "BILL_AMT5           0        nan        nan        nan         nan         nan\n",
       "BILL_AMT6           0        nan        nan        nan         nan         nan\n",
       "PAY_AMT1      -0.1224      0.028     -4.330      0.000      -0.178      -0.067\n",
       "PAY_AMT2      -0.0972      0.033     -2.974      0.003      -0.161      -0.033\n",
       "PAY_AMT3      -0.0207      0.022     -0.919      0.358      -0.065       0.023\n",
       "PAY_AMT4      -0.0308      0.021     -1.455      0.146      -0.072       0.011\n",
       "PAY_AMT5      -0.0278      0.020     -1.378      0.168      -0.067       0.012\n",
       "PAY_AMT6      -0.0194      0.020     -0.955      0.339      -0.059       0.020\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_features_df = pd.DataFrame(\n",
    "        scaled_features,\n",
    "        columns=train_features.columns,\n",
    "        index=train_features.index\n",
    ")\n",
    "\n",
    "\n",
    "lr_model = sm.Logit(train_targets, sm.add_constant(scaled_features_df))\n",
    "\n",
    "reg_results = lr_model.fit_regularized(alpha=100)\n",
    "\n",
    "reg_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98fac90",
   "metadata": {},
   "source": [
    "First, we create a pandas DataFrame from our NumPy array of scaled data. We also\n",
    "need to set the column names when creating the DataFrame as well as the index.\n",
    "This is so that we will have the column names from our DataFrame in our summary\n",
    "report. The targets and features must have the same index if they are DataFrames\n",
    "or Series.\n",
    "\n",
    "Once we create our model object, we fit it and specify the regularization strength\n",
    "with the alpha parameter. Larger values mean more regularization, and this is the\n",
    "same as 1/C from the sklearn model. These values may be somewhere around 0.001\n",
    "to 100. Once we fit the model and examine the report, we can see that the same five\n",
    "features have been regularized to 0 – BILL_AMT features 2-6.\n",
    "\n",
    "Regularization is a handy tool to prevent overfitting, but choosing the optimal\n",
    "C requires some iteration. We could do this with a for loop, but an easier way is\n",
    "to use pre-built cross-validation tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a8f7f1",
   "metadata": {},
   "source": [
    "### Hyperparameters and Cross-Validation\n",
    "\n",
    "\n",
    "The C parameter in the sklearn logistic regression and the alpha value in the\n",
    "statsmodels implementation are called hyperparameters. These are settings for the\n",
    "model that we choose. By contrast, parameters are the coefficients that the model\n",
    "learns. This gets confusing because in Python programming, a parameter is also\n",
    "a setting we provide to a function or class, also called an argument. `So, the C value\n",
    "is a parameter in Python programming parlance and a hyperparameter in machine\n",
    "learning terminology.`\n",
    "\n",
    "\n",
    "For now, we can easily perform a hyperparameter search for the C parameter with the\n",
    "LogisticRegressionCV class in sklearn. This uses cross-validation, which breaks the\n",
    "data up into train and validation (or test) sets. For example, we could use 75% of our\n",
    "data for our model training and keep 25% as a validation set. We fit the model to the\n",
    "training set, and then calculate the accuracy or other metrics on the validation set.\n",
    "\n",
    "*Cross-validation (CV)* does this several times so that we train on and evaluate every\n",
    "part of the dataset as train and test. For example, we can use 3-fold cross-validation,\n",
    "which divides the data into thirds. We first fit to the first two-thirds of the data and\n",
    "evaluate the last third. Then we fit the model from scratch on the last two-thirds of\n",
    "the data and evaluate on the first third.\n",
    "\n",
    "Finally, we train on the first and last thirds, and evaluate on the middle third. Then\n",
    "we average the scores obtained on the three validation sections to get our score\n",
    "for that hyperparameter setting. We can change the hyperparameter settings and\n",
    "repeat the CV process and keep the hyperparameter settings with the highest score.\n",
    "\n",
    "We will obtain three validation scores, which we then average to get an overall cross-validation score on the dataset for one algorithm with one set of hyperparameters. We can perform this for a range of\n",
    "hyperparameters (for example, C values) and choose the hyperparameters with the\n",
    "best validation score.\n",
    "\n",
    ">For some models in sklearn such as logistic regression, this is made easy with a CV\n",
    "class. We can perform cross-validation on our data to find the optimal C value like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cae8b1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=[0.001, 0.01, 0.1, 1, 10, 100], n_jobs=-1, penalty='l1',\n",
       "                     random_state=42, solver='liblinear')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "lr_cv = LogisticRegressionCV(Cs=[0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                             solver=\"liblinear\",\n",
    "                             penalty=\"l1\",\n",
    "                             n_jobs=-1,\n",
    "                             random_state=42\n",
    "                            )\n",
    "\n",
    "lr_cv.fit(scaled_features, train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e631997c",
   "metadata": {},
   "source": [
    "First, we import the LogisticRegressionCV class. Then, we initialize it with several\n",
    "parameters: the list of C values we want to try (Cs), along with the solver and\n",
    "penalty arguments. We are using the L1 penalty here so that we may be able to\n",
    "remove some features. Since only the liblinear and saga solvers work with L1\n",
    "(as described in the documentation for the LogisticRegressionCV class), we are\n",
    "specifying the liblinear solver. We also set the n_jobs parameter to -1, which tells\n",
    "sklearn to use all available CPU cores in parallel to run the cross-validation process.\n",
    "This is a common argument we will see in other sklearn functions. Lastly, we set\n",
    "random_state, which affects how the liblinear solver works and will provide more\n",
    "reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea422547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best C Value\n",
    "\n",
    "lr_cv.C_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e2629d",
   "metadata": {},
   "source": [
    "We can also look at the cross-validation scores with lr_cv.scores_.\n",
    "To get the average validation set score for each value of C, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b977a4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78556667, 0.80863333, 0.8094    , 0.80996667, 0.80993333,\n",
       "       0.80993333])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_cv.scores_[1].mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7a6e48",
   "metadata": {},
   "source": [
    "This returns an array the same length as our number of C values and shows us the\n",
    "score was around 81% (0.81) for the value of C=1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55daa04c",
   "metadata": {},
   "source": [
    "### Logistic regression (and other models) with big data\n",
    "\n",
    "Often, we'll find that we may encounter big data in our work and need a way to\n",
    "deal with it. Both sklearn and statsmodels can be adapted to work with big data.\n",
    "For example, statsmodels has a DistributedModel class that could be used, though\n",
    "examples of how to use it are lacking. For sklearn, the dask Python package can\n",
    "be used (with an example here: https://examples.dask.org/machine-learning/incremental.html). However, one simple and quick fix (although not ideal) to\n",
    "dealing with big data is to sample the data down to something that can be handled\n",
    "on a single machine. But for actually using all the data, there are several other Python\n",
    "packages available that can perform logistic regression on big datasets:\n",
    "\n",
    "- Vowpal Wabbit\n",
    "- H2O\n",
    "- TensorFlow\n",
    "- Dask\n",
    "- Spark (pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f280f5",
   "metadata": {},
   "source": [
    "> H2O can also be used for logistic regression on a single machine\n",
    "or scale up to a cluster. A small example is shown in the code for\n",
    "this chapter on this book's GitHub repository. Some advantages of\n",
    "H2O for ML are that it can handle missing values and non-numeric\n",
    "values (for example, strings) gracefully. It also has convenience\n",
    "methods for plotting the feature importances from ML models\n",
    "(which H2O calls \"variable importances\").\n",
    "Logistic regression has a page in H2O's documentation here,\n",
    "with examples in Python: https://docs.h2o.ai/h2o/lateststable/\n",
    "h2o-docs/data-science/glm.html#examples.\n",
    "However, the R documentation tends to be more organized and\n",
    "easier to read: https://docs.h2o.ai/h2o/latest-stable/\n",
    "h2o-r/docs/reference/h2o.glm.html.\n",
    "Most, if not all, of the R and Python function names and arguments\n",
    "are the same with H2O."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54279ee",
   "metadata": {},
   "source": [
    "## Naive Bayes for Binary Classification\n",
    "\n",
    "\n",
    "With the Naïve Bayes model, we predict the\n",
    "probability of a class by multiplying conditional probabilities along with the prior\n",
    "probability of each class, P(y). There are a few different Naïve Bayes classifiers in sklearn. All are used for\n",
    "classification, but take different types of features:\n",
    "\n",
    "\n",
    "- BernoulliNB – binary features (1s and 0s)\n",
    "- CategoricalNB – discrete, non-negative categorical features (for example, onefeature could contain 0s, 1s, and 2s)\n",
    "- ComplementNB – takes non-negative categorical and numeric features; similar to MultinomialNB, but better for imbalanced datasets where the distribution of targets is not uniform\n",
    "- GaussianNB – takes any numeric features; assumes the likelihood of features (the P(xi|y) terms) is Gaussian\n",
    "- MultinomialNB – takes non-negative categorical and numeric features\n",
    "\n",
    "With our data, we can try using GaussianNB. However, we can easily see our features\n",
    "and target are not normally distributed, so we could guess this algorithm won't do\n",
    "too well. We can fit it and check the score like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5290c58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.378"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb.fit(train_features, train_targets)\n",
    "\n",
    "gnb.score(train_features, train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de4e060",
   "metadata": {},
   "source": [
    "As with all sklearn algorithms, we first import it, and then initialize the model\n",
    "object. Then we fit the model to the training features and targets and can evaluate the\n",
    "score. Our score is only 37.8% accuracy, which is not great – far worse than guessing\n",
    "the majority class.\n",
    "\n",
    "These Naïve Bayes models have the predict() and predict_proba() functions that\n",
    "work the same as with the logistic regression class (we simply provide the features\n",
    "to the prediction functions). The Naïve Bayes models also have a partial_fit()\n",
    "method, where we can provide part of the data at a time for the model to update its\n",
    "parameters. In this way, we can handle bigger data or streaming data by fitting it a\n",
    "little bit at a time.\n",
    "\n",
    "One way to use the Naïve Bayes models is for text classification. For example, if we\n",
    "want to categorize text as positive or negative, we could use the multinomial Naïve\n",
    "Bayes classifier with word counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89ccc09",
   "metadata": {},
   "source": [
    "### k-nearest neighbors\n",
    "\n",
    "This is a\n",
    "distance-based algorithm. If we want to predict the class of a data point, we take the\n",
    "nearest k points to our data point (measured by distances between features of the\n",
    "points) and take a weighted average of the classes of the nearest points to make our\n",
    "prediction. KNN is different from other ML algorithms because there is no training\n",
    "– we only store our data, and then calculate the distances upon evaluation to make\n",
    "predictions. We can easily use KNN in sklearn, but we want to make sure to scale\n",
    "our features before using it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a4be73ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [\"LIMIT_BAL\"] + [f\"BILL_AMT{i}\" for i in range(1, 7)] + [f\"PAY_AMT{i}\" for i in range(1, 7)]\n",
    "\n",
    "categorical_columns = [\"SEX\", \"EDUCATION\", \"MARRIAGE\", \"PAY_0\"] + [f\"PAY_{i}\" for i in range(2, 6)]\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "scaled_numeric_features = scaler.fit_transform(train_features[numeric_columns])\n",
    "\n",
    "\n",
    "scaled_features = pd.concat(\n",
    "                [pd.DataFrame(data=scaled_numeric_features, columns=numeric_columns, index=credit_df.index),\n",
    "                train_features[categorical_columns]],\n",
    "                axis=1\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0985df",
   "metadata": {},
   "source": [
    "First, we create lists of our numeric and categorical features. Then we use\n",
    "StandardScaler to scale the numeric features. Finally, we join the two sets of features\n",
    "back together using pd.concat(). We create a DataFrame out of our scaled features,\n",
    "providing the column names and the index from the original DataFrame so that our\n",
    "DataFrame merges properly. Then we can fit the KNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1d99c910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8456666666666667"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "knn.fit(scaled_features, train_targets)\n",
    "\n",
    "knn.score(scaled_features, train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b016b4",
   "metadata": {},
   "source": [
    "First, we import the class as usual with sklearn models and then instantiate it.\n",
    "The n_jobs=-1 argument specifies that we should use all available processers for\n",
    "calculating distances, which speeds up the runtime. There are other arguments\n",
    "available for the distance calculations. By default, it uses the Euclidean distance,\n",
    "which is the straight-line distance between points. We can set the argument p=1 in\n",
    "KNeighborsClassifier() to use the Manhattan or city block distance, which tends\n",
    "to work better for higher-dimensional data (with many features).\n",
    "\n",
    "\n",
    "Instead of a straight-line distance between points, the Manhattan distance is the\n",
    "distance between two points measured along axes at right angles (orthogonal). It\n",
    "looks like traversing through the downtown area of a city (for example, Manhattan\n",
    "in New York City) via car. An example of Manhattan distance is shown in Figure\n",
    "11.6. The Manhattan distance will be a larger value than the Euclidean distance.\n",
    "\n",
    "\n",
    ">Both the Euclidean and Manhattan distances are derived from\n",
    "the Minkowski distance (https://en.wikipedia.org/wiki/\n",
    "Minkowski_distance).\n",
    "\n",
    "\n",
    "KNN is very easy to understand and implement. The main hyperparameters we\n",
    "can tune are the number of neighbors (the n_neighbors argument), the weighting\n",
    "of nearby points for calculating the predicted class (weights), and the distance\n",
    "calculation (p and metric). The easiest way to search these is to simply optimize the\n",
    "n_neighbors argument.\n",
    "\n",
    "\n",
    "Our accuracy on the training set for this model is 0.8463 (84%), which is not bad for such\n",
    "a simple model, and actually looks better than our logistic regression models so far.\n",
    "In fact, our accuracy improves a little after scaling/standardizing all the features\n",
    "with sklearn's StandardScaler. Without scaling, our accuracy on the training dataset\n",
    "is 81.7%. However, we do need to be careful when evaluating accuracy on the same\n",
    "data we trained on. If our model is overfitting to the data (fitting to noise in the data),\n",
    "we may see a very high accuracy on the training data, but low accuracy on new data.\n",
    "\n",
    "\n",
    "The sklearn documentation has explanations and examples for its functionality. For\n",
    "example, the KNN examples and explanation can be found here: https://scikitlearn.\n",
    "org/stable/modules/neighbors.html#nearest-neighbors-classification. \n",
    "- Store our training data in memory\n",
    "- To make a prediction, compute distances using a distance metric between\n",
    "the new data point and existing data points (there are multiple ways of doing\n",
    "this; the brute-force method is to compute all distances)\n",
    "- Take the k nearest points and take an average or weighted average to arrive at our 𝑦𝑦𝑦 value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802548fa",
   "metadata": {},
   "source": [
    "## Multiclass Classification\n",
    "\n",
    "In multiclass classification, we have three or more classes in our target. This is also\n",
    "called multinomial classification. All classifiers in sklearn can perform multiclass\n",
    "classification, and there are a few other multiclass classification tools in sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d498507",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Let's use the same logistic regression model as before, but using PAY_0 as our target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a540ac83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=100000)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pay_0_target = credit_df[\"PAY_0\"].replace({i: 1 for i in range(1, 9)})\n",
    "pay_0_features = credit_df.drop([\"PAY_0\", \"default payment next month\"], axis=1)\n",
    "\n",
    "lr_multi = LogisticRegression(max_iter=100000)\n",
    "lr_multi.fit(pay_0_features, pay_0_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c062653f",
   "metadata": {},
   "source": [
    "First, we create a pandas series with the PAY_0 column as a target. This has values\n",
    "-2 through 8 for different categories of payments (-2 is \"no consumption\", -1 is paid\n",
    "on time, 0 is the use of revolving credit, and 1-8 is the number of months late on\n",
    "payment). We convert any value from 1 through 8 to the value 1 to simplify the\n",
    "classes – with this change, 1 signifies a late payment. Then we create a set of features\n",
    "without our target column and without the binary default column. We can use\n",
    "the sklearn LogisticRegression class on this with no modifications, although we\n",
    "increased the max_iter argument so that our model can fully fit the data (it takes\n",
    "228 iterations to converge, greater than the default max_iter of 100). With other\n",
    "classification models in sklearn, we can also simply give it a multi-class target and\n",
    "use the classifier as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d968f1ce",
   "metadata": {},
   "source": [
    "The multinomial logistic regression algorithm carries out the same logistic regression\n",
    "equation as before, but on k classes. There are multiple ways to formulate the\n",
    "equations for solving for the coefficients and predictions for class probabilities,\n",
    "but we end up with the same result – a probability for each class, where the sum of\n",
    "probabilities for each prediction sums to 1. For example, we might end up with a\n",
    "vector of probability predictions that looks like [0.25, 0.25, 0,5] for the probability of\n",
    "3 different classes from multinomial logistic regression. With sklearn, the logistic\n",
    "regression coefficients are an array of shape (n_classes, n_features + 1). The +1 is\n",
    "for the intercept term.\n",
    "\n",
    ">The loss function used for multi-class logistic regression is the\n",
    "same as the binary case (cross-entropy) but is generalized to\n",
    "multiple classes. The sklearn documentation shows the exact\n",
    "equation for this: https://scikit-learn.org/stable/modules/\n",
    "model_evaluation.html#log-loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb7644b",
   "metadata": {},
   "source": [
    "We can also use the MNLogit class from statsmodels, which gives us the coefficients\n",
    "for k-1 of the classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2ed14c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.762369\n",
      "         Iterations 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>MNLogit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>PAY_0</td>      <th>  No. Observations:  </th>  <td> 30000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                <td>MNLogit</td>     <th>  Df Residuals:      </th>  <td> 29931</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    66</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sat, 09 Apr 2022</td> <th>  Pseudo R-squ.:     </th>  <td>0.3754</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>19:21:11</td>     <th>  Log-Likelihood:    </th> <td> -22871.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -36618.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 0.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>PAY_0=-1</th>     <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>     <td>    8.7368</td> <td>    0.328</td> <td>   26.622</td> <td> 0.000</td> <td>    8.094</td> <td>    9.380</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL</th> <td>-1.051e-06</td> <td> 2.92e-07</td> <td>   -3.596</td> <td> 0.000</td> <td>-1.62e-06</td> <td>-4.78e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SEX</th>       <td>   -0.1590</td> <td>    0.072</td> <td>   -2.223</td> <td> 0.026</td> <td>   -0.299</td> <td>   -0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>EDUCATION</th> <td>   -0.2542</td> <td>    0.044</td> <td>   -5.838</td> <td> 0.000</td> <td>   -0.339</td> <td>   -0.169</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MARRIAGE</th>  <td>   -0.0991</td> <td>    0.073</td> <td>   -1.351</td> <td> 0.177</td> <td>   -0.243</td> <td>    0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AGE</th>       <td>    0.0074</td> <td>    0.004</td> <td>    1.684</td> <td> 0.092</td> <td>   -0.001</td> <td>    0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_2</th>     <td>    4.2855</td> <td>    0.114</td> <td>   37.478</td> <td> 0.000</td> <td>    4.061</td> <td>    4.510</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_3</th>     <td>   -0.4013</td> <td>    0.079</td> <td>   -5.059</td> <td> 0.000</td> <td>   -0.557</td> <td>   -0.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_4</th>     <td>    0.2645</td> <td>    0.088</td> <td>    3.013</td> <td> 0.003</td> <td>    0.092</td> <td>    0.437</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_5</th>     <td>    0.1763</td> <td>    0.088</td> <td>    2.005</td> <td> 0.045</td> <td>    0.004</td> <td>    0.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_6</th>     <td>    0.3368</td> <td>    0.070</td> <td>    4.840</td> <td> 0.000</td> <td>    0.200</td> <td>    0.473</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT1</th> <td>-1.451e-05</td> <td>    2e-06</td> <td>   -7.256</td> <td> 0.000</td> <td>-1.84e-05</td> <td>-1.06e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT2</th> <td> -3.93e-05</td> <td> 3.27e-06</td> <td>  -12.005</td> <td> 0.000</td> <td>-4.57e-05</td> <td>-3.29e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT3</th> <td>-8.032e-06</td> <td> 3.09e-06</td> <td>   -2.600</td> <td> 0.009</td> <td>-1.41e-05</td> <td>-1.98e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT4</th> <td> 2.279e-06</td> <td> 2.89e-06</td> <td>    0.789</td> <td> 0.430</td> <td>-3.38e-06</td> <td> 7.94e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT5</th> <td> -1.86e-06</td> <td> 3.16e-06</td> <td>   -0.589</td> <td> 0.556</td> <td>-8.05e-06</td> <td> 4.33e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT6</th> <td> -7.21e-06</td> <td> 2.64e-06</td> <td>   -2.734</td> <td> 0.006</td> <td>-1.24e-05</td> <td>-2.04e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT1</th>  <td> 3.423e-05</td> <td> 3.06e-06</td> <td>   11.201</td> <td> 0.000</td> <td> 2.82e-05</td> <td> 4.02e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT2</th>  <td>   3.8e-06</td> <td> 1.92e-06</td> <td>    1.975</td> <td> 0.048</td> <td> 2.95e-08</td> <td> 7.57e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT3</th>  <td> 4.261e-07</td> <td> 2.74e-06</td> <td>    0.156</td> <td> 0.876</td> <td>-4.94e-06</td> <td> 5.79e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT4</th>  <td>  3.94e-07</td> <td> 2.84e-06</td> <td>    0.139</td> <td> 0.890</td> <td>-5.17e-06</td> <td> 5.96e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT5</th>  <td> 6.995e-08</td> <td> 2.96e-06</td> <td>    0.024</td> <td> 0.981</td> <td>-5.72e-06</td> <td> 5.86e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT6</th>  <td>-2.944e-06</td> <td> 1.65e-06</td> <td>   -1.780</td> <td> 0.075</td> <td>-6.19e-06</td> <td> 2.98e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <th>PAY_0=0</th>     <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>     <td>    8.9797</td> <td>    0.333</td> <td>   26.930</td> <td> 0.000</td> <td>    8.326</td> <td>    9.633</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL</th> <td>-4.532e-06</td> <td> 3.07e-07</td> <td>  -14.777</td> <td> 0.000</td> <td>-5.13e-06</td> <td>-3.93e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SEX</th>       <td>   -0.2313</td> <td>    0.073</td> <td>   -3.155</td> <td> 0.002</td> <td>   -0.375</td> <td>   -0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>EDUCATION</th> <td>   -0.0285</td> <td>    0.044</td> <td>   -0.651</td> <td> 0.515</td> <td>   -0.115</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MARRIAGE</th>  <td>    0.0287</td> <td>    0.075</td> <td>    0.383</td> <td> 0.702</td> <td>   -0.118</td> <td>    0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AGE</th>       <td>   -0.0047</td> <td>    0.004</td> <td>   -1.053</td> <td> 0.292</td> <td>   -0.014</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_2</th>     <td>    3.8719</td> <td>    0.114</td> <td>   33.887</td> <td> 0.000</td> <td>    3.648</td> <td>    4.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_3</th>     <td>   -0.2820</td> <td>    0.079</td> <td>   -3.553</td> <td> 0.000</td> <td>   -0.438</td> <td>   -0.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_4</th>     <td>    0.4448</td> <td>    0.088</td> <td>    5.082</td> <td> 0.000</td> <td>    0.273</td> <td>    0.616</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_5</th>     <td>    0.2042</td> <td>    0.088</td> <td>    2.321</td> <td> 0.020</td> <td>    0.032</td> <td>    0.377</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_6</th>     <td>    0.4338</td> <td>    0.070</td> <td>    6.226</td> <td> 0.000</td> <td>    0.297</td> <td>    0.570</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT1</th> <td> 7.103e-06</td> <td> 1.76e-06</td> <td>    4.039</td> <td> 0.000</td> <td> 3.66e-06</td> <td> 1.05e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT2</th> <td>-6.862e-06</td> <td> 2.86e-06</td> <td>   -2.402</td> <td> 0.016</td> <td>-1.25e-05</td> <td>-1.26e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT3</th> <td> 2.105e-06</td> <td> 3.25e-06</td> <td>    0.648</td> <td> 0.517</td> <td>-4.26e-06</td> <td> 8.47e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT4</th> <td> -1.78e-06</td> <td> 3.15e-06</td> <td>   -0.565</td> <td> 0.572</td> <td>-7.95e-06</td> <td> 4.39e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT5</th> <td> 3.998e-06</td> <td> 3.57e-06</td> <td>    1.120</td> <td> 0.263</td> <td>   -3e-06</td> <td>  1.1e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT6</th> <td> 3.818e-06</td> <td> 2.76e-06</td> <td>    1.381</td> <td> 0.167</td> <td> -1.6e-06</td> <td> 9.24e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT1</th>  <td>-1.115e-05</td> <td> 2.49e-06</td> <td>   -4.480</td> <td> 0.000</td> <td> -1.6e-05</td> <td>-6.27e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT2</th>  <td>-1.146e-05</td> <td> 2.47e-06</td> <td>   -4.641</td> <td> 0.000</td> <td>-1.63e-05</td> <td>-6.62e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT3</th>  <td>-8.701e-06</td> <td> 2.66e-06</td> <td>   -3.267</td> <td> 0.001</td> <td>-1.39e-05</td> <td>-3.48e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT4</th>  <td>-1.289e-05</td> <td> 2.99e-06</td> <td>   -4.313</td> <td> 0.000</td> <td>-1.87e-05</td> <td>-7.03e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT5</th>  <td>-1.132e-05</td> <td> 3.15e-06</td> <td>   -3.593</td> <td> 0.000</td> <td>-1.75e-05</td> <td>-5.15e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT6</th>  <td>-8.818e-06</td> <td> 1.97e-06</td> <td>   -4.476</td> <td> 0.000</td> <td>-1.27e-05</td> <td>-4.96e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <th>PAY_0=1</th>     <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>     <td>    8.5661</td> <td>    0.346</td> <td>   24.767</td> <td> 0.000</td> <td>    7.888</td> <td>    9.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LIMIT_BAL</th> <td>-1.844e-06</td> <td> 3.25e-07</td> <td>   -5.680</td> <td> 0.000</td> <td>-2.48e-06</td> <td>-1.21e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SEX</th>       <td>   -0.2155</td> <td>    0.077</td> <td>   -2.801</td> <td> 0.005</td> <td>   -0.366</td> <td>   -0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>EDUCATION</th> <td>   -0.1804</td> <td>    0.047</td> <td>   -3.824</td> <td> 0.000</td> <td>   -0.273</td> <td>   -0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MARRIAGE</th>  <td>   -0.1131</td> <td>    0.079</td> <td>   -1.433</td> <td> 0.152</td> <td>   -0.268</td> <td>    0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AGE</th>       <td>    0.0051</td> <td>    0.005</td> <td>    1.082</td> <td> 0.279</td> <td>   -0.004</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_2</th>     <td>    4.9348</td> <td>    0.115</td> <td>   42.861</td> <td> 0.000</td> <td>    4.709</td> <td>    5.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_3</th>     <td>   -0.4706</td> <td>    0.081</td> <td>   -5.790</td> <td> 0.000</td> <td>   -0.630</td> <td>   -0.311</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_4</th>     <td>    0.4015</td> <td>    0.090</td> <td>    4.476</td> <td> 0.000</td> <td>    0.226</td> <td>    0.577</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_5</th>     <td>    0.1736</td> <td>    0.091</td> <td>    1.917</td> <td> 0.055</td> <td>   -0.004</td> <td>    0.351</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_6</th>     <td>    0.3552</td> <td>    0.072</td> <td>    4.942</td> <td> 0.000</td> <td>    0.214</td> <td>    0.496</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT1</th> <td>-2.314e-05</td> <td> 2.44e-06</td> <td>   -9.474</td> <td> 0.000</td> <td>-2.79e-05</td> <td>-1.84e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT2</th> <td> 4.186e-06</td> <td> 3.46e-06</td> <td>    1.211</td> <td> 0.226</td> <td>-2.59e-06</td> <td>  1.1e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT3</th> <td> 4.224e-06</td> <td> 3.59e-06</td> <td>    1.175</td> <td> 0.240</td> <td>-2.82e-06</td> <td> 1.13e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT4</th> <td> -1.01e-06</td> <td> 3.53e-06</td> <td>   -0.286</td> <td> 0.775</td> <td>-7.93e-06</td> <td> 5.91e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT5</th> <td> 1.317e-05</td> <td> 3.88e-06</td> <td>    3.398</td> <td> 0.001</td> <td> 5.58e-06</td> <td> 2.08e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BILL_AMT6</th> <td> 3.084e-06</td> <td> 2.98e-06</td> <td>    1.036</td> <td> 0.300</td> <td>-2.75e-06</td> <td> 8.92e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT1</th>  <td> -3.61e-05</td> <td> 3.52e-06</td> <td>  -10.246</td> <td> 0.000</td> <td> -4.3e-05</td> <td>-2.92e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT2</th>  <td>-2.341e-05</td> <td> 3.08e-06</td> <td>   -7.606</td> <td> 0.000</td> <td>-2.94e-05</td> <td>-1.74e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT3</th>  <td>-2.167e-05</td> <td> 3.45e-06</td> <td>   -6.281</td> <td> 0.000</td> <td>-2.84e-05</td> <td>-1.49e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT4</th>  <td>-2.499e-05</td> <td> 3.61e-06</td> <td>   -6.933</td> <td> 0.000</td> <td>-3.21e-05</td> <td>-1.79e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT5</th>  <td>-1.809e-05</td> <td> 3.75e-06</td> <td>   -4.821</td> <td> 0.000</td> <td>-2.54e-05</td> <td>-1.07e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PAY_AMT6</th>  <td>-1.122e-05</td> <td> 2.35e-06</td> <td>   -4.779</td> <td> 0.000</td> <td>-1.58e-05</td> <td>-6.62e-06</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                          MNLogit Regression Results                          \n",
       "==============================================================================\n",
       "Dep. Variable:                  PAY_0   No. Observations:                30000\n",
       "Model:                        MNLogit   Df Residuals:                    29931\n",
       "Method:                           MLE   Df Model:                           66\n",
       "Date:                Sat, 09 Apr 2022   Pseudo R-squ.:                  0.3754\n",
       "Time:                        19:21:11   Log-Likelihood:                -22871.\n",
       "converged:                       True   LL-Null:                       -36618.\n",
       "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
       "==============================================================================\n",
       "  PAY_0=-1       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          8.7368      0.328     26.622      0.000       8.094       9.380\n",
       "LIMIT_BAL  -1.051e-06   2.92e-07     -3.596      0.000   -1.62e-06   -4.78e-07\n",
       "SEX           -0.1590      0.072     -2.223      0.026      -0.299      -0.019\n",
       "EDUCATION     -0.2542      0.044     -5.838      0.000      -0.339      -0.169\n",
       "MARRIAGE      -0.0991      0.073     -1.351      0.177      -0.243       0.045\n",
       "AGE            0.0074      0.004      1.684      0.092      -0.001       0.016\n",
       "PAY_2          4.2855      0.114     37.478      0.000       4.061       4.510\n",
       "PAY_3         -0.4013      0.079     -5.059      0.000      -0.557      -0.246\n",
       "PAY_4          0.2645      0.088      3.013      0.003       0.092       0.437\n",
       "PAY_5          0.1763      0.088      2.005      0.045       0.004       0.349\n",
       "PAY_6          0.3368      0.070      4.840      0.000       0.200       0.473\n",
       "BILL_AMT1  -1.451e-05      2e-06     -7.256      0.000   -1.84e-05   -1.06e-05\n",
       "BILL_AMT2   -3.93e-05   3.27e-06    -12.005      0.000   -4.57e-05   -3.29e-05\n",
       "BILL_AMT3  -8.032e-06   3.09e-06     -2.600      0.009   -1.41e-05   -1.98e-06\n",
       "BILL_AMT4   2.279e-06   2.89e-06      0.789      0.430   -3.38e-06    7.94e-06\n",
       "BILL_AMT5   -1.86e-06   3.16e-06     -0.589      0.556   -8.05e-06    4.33e-06\n",
       "BILL_AMT6   -7.21e-06   2.64e-06     -2.734      0.006   -1.24e-05   -2.04e-06\n",
       "PAY_AMT1    3.423e-05   3.06e-06     11.201      0.000    2.82e-05    4.02e-05\n",
       "PAY_AMT2      3.8e-06   1.92e-06      1.975      0.048    2.95e-08    7.57e-06\n",
       "PAY_AMT3    4.261e-07   2.74e-06      0.156      0.876   -4.94e-06    5.79e-06\n",
       "PAY_AMT4     3.94e-07   2.84e-06      0.139      0.890   -5.17e-06    5.96e-06\n",
       "PAY_AMT5    6.995e-08   2.96e-06      0.024      0.981   -5.72e-06    5.86e-06\n",
       "PAY_AMT6   -2.944e-06   1.65e-06     -1.780      0.075   -6.19e-06    2.98e-07\n",
       "------------------------------------------------------------------------------\n",
       "   PAY_0=0       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          8.9797      0.333     26.930      0.000       8.326       9.633\n",
       "LIMIT_BAL  -4.532e-06   3.07e-07    -14.777      0.000   -5.13e-06   -3.93e-06\n",
       "SEX           -0.2313      0.073     -3.155      0.002      -0.375      -0.088\n",
       "EDUCATION     -0.0285      0.044     -0.651      0.515      -0.115       0.057\n",
       "MARRIAGE       0.0287      0.075      0.383      0.702      -0.118       0.176\n",
       "AGE           -0.0047      0.004     -1.053      0.292      -0.014       0.004\n",
       "PAY_2          3.8719      0.114     33.887      0.000       3.648       4.096\n",
       "PAY_3         -0.2820      0.079     -3.553      0.000      -0.438      -0.126\n",
       "PAY_4          0.4448      0.088      5.082      0.000       0.273       0.616\n",
       "PAY_5          0.2042      0.088      2.321      0.020       0.032       0.377\n",
       "PAY_6          0.4338      0.070      6.226      0.000       0.297       0.570\n",
       "BILL_AMT1   7.103e-06   1.76e-06      4.039      0.000    3.66e-06    1.05e-05\n",
       "BILL_AMT2  -6.862e-06   2.86e-06     -2.402      0.016   -1.25e-05   -1.26e-06\n",
       "BILL_AMT3   2.105e-06   3.25e-06      0.648      0.517   -4.26e-06    8.47e-06\n",
       "BILL_AMT4   -1.78e-06   3.15e-06     -0.565      0.572   -7.95e-06    4.39e-06\n",
       "BILL_AMT5   3.998e-06   3.57e-06      1.120      0.263      -3e-06     1.1e-05\n",
       "BILL_AMT6   3.818e-06   2.76e-06      1.381      0.167    -1.6e-06    9.24e-06\n",
       "PAY_AMT1   -1.115e-05   2.49e-06     -4.480      0.000    -1.6e-05   -6.27e-06\n",
       "PAY_AMT2   -1.146e-05   2.47e-06     -4.641      0.000   -1.63e-05   -6.62e-06\n",
       "PAY_AMT3   -8.701e-06   2.66e-06     -3.267      0.001   -1.39e-05   -3.48e-06\n",
       "PAY_AMT4   -1.289e-05   2.99e-06     -4.313      0.000   -1.87e-05   -7.03e-06\n",
       "PAY_AMT5   -1.132e-05   3.15e-06     -3.593      0.000   -1.75e-05   -5.15e-06\n",
       "PAY_AMT6   -8.818e-06   1.97e-06     -4.476      0.000   -1.27e-05   -4.96e-06\n",
       "------------------------------------------------------------------------------\n",
       "   PAY_0=1       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          8.5661      0.346     24.767      0.000       7.888       9.244\n",
       "LIMIT_BAL  -1.844e-06   3.25e-07     -5.680      0.000   -2.48e-06   -1.21e-06\n",
       "SEX           -0.2155      0.077     -2.801      0.005      -0.366      -0.065\n",
       "EDUCATION     -0.1804      0.047     -3.824      0.000      -0.273      -0.088\n",
       "MARRIAGE      -0.1131      0.079     -1.433      0.152      -0.268       0.042\n",
       "AGE            0.0051      0.005      1.082      0.279      -0.004       0.014\n",
       "PAY_2          4.9348      0.115     42.861      0.000       4.709       5.160\n",
       "PAY_3         -0.4706      0.081     -5.790      0.000      -0.630      -0.311\n",
       "PAY_4          0.4015      0.090      4.476      0.000       0.226       0.577\n",
       "PAY_5          0.1736      0.091      1.917      0.055      -0.004       0.351\n",
       "PAY_6          0.3552      0.072      4.942      0.000       0.214       0.496\n",
       "BILL_AMT1  -2.314e-05   2.44e-06     -9.474      0.000   -2.79e-05   -1.84e-05\n",
       "BILL_AMT2   4.186e-06   3.46e-06      1.211      0.226   -2.59e-06     1.1e-05\n",
       "BILL_AMT3   4.224e-06   3.59e-06      1.175      0.240   -2.82e-06    1.13e-05\n",
       "BILL_AMT4   -1.01e-06   3.53e-06     -0.286      0.775   -7.93e-06    5.91e-06\n",
       "BILL_AMT5   1.317e-05   3.88e-06      3.398      0.001    5.58e-06    2.08e-05\n",
       "BILL_AMT6   3.084e-06   2.98e-06      1.036      0.300   -2.75e-06    8.92e-06\n",
       "PAY_AMT1    -3.61e-05   3.52e-06    -10.246      0.000    -4.3e-05   -2.92e-05\n",
       "PAY_AMT2   -2.341e-05   3.08e-06     -7.606      0.000   -2.94e-05   -1.74e-05\n",
       "PAY_AMT3   -2.167e-05   3.45e-06     -6.281      0.000   -2.84e-05   -1.49e-05\n",
       "PAY_AMT4   -2.499e-05   3.61e-06     -6.933      0.000   -3.21e-05   -1.79e-05\n",
       "PAY_AMT5   -1.809e-05   3.75e-06     -4.821      0.000   -2.54e-05   -1.07e-05\n",
       "PAY_AMT6   -1.122e-05   2.35e-06     -4.779      0.000   -1.58e-05   -6.62e-06\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_sm = sm.MNLogit(pay_0_target, sm.add_constant(pay_0_features))\n",
    "multi_sm_results = multi_sm.fit()\n",
    "multi_sm_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf89a5c",
   "metadata": {},
   "source": [
    "We can get predictions from this model with multi_sm_results.predict(sm.add_\n",
    "constant(pay_0_features)), which again are probabilities for each class, and sum\n",
    "to 1 for each data point.\n",
    "\n",
    "This multinomial logistic regression technique relies on an assumption of\n",
    "independence of irrelevant attributes (IIA), which says the probability of preferring\n",
    "one class over a second class shouldn't depend on another \"irrelevant\" class.\n",
    "\n",
    "In our case, this assumption holds, but an example where this does not hold is if we\n",
    "have a perfect substitute for another class. For example, if our 0 value of PAY_0 (using\n",
    "revolving credit) had a perfect substitute, such as another class with a value of -3 that\n",
    "involved using nearly identical revolving credit, this IIA assumption would not hold,\n",
    "and our logistic regression results may not be reliable.\n",
    "\n",
    "This implementations of multinomial logistic regression can be thought of as fitting\n",
    "k-1 binary logistic regression models against each class versus the last class, which\n",
    "is why statsmodels returns k-1 sets of coefficients for k classes. However, there are\n",
    "a few other multiclass modeling strategies: one-versus-rest and one-versus-one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54430b2",
   "metadata": {},
   "source": [
    "### One-versus-rest and one-versus-one formulation\n",
    "\n",
    "The one-versus-rest (OVR) and one-versus-one (OVO) formulations of\n",
    "multinomial classification are slightly different. OVR formulates the problem so\n",
    "each class is fit against all other classes in a binary classification problem, giving\n",
    "us k models for k classes. An advantage of OVR is that it's easy to interpret – each\n",
    "model has the coefficients or parameters for that single class versus all others.\n",
    "The OVO implementation creates a model for each pair of classes, and results in\n",
    "k * (k – 1) / 2 models. These models are not as easy to interpret as OVR or the\n",
    "default implementation from sklearn models, but OVO can provide an advantage\n",
    "for algorithms that don't scale well with the number of samples (for example, KNN\n",
    "when using the brute-force distance calculation approach). The OVO gets predictions by taking the\n",
    "majority vote for the predicted class from all its classifiers.\n",
    "\n",
    "> To use OVR with logistic regression, we can set the multi_class argument to 'ovr':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "34101c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_multi_ovr = LogisticRegression(max_iter=1000, multi_class=\"ovr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b6c13a",
   "metadata": {},
   "source": [
    "This keeps the same logistic regression object and interface that we've used before.\n",
    "Another method that is general to any model is to use other classes from sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c85ba4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsOneClassifier(estimator=LogisticRegression(max_iter=1000), n_jobs=-1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "\n",
    "\n",
    "lr_ovr = OneVsRestClassifier(LogisticRegression(max_iter=1000), n_jobs=-1)\n",
    "lr_ovo = OneVsOneClassifier(LogisticRegression(max_iter=1000), n_jobs=-1)\n",
    "\n",
    "\n",
    "lr_ovr.fit(pay_0_features, pay_0_target)\n",
    "lr_ovo.fit(pay_0_features, pay_0_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0807d62d",
   "metadata": {},
   "source": [
    "Here, we import the OVO and OVR classes from sklearn and simply wrap our\n",
    "logistic regression algorithm with these classes. We can then use the same fit,\n",
    "predict, and some other methods that we used before. The predict_proba is not\n",
    "available to the OVO model, however. We are setting the n_jobs argument to -1 in\n",
    "order to use all available processors. We can also get each individual model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "875578b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_ovo.estimators_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccdecaf",
   "metadata": {},
   "source": [
    "This gives us the first logistic regression model out of 6 models. There are 4 classes\n",
    "(-2, -1, 0, 1) and the pairs of targets for the models follow a pattern: the first class\n",
    "is paired with the next classes in a row, then the second class is paired with the\n",
    "following classes in a row, and so on. This gives us the pairs: (-2, -1), (-2, 0), (-2, 1),\n",
    "(-1, 0), (-1, 1), and 1, and (0, 1). With the individual models, we can access any of the\n",
    "models' attributes and methods, such as predict_proba.\n",
    "\n",
    "Models in sklearn will tend to use the best implementation for each algorithm, OVR,\n",
    "or OVO. However, we can always try the different implementations for algorithms\n",
    "and see which works best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d6ecab",
   "metadata": {},
   "source": [
    "### Multi-label Classification\n",
    "\n",
    "Some classification problems can have multiple labels for each target. For example,\n",
    "if we are classifying the topic of a news story, it might fall under several categories\n",
    "at the same time (a news story could be about both economics and politics, for\n",
    "example). Classifiers for multilabel problems (which we could also call multitarget)\n",
    "are multi-output classifiers by definition. We can use the PAY columns in our default\n",
    "dataset to create a multilabel classification problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b0a4175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import swifter\n",
    "\n",
    "\n",
    "mo_targets = credit_df[[\"PAY_0\"] + [f\"PAY_{i}\" for i in range(2, 7)]].copy()\n",
    "\n",
    "mo_targets = mo_targets.swifter.apply(lambda x: (x > 0).astype(int), axis=1)\n",
    "\n",
    "mo_features = credit_df.drop([\"PAY_0\", \"default payment next month\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672e3541",
   "metadata": {},
   "source": [
    "Here, we retrieve the PAY columns from our original DataFrame and make a copy\n",
    "(so as not to alter the original DataFrame or have any SettingwithCopyWarning\n",
    "warnings from pandas). We then apply the function across rows (with axis=1),\n",
    "which returns 1 if the value is greater than 0, and 0 otherwise. This gives us 1s for\n",
    "each of the PAY columns where there was a late payment, and 0 otherwise. We use\n",
    "the swifter package to parallelize the apply function.\n",
    "\n",
    "\n",
    "Some sklearn models can handle multi-label data out of the box – for example, treebased\n",
    "models such as DecisionTreeClassifier, as well as KNN. However, many\n",
    "other models cannot handle this, and we must use another sklearn class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "123921bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=LogisticRegression(max_iter=1000), n_jobs=-1)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "\n",
    "mo_clf = MultiOutputClassifier(LogisticRegression(max_iter=1000), n_jobs=-1)\n",
    "mo_clf.fit(mo_features, mo_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80555997",
   "metadata": {},
   "source": [
    "Here, we import and use the MultiOutputClassifier class, which behaves nearly\n",
    "identically to the OVO and OVR classes. We give it an estimator as the first argument\n",
    "and tell it to use all available processors with the n_jobs=-1 argument. Then we can\n",
    "use similar methods as before, such as predict, predict_proba, and score. This class\n",
    "fits an individual model for each target, and stores the models in the estimators_\n",
    "attribute of our mo_clf variable.\n",
    "\n",
    "Another similar method is the ClassifierChain class from sklearn. This fits\n",
    "individual models for each class, but fits them sequentially. It first fits a model to the\n",
    "first target value, and then sequentially fits models for the other target values using\n",
    "the actual value (in training) or predicted value (during inference or prediction) of\n",
    "the class from the previous model. We can use this model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a3182de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierChain(base_estimator=LogisticRegression(max_iter=100000))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multioutput import ClassifierChain\n",
    "\n",
    "cc_clf = ClassifierChain(LogisticRegression(max_iter=100000))\n",
    "\n",
    "cc_clf.fit(mo_features, mo_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e39275",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
